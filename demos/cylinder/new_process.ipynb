{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1efd8d1c-fd77-4fc8-b29f-6d15a162916a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.0+cu126\n",
      "CUDA version: 12.6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d070c206-c69d-46c2-97ae-b3027f724777",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANALYZING NEW DATA STRUCTURE\n",
      "================================================================================\n",
      "Searching for VTK/VTU files in: /standard/sds_baek_energetic/von_karman_vortex/Reynolds 1~150\n",
      "================================================================================\n",
      "\n",
      "Found 2406 VTK/VTU files\n",
      "\n",
      "VTK FILES BY DIRECTORY DEPTH:\n",
      "\n",
      "  Depth 4 (2406 files):\n",
      "    Pattern: Reynolds_50/Reynolds_50/VTK/Reynolds_50_4450/...\n",
      "    Example: Reynolds_50/Reynolds_50/VTK/Reynolds_50_4450/internal.vtu\n",
      "\n",
      "================================================================================\n",
      "PATTERN ANALYSIS:\n",
      "\n",
      "  Reynolds numbers: [1]\n",
      "    Re=1: 2406 files\n",
      "\n",
      "  File naming patterns:\n",
      "    Unique filenames: 1\n",
      "      'internal.vtu': 2406 occurrences\n",
      "\n",
      "\n",
      "\n",
      "COMPARING OLD VS NEW STRUCTURES\n",
      "================================================================================\n",
      "COMPARING DIRECTORY STRUCTURES\n",
      "================================================================================\n",
      "\n",
      "STRUCTURE 1 (OLD DATA):\n",
      "  Path: /home/jtb3sud/K치rm치n vortex street\n",
      "  Total directories: 826\n",
      "  Total files: 2920\n",
      "  VTK files: 402\n",
      "  Reynolds numbers: [20, 100]\n",
      "  Timesteps: 0\n",
      "  Avg VTK depth: 4.0\n",
      "\n",
      "================================================================================\n",
      "\n",
      "STRUCTURE 2 (NEW DATA):\n",
      "  Path: /standard/sds_baek_energetic/von_karman_vortex/Reynolds 1~150\n",
      "  Total directories: 12094\n",
      "  Total files: 38815\n",
      "  VTK files: 2406\n",
      "  Reynolds numbers: [1]\n",
      "  Timesteps: 0\n",
      "  Avg VTK depth: 5.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "def find_vtk_files_summary(base_path):\n",
    "    \"\"\"Find VTK files and provide a compact summary.\"\"\"\n",
    "    base_path = Path(base_path)\n",
    "    \n",
    "    print(f\"Searching for VTK/VTU files in: {base_path}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    vtk_files = list(base_path.rglob('*.vtu')) + list(base_path.rglob('*.vtk'))\n",
    "    \n",
    "    if not vtk_files:\n",
    "        print(\"No VTK/VTU files found!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(vtk_files)} VTK/VTU files\\n\")\n",
    "    \n",
    "    # Group by directory depth\n",
    "    depth_groups = {}\n",
    "    for vtk_file in vtk_files:\n",
    "        rel_path = vtk_file.relative_to(base_path)\n",
    "        depth = len(rel_path.parts) - 1\n",
    "        \n",
    "        if depth not in depth_groups:\n",
    "            depth_groups[depth] = []\n",
    "        depth_groups[depth].append(rel_path)\n",
    "    \n",
    "    print(\"VTK FILES BY DIRECTORY DEPTH:\")\n",
    "    for depth in sorted(depth_groups.keys()):\n",
    "        files = depth_groups[depth]\n",
    "        print(f\"\\n  Depth {depth} ({len(files)} files):\")\n",
    "        if len(files) > 0:\n",
    "            sample_path = files[0]\n",
    "            print(f\"    Pattern: {'/'.join(sample_path.parts[:-1])}/...\")\n",
    "            print(f\"    Example: {sample_path}\")\n",
    "    \n",
    "    # Pattern analysis\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PATTERN ANALYSIS:\")\n",
    "    \n",
    "    # Reynolds numbers\n",
    "    reynolds_patterns = {}\n",
    "    for vtk_file in vtk_files:\n",
    "        path_str = str(vtk_file)\n",
    "        reynolds_match = re.search(r'[Rr]eynolds[_\\s]*(\\d+)', path_str)\n",
    "        if reynolds_match:\n",
    "            re_num = int(reynolds_match.group(1))\n",
    "            if re_num not in reynolds_patterns:\n",
    "                reynolds_patterns[re_num] = []\n",
    "            reynolds_patterns[re_num].append(vtk_file)\n",
    "    \n",
    "    if reynolds_patterns:\n",
    "        print(f\"\\n  Reynolds numbers: {sorted(reynolds_patterns.keys())}\")\n",
    "        for re_num in sorted(reynolds_patterns.keys())[:3]:\n",
    "            print(f\"    Re={re_num}: {len(reynolds_patterns[re_num])} files\")\n",
    "        if len(reynolds_patterns) > 3:\n",
    "            print(f\"    ... and {len(reynolds_patterns) - 3} more Reynolds numbers\")\n",
    "    \n",
    "    # Timestep patterns\n",
    "    timestep_dirs = set()\n",
    "    for vtk_file in vtk_files:\n",
    "        path_str = str(vtk_file.parent)\n",
    "        timestep_match = re.search(r'/(\\d+)$', path_str)\n",
    "        if timestep_match:\n",
    "            timestep_dirs.add(timestep_match.group(1))\n",
    "    \n",
    "    if timestep_dirs:\n",
    "        sorted_timesteps = sorted([int(t) for t in timestep_dirs])\n",
    "        print(f\"\\n  Timestep directories: {len(sorted_timesteps)} unique\")\n",
    "        print(f\"    Range: {min(sorted_timesteps)} to {max(sorted_timesteps)}\")\n",
    "        print(f\"    Samples: {sorted_timesteps[:5]}{'...' if len(sorted_timesteps) > 5 else ''}\")\n",
    "    \n",
    "    # File naming patterns\n",
    "    print(f\"\\n  File naming patterns:\")\n",
    "    filenames = set(f.name for f in vtk_files)\n",
    "    print(f\"    Unique filenames: {len(filenames)}\")\n",
    "    for name in sorted(filenames)[:5]:\n",
    "        count = sum(1 for f in vtk_files if f.name == name)\n",
    "        print(f\"      '{name}': {count} occurrences\")\n",
    "    if len(filenames) > 5:\n",
    "        print(f\"      ... and {len(filenames) - 5} more unique names\")\n",
    "    \n",
    "    return vtk_files\n",
    "\n",
    "\n",
    "def compare_structures_compact(path1, path2):\n",
    "    \"\"\"Compact comparison of two directory structures.\"\"\"\n",
    "    print(\"COMPARING DIRECTORY STRUCTURES\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    path1 = Path(path1)\n",
    "    path2 = Path(path2)\n",
    "    \n",
    "    def get_structure_summary(path):\n",
    "        \"\"\"Get summary statistics for a path.\"\"\"\n",
    "        vtk_files = list(path.rglob('*.vtu')) + list(path.rglob('*.vtk'))\n",
    "        \n",
    "        reynolds_nums = set()\n",
    "        timesteps = set()\n",
    "        \n",
    "        for vtk_file in vtk_files:\n",
    "            path_str = str(vtk_file)\n",
    "            re_match = re.search(r'[Rr]eynolds[_\\s]*(\\d+)', path_str)\n",
    "            if re_match:\n",
    "                reynolds_nums.add(int(re_match.group(1)))\n",
    "            \n",
    "            ts_match = re.search(r'/(\\d+)/', path_str)\n",
    "            if ts_match:\n",
    "                timesteps.add(int(ts_match.group(1)))\n",
    "        \n",
    "        # Get directory depth of VTK files\n",
    "        if vtk_files:\n",
    "            depths = [len(f.relative_to(path).parts) for f in vtk_files]\n",
    "            avg_depth = sum(depths) / len(depths)\n",
    "        else:\n",
    "            avg_depth = 0\n",
    "        \n",
    "        return {\n",
    "            'total_dirs': sum(1 for _ in path.rglob('*') if _.is_dir()),\n",
    "            'total_files': sum(1 for _ in path.rglob('*') if _.is_file()),\n",
    "            'vtk_files': len(vtk_files),\n",
    "            'reynolds_numbers': sorted(reynolds_nums),\n",
    "            'timesteps': len(timesteps),\n",
    "            'avg_vtk_depth': avg_depth\n",
    "        }\n",
    "    \n",
    "    print(\"STRUCTURE 1 (OLD DATA):\")\n",
    "    print(f\"  Path: {path1}\")\n",
    "    if path1.exists():\n",
    "        summary1 = get_structure_summary(path1)\n",
    "        print(f\"  Total directories: {summary1['total_dirs']}\")\n",
    "        print(f\"  Total files: {summary1['total_files']}\")\n",
    "        print(f\"  VTK files: {summary1['vtk_files']}\")\n",
    "        print(f\"  Reynolds numbers: {summary1['reynolds_numbers']}\")\n",
    "        print(f\"  Timesteps: {summary1['timesteps']}\")\n",
    "        print(f\"  Avg VTK depth: {summary1['avg_vtk_depth']:.1f}\")\n",
    "    else:\n",
    "        print(\"  Path does not exist!\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\\n\")\n",
    "    \n",
    "    print(\"STRUCTURE 2 (NEW DATA):\")\n",
    "    print(f\"  Path: {path2}\")\n",
    "    if path2.exists():\n",
    "        summary2 = get_structure_summary(path2)\n",
    "        print(f\"  Total directories: {summary2['total_dirs']}\")\n",
    "        print(f\"  Total files: {summary2['total_files']}\")\n",
    "        print(f\"  VTK files: {summary2['vtk_files']}\")\n",
    "        print(f\"  Reynolds numbers: {summary2['reynolds_numbers']}\")\n",
    "        print(f\"  Timesteps: {summary2['timesteps']}\")\n",
    "        print(f\"  Avg VTK depth: {summary2['avg_vtk_depth']:.1f}\")\n",
    "    else:\n",
    "        print(\"  Path does not exist!\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# RUN ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ANALYZING NEW DATA STRUCTURE\")\n",
    "print(\"=\"*80)\n",
    "new_data_path = \"/standard/sds_baek_energetic/von_karman_vortex/Reynolds 1~150\"\n",
    "find_vtk_files_summary(new_data_path)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"COMPARING OLD VS NEW STRUCTURES\")\n",
    "print(\"=\"*80)\n",
    "old_data_path = \"/home/jtb3sud/K치rm치n vortex street\"\n",
    "compare_structures_compact(old_data_path, new_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d37b411-21a4-4bc0-bfad-a05d07e63799",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reynolds directories found: ['Reynolds_1', 'Reynolds_100', 'Reynolds_150', 'Reynolds_20', 'Reynolds_40', 'Reynolds_50']\n",
      "Total: 6\n"
     ]
    }
   ],
   "source": [
    "# Quick check - what Reynolds directories exist?\n",
    "new_data_path = Path(\"/standard/sds_baek_energetic/von_karman_vortex/Reynolds 1~150\")\n",
    "reynolds_dirs = [d.name for d in new_data_path.iterdir() if d.is_dir() and 'Reynolds' in d.name]\n",
    "print(f\"Reynolds directories found: {sorted(reynolds_dirs)[:10]}\")\n",
    "print(f\"Total: {len(reynolds_dirs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "835d61d6-c62e-40b1-946d-e92d60d3f946",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directly inspecting timestep directory names in: /standard/sds_baek_energetic/von_karman_vortex/Reynolds 200~\n",
      "================================================================================\n",
      "Found 8 Reynolds directories to inspect.\n",
      "\n",
      "--- Case: Re = 200 ---\n",
      "  Inspecting path: /standard/sds_baek_energetic/von_karman_vortex/Reynolds 200~/Reynolds_200/Reynolds_200\n",
      "  Found 401 numerical timestep directories.\n",
      "  First 15 sorted timestep values: [0.0, 0.025, 0.05, 0.075, 0.1, 0.125, 0.15, 0.175, 0.2, 0.225, 0.25, 0.275, 0.3, 0.325, 0.35]\n",
      "\n",
      "--- Case: Re = 250 ---\n",
      "  Inspecting path: /standard/sds_baek_energetic/von_karman_vortex/Reynolds 200~/Reynolds_250/Reynolds_250\n",
      "  Found 401 numerical timestep directories.\n",
      "  First 15 sorted timestep values: [0.0, 0.025, 0.05, 0.075, 0.1, 0.125, 0.15, 0.175, 0.2, 0.225, 0.25, 0.275, 0.3, 0.325, 0.35]\n",
      "\n",
      "--- Case: Re = 300 ---\n",
      "  Inspecting path: /standard/sds_baek_energetic/von_karman_vortex/Reynolds 200~/Reynolds_300/Reynolds_300\n",
      "  Found 401 numerical timestep directories.\n",
      "  First 15 sorted timestep values: [0.0, 0.025, 0.05, 0.075, 0.1, 0.125, 0.15, 0.175, 0.2, 0.225, 0.25, 0.275, 0.3, 0.325, 0.35]\n",
      "\n",
      "--- Case: Re = 350 ---\n",
      "  Inspecting path: /standard/sds_baek_energetic/von_karman_vortex/Reynolds 200~/Reynolds_350/Reynolds_350\n",
      "  Found 401 numerical timestep directories.\n",
      "  First 15 sorted timestep values: [0.0, 0.025, 0.05, 0.075, 0.1, 0.125, 0.15, 0.175, 0.2, 0.225, 0.25, 0.275, 0.3, 0.325, 0.35]\n",
      "\n",
      "--- Case: Re = 400 ---\n",
      "  Inspecting path: /standard/sds_baek_energetic/von_karman_vortex/Reynolds 200~/Reynolds_400/Reynolds_400\n",
      "  Found 401 numerical timestep directories.\n",
      "  First 15 sorted timestep values: [0.0, 0.025, 0.05, 0.075, 0.1, 0.125, 0.15, 0.175, 0.2, 0.225, 0.25, 0.275, 0.3, 0.325, 0.35]\n",
      "\n",
      "--- Case: Re = 450 ---\n",
      "  Inspecting path: /standard/sds_baek_energetic/von_karman_vortex/Reynolds 200~/Reynolds_450/Reynolds_450\n",
      "  Found 401 numerical timestep directories.\n",
      "  First 15 sorted timestep values: [0.0, 0.025, 0.05, 0.075, 0.1, 0.125, 0.15, 0.175, 0.2, 0.225, 0.25, 0.275, 0.3, 0.325, 0.35]\n",
      "\n",
      "--- Case: Re = 500 ---\n",
      "  Inspecting path: /standard/sds_baek_energetic/von_karman_vortex/Reynolds 200~/Reynolds_500/Reynolds_500\n",
      "  Found 401 numerical timestep directories.\n",
      "  First 15 sorted timestep values: [0.0, 0.025, 0.05, 0.075, 0.1, 0.125, 0.15, 0.175, 0.2, 0.225, 0.25, 0.275, 0.3, 0.325, 0.35]\n",
      "\n",
      "--- Case: Re = 550 ---\n",
      "  Inspecting path: /standard/sds_baek_energetic/von_karman_vortex/Reynolds 200~/Reynolds_550/Reynolds_550\n",
      "  Found 401 numerical timestep directories.\n",
      "  First 15 sorted timestep values: [0.0, 0.025, 0.05, 0.075, 0.1, 0.125, 0.15, 0.175, 0.2, 0.225, 0.25, 0.275, 0.3, 0.325, 0.35]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def inspect_directory_names(base_data_dir):\n",
    "    \"\"\"\n",
    "    Directly reads and prints the numerical timestep values from directory names\n",
    "    for each Reynolds case to show the raw data sequence.\n",
    "    This version correctly handles floating-point numbers in directory names\n",
    "    and looks for them in the correct nested directory structure.\n",
    "    \"\"\"\n",
    "    base_data_dir = Path(base_data_dir)\n",
    "    print(f\"Directly inspecting timestep directory names in: {base_data_dir}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if not base_data_dir.exists():\n",
    "        print(f\"Directory {base_data_dir} does not exist!\")\n",
    "        return\n",
    "\n",
    "    # Find all Reynolds_XX directories at the base level, sorted numerically\n",
    "    try:\n",
    "        reynolds_dirs = sorted(\n",
    "            [d for d in base_data_dir.iterdir() if d.is_dir() and d.name.startswith('Reynolds_')],\n",
    "            key=lambda x: int(re.search(r'Reynolds_(\\d+)', x.name).group(1))\n",
    "        )\n",
    "    except (ValueError, AttributeError):\n",
    "        print(\"Could not sort Reynolds directories numerically. Using alphabetical sort.\")\n",
    "        reynolds_dirs = sorted([d for d in base_data_dir.iterdir() if d.is_dir() and d.name.startswith('Reynolds_')])\n",
    "\n",
    "    print(f\"Found {len(reynolds_dirs)} Reynolds directories to inspect.\")\n",
    "    \n",
    "    for re_dir in reynolds_dirs:\n",
    "        re_match = re.search(r'Reynolds_(\\d+)', re_dir.name)\n",
    "        if not re_match:\n",
    "            continue\n",
    "        reynolds_num = int(re_match.group(1))\n",
    "        \n",
    "        print(f\"\\n--- Case: Re = {reynolds_num} ---\")\n",
    "        \n",
    "        # Navigate to the double-nested Reynolds directory, as per the correct path structure.\n",
    "        data_dir = re_dir / re_dir.name\n",
    "        print(f\"  Inspecting path: {data_dir}\")\n",
    "\n",
    "        if not data_dir.is_dir():\n",
    "            print(\"  -> Nested Reynolds directory not found.\")\n",
    "            continue\n",
    "            \n",
    "        # Get all child directories directly from this path.\n",
    "        timestep_dirs = [d for d in data_dir.iterdir() if d.is_dir()]\n",
    "        \n",
    "        # Attempt to convert directory names to floating-point numbers\n",
    "        time_values = []\n",
    "        for ts_dir in timestep_dirs:\n",
    "            # Use a regex that captures integers and decimals from the entire directory name\n",
    "            match = re.search(r'^(\\d+\\.?\\d*)$', ts_dir.name)\n",
    "            if match:\n",
    "                try:\n",
    "                    time_values.append(float(match.group(1)))\n",
    "                except ValueError:\n",
    "                    print(f\"  Could not convert '{match.group(1)}' to a float.\")\n",
    "\n",
    "        if not time_values:\n",
    "            print(\"  -> Could not find any directories with a valid numerical name (e.g., '0.025').\")\n",
    "            continue\n",
    "\n",
    "        # Sort the numbers to see the actual sequence\n",
    "        time_values.sort()\n",
    "        \n",
    "        print(f\"  Found {len(time_values)} numerical timestep directories.\")\n",
    "        print(f\"  First 15 sorted timestep values: {time_values[:15]}\")\n",
    "\n",
    "\n",
    "# Test the inspection function\n",
    "if __name__ == \"__main__\":\n",
    "    NEW_DATA_DIR = \"/standard/sds_baek_energetic/von_karman_vortex/Reynolds 200~\"\n",
    "    inspect_directory_names(NEW_DATA_DIR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9840ad7-cd24-4c15-ae52-04b072e7a627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing Karman vortex cases in: /standard/sds_baek_energetic/von_karman_vortex/Reynolds 200~\n",
      "Found 8 Reynolds directories to analyze.\n",
      "  Re=250: No valid timestep directories found.\n",
      "  Re=350: No valid timestep directories found.\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS SUMMARY\n",
      "================================================================================\n",
      "Re         Timesteps    Start Time      End Time        Write Interval \n",
      "--------   ----------   -------------   -------------   -------------- \n",
      "200        761          0               100000          25             \n",
      "300        401          0               100000          250            \n",
      "400        401          0               100000          250            \n",
      "450        401          0               100000          250            \n",
      "500        401          0               100000          250            \n",
      "550        401          0               100000          250            \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "def analyze_reynolds_cases(base_data_dir):\n",
    "    \"\"\"\n",
    "    Analyzes Reynolds cases to determine simulation duration and write intervals.\n",
    "\n",
    "    This function inspects the timestep directories to extract:\n",
    "    - The total number of timesteps.\n",
    "    - The start and end \"time\" values (from directory names).\n",
    "    - The most common interval between saved timesteps.\n",
    "    \"\"\"\n",
    "    base_data_dir = Path(base_data_dir)\n",
    "    print(f\"Analyzing Karman vortex cases in: {base_data_dir}\")\n",
    "    \n",
    "    if not base_data_dir.exists():\n",
    "        print(f\"Directory {base_data_dir} does not exist!\")\n",
    "        return []\n",
    "\n",
    "    analysis_results = []\n",
    "    \n",
    "    # Find all Reynolds_XX directories at the base level\n",
    "    reynolds_dirs = sorted(\n",
    "        [d for d in base_data_dir.iterdir() if d.is_dir() and d.name.startswith('Reynolds_')],\n",
    "        key=lambda x: int(re.search(r'Reynolds_(\\d+)', x.name).group(1))\n",
    "    )\n",
    "    \n",
    "    print(f\"Found {len(reynolds_dirs)} Reynolds directories to analyze.\")\n",
    "    \n",
    "    for re_dir in reynolds_dirs:\n",
    "        re_match = re.search(r'Reynolds_(\\d+)', re_dir.name)\n",
    "        if not re_match:\n",
    "            continue\n",
    "        reynolds_num = int(re_match.group(1))\n",
    "        \n",
    "        # Navigate to the VTK directory\n",
    "        vtk_dir = re_dir / re_dir.name / \"VTK\"\n",
    "        if not vtk_dir.is_dir():\n",
    "            continue\n",
    "            \n",
    "        # Get all timestep directories\n",
    "        timestep_dirs = [d for d in vtk_dir.iterdir() if d.is_dir()]\n",
    "        \n",
    "        # Extract numerical time values from directory names\n",
    "        time_values = []\n",
    "        for ts_dir in timestep_dirs:\n",
    "            # Pattern: Reynolds_XX_timestep\n",
    "            ts_match = re.search(rf'Reynolds_{reynolds_num}_(\\d+)', ts_dir.name)\n",
    "            if ts_match:\n",
    "                time_values.append(int(ts_match.group(1)))\n",
    "        \n",
    "        if not time_values:\n",
    "            print(f\"  Re={reynolds_num}: No valid timestep directories found.\")\n",
    "            continue\n",
    "\n",
    "        # Sort times to ensure correct order\n",
    "        time_values.sort()\n",
    "        \n",
    "        # Calculate intervals between consecutive timesteps\n",
    "        intervals = [time_values[i] - time_values[i-1] for i in range(1, len(time_values))]\n",
    "        \n",
    "        # Find the most common interval\n",
    "        if intervals:\n",
    "            most_common_interval = Counter(intervals).most_common(1)[0][0]\n",
    "        else:\n",
    "            most_common_interval = 0 # Case with only one timestep\n",
    "\n",
    "        analysis_results.append({\n",
    "            \"reynolds_num\": reynolds_num,\n",
    "            \"timesteps\": len(time_values),\n",
    "            \"start_time\": time_values[0],\n",
    "            \"end_time\": time_values[-1],\n",
    "            \"write_interval\": most_common_interval\n",
    "        })\n",
    "\n",
    "    return analysis_results\n",
    "\n",
    "def print_analysis_summary(results):\n",
    "    \"\"\"Prints a formatted summary of the analysis results.\"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to display.\")\n",
    "        return\n",
    "        \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Re':<10} {'Timesteps':<12} {'Start Time':<15} {'End Time':<15} {'Write Interval':<15}\")\n",
    "    print(f\"{'-'*8:<10} {'-'*10:<12} {'-'*13:<15} {'-'*13:<15} {'-'*14:<15}\")\n",
    "    \n",
    "    for res in results:\n",
    "        print(f\"{res['reynolds_num']:<10} {res['timesteps']:<12} {res['start_time']:<15} {res['end_time']:<15} {res['write_interval']:<15}\")\n",
    "\n",
    "\n",
    "# Test the analysis function\n",
    "if __name__ == \"__main__\":\n",
    "    NEW_DATA_DIR = \"/standard/sds_baek_energetic/von_karman_vortex/Reynolds 200~\"\n",
    "    \n",
    "    results = analyze_reynolds_cases(NEW_DATA_DIR)\n",
    "    print_analysis_summary(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "599adfdf-1b31-46d4-9a43-c5dd72d06f29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring Reynolds_50 structure:\n",
      "================================================================================\n",
      "\n",
      "1. Contents of Reynolds_50/:\n",
      "   Reynolds_50/ \n",
      "\n",
      "2. Contents of Reynolds_50/Reynolds_50/:\n",
      "   0/ \n",
      "   0.025/ \n",
      "   0.05/ \n",
      "   0.075/ \n",
      "   0.1/ \n",
      "   0.125/ \n",
      "   0.15/ \n",
      "   0.175/ \n",
      "   0.2/ \n",
      "   0.225/ \n",
      "\n",
      "3. Contents of Reynolds_50/Reynolds_50/VTK/ (first 10 items):\n",
      "   Reynolds_50.vtm.series\n",
      "   Reynolds_50_0/ \n",
      "   Reynolds_50_0.vtm\n",
      "   Reynolds_50_100/ \n",
      "   Reynolds_50_100.vtm\n",
      "   Reynolds_50_1000/ \n",
      "   Reynolds_50_1000.vtm\n",
      "   Reynolds_50_10000/ \n",
      "   Reynolds_50_10000.vtm\n",
      "   Reynolds_50_1025/ \n",
      "   ... (total: 803 items)\n",
      "\n",
      "4. Contents of Reynolds_50_0/:\n",
      "   boundary/\n",
      "   boundary.vtm (0.00 MB)\n",
      "   internal.vtu (5.78 MB)\n"
     ]
    }
   ],
   "source": [
    "# Let's look at what's actually inside one Reynolds case\n",
    "from pathlib import Path\n",
    "\n",
    "new_data_path = Path(\"/standard/sds_baek_energetic/von_karman_vortex/Reynolds 1~150\")\n",
    "\n",
    "# Pick one Reynolds number to explore\n",
    "re_dir = new_data_path / \"Reynolds_50\"\n",
    "\n",
    "print(\"Exploring Reynolds_50 structure:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Level 1: What's directly in Reynolds_50?\n",
    "print(\"\\n1. Contents of Reynolds_50/:\")\n",
    "for item in sorted(re_dir.iterdir())[:10]:\n",
    "    print(f\"   {item.name}/ \" if item.is_dir() else f\"   {item.name}\")\n",
    "\n",
    "# Level 2: What's in the nested Reynolds_50?\n",
    "inner_dir = re_dir / \"Reynolds_50\"\n",
    "if inner_dir.exists():\n",
    "    print(\"\\n2. Contents of Reynolds_50/Reynolds_50/:\")\n",
    "    for item in sorted(inner_dir.iterdir())[:10]:\n",
    "        print(f\"   {item.name}/ \" if item.is_dir() else f\"   {item.name}\")\n",
    "\n",
    "# Level 3: What's in VTK?\n",
    "vtk_dir = inner_dir / \"VTK\"\n",
    "if vtk_dir.exists():\n",
    "    print(\"\\n3. Contents of Reynolds_50/Reynolds_50/VTK/ (first 10 items):\")\n",
    "    items = sorted(vtk_dir.iterdir())[:10]\n",
    "    for item in items:\n",
    "        print(f\"   {item.name}/ \" if item.is_dir() else f\"   {item.name}\")\n",
    "    print(f\"   ... (total: {len(list(vtk_dir.iterdir()))} items)\")\n",
    "\n",
    "# Level 4: What's in a timestep directory?\n",
    "if vtk_dir.exists():\n",
    "    timestep_dirs = [d for d in vtk_dir.iterdir() if d.is_dir()]\n",
    "    if timestep_dirs:\n",
    "        sample_ts = sorted(timestep_dirs)[0]\n",
    "        print(f\"\\n4. Contents of {sample_ts.name}/:\")\n",
    "        for item in sorted(sample_ts.iterdir()):\n",
    "            if item.is_file():\n",
    "                size = item.stat().st_size / (1024*1024)  # MB\n",
    "                print(f\"   {item.name} ({size:.2f} MB)\")\n",
    "            else:\n",
    "                print(f\"   {item.name}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34b540a2-9c5c-4e73-a2da-c7b96f0beb13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of points: 60746\n",
      "Number of cells: 29938\n",
      "\n",
      "Available fields:\n",
      "  - p (components: 1)\n",
      "  - U (components: 3)\n",
      "  - vorticity (components: 3)\n"
     ]
    }
   ],
   "source": [
    "import vtk\n",
    "\n",
    "# Test reading one file\n",
    "test_file = \"/standard/sds_baek_energetic/von_karman_vortex/Reynolds 1~150/Reynolds_50/Reynolds_50/VTK/Reynolds_50_0/internal.vtu\"\n",
    "\n",
    "reader = vtk.vtkXMLUnstructuredGridReader()\n",
    "reader.SetFileName(test_file)\n",
    "reader.Update()\n",
    "mesh = reader.GetOutput()\n",
    "\n",
    "print(f\"Number of points: {mesh.GetNumberOfPoints()}\")\n",
    "print(f\"Number of cells: {mesh.GetNumberOfCells()}\")\n",
    "\n",
    "print(\"\\nAvailable fields:\")\n",
    "point_data = mesh.GetPointData()\n",
    "for i in range(point_data.GetNumberOfArrays()):\n",
    "    array = point_data.GetArray(i)\n",
    "    print(f\"  - {array.GetName()} (components: {array.GetNumberOfComponents()})\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6856975d-d6b6-474a-b8b7-072d5bc30561",
   "metadata": {
    "tags": []
   },
   "source": [
    "import vtk\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class KarmanVortexProcessorNewStructure:\n",
    "    \"\"\"\n",
    "    Processor for Karman vortex street simulations - NEW DATA STRUCTURE.\n",
    "    Modified to match shock tube format: positions + physics in x tensor.\n",
    "    \n",
    "    New structure pattern: Reynolds_50/Reynolds_50/VTK/Reynolds_50_4450/internal.vtu\n",
    "    \n",
    "    Converts VTK data to PyTorch Geometric format with Reynolds number as global parameter.\n",
    "    Uses ONLY raw data from VTK files - no derived features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_data_dir, include_parameters_as_features=True):\n",
    "        self.base_data_dir = Path(base_data_dir)\n",
    "        self.include_parameters_as_features = include_parameters_as_features\n",
    "        \n",
    "        # Position variables (will be first in x tensor)\n",
    "        self.position_vars = ['x_pos', 'y_pos', 'z_pos']\n",
    "        \n",
    "        # Physical variable names from VTK files - RAW DATA ONLY\n",
    "        self.physics_vars = ['pressure', 'velocity_x', 'velocity_y', 'velocity_z', \n",
    "                           'vorticity_x', 'vorticity_y', 'vorticity_z']\n",
    "        \n",
    "        # All variables in x tensor (positions + physics)\n",
    "        self.var_names = self.position_vars + self.physics_vars\n",
    "        \n",
    "        # Global parameters\n",
    "        self.parameter_names = ['reynolds_number']\n",
    "        \n",
    "        # For tracking processed cases\n",
    "        self.case_info = {}\n",
    "        self.all_variable_stats = {}\n",
    "        self.parameter_stats = {}\n",
    "        \n",
    "    def discover_reynolds_cases(self):\n",
    "        \"\"\"Discover all available Reynolds number cases in NEW structure.\"\"\"\n",
    "        print(f\"Discovering Karman vortex cases (NEW STRUCTURE) in: {self.base_data_dir}\")\n",
    "        \n",
    "        if not self.base_data_dir.exists():\n",
    "            print(f\"Directory {self.base_data_dir} does not exist!\")\n",
    "            return []\n",
    "        \n",
    "        case_dirs = []\n",
    "        \n",
    "        # Find all Reynolds_XX directories at the base level\n",
    "        reynolds_dirs = sorted([d for d in self.base_data_dir.iterdir() \n",
    "                               if d.is_dir() and d.name.startswith('Reynolds_')])\n",
    "        \n",
    "        print(f\"Found {len(reynolds_dirs)} Reynolds directories\")\n",
    "        \n",
    "        for re_dir in reynolds_dirs:\n",
    "            # Extract Reynolds number from directory name\n",
    "            re_match = re.search(r'Reynolds_(\\d+)', re_dir.name)\n",
    "            if not re_match:\n",
    "                print(f\"  {re_dir.name}: Could not extract Reynolds number\")\n",
    "                continue\n",
    "                \n",
    "            reynolds_num = int(re_match.group(1))\n",
    "            \n",
    "            # NEW STRUCTURE: Look for double-nested Reynolds directory\n",
    "            inner_re_dir = re_dir / re_dir.name\n",
    "            \n",
    "            if not inner_re_dir.exists():\n",
    "                print(f\"  {re_dir.name}: No double-nested directory found\")\n",
    "                continue\n",
    "            \n",
    "            # Look for VTK directory\n",
    "            vtk_dir = inner_re_dir / \"VTK\"\n",
    "            \n",
    "            if not vtk_dir.exists():\n",
    "                print(f\"  {re_dir.name}: No VTK directory found\")\n",
    "                continue\n",
    "            \n",
    "            # Look for timestep directories (they have Reynolds prefix)\n",
    "            timestep_dirs = [d for d in vtk_dir.iterdir() if d.is_dir()]\n",
    "            \n",
    "            if not timestep_dirs:\n",
    "                print(f\"  {re_dir.name}: No timestep directories in VTK\")\n",
    "                continue\n",
    "            \n",
    "            # Check for internal.vtu files in timestep directories\n",
    "            valid_timesteps = []\n",
    "            for ts_dir in timestep_dirs:\n",
    "                internal_file = ts_dir / \"internal.vtu\"\n",
    "                if internal_file.exists():\n",
    "                    # Extract timestep number from directory name\n",
    "                    ts_match = re.search(rf'Reynolds_{reynolds_num}_(\\d+)', ts_dir.name)\n",
    "                    if ts_match:\n",
    "                        timestep = int(ts_match.group(1))\n",
    "                        valid_timesteps.append((timestep, ts_dir))\n",
    "                    else:\n",
    "                        # Try to extract just a number if the pattern doesn't match\n",
    "                        ts_match = re.search(r'(\\d+)$', ts_dir.name)\n",
    "                        if ts_match:\n",
    "                            timestep = int(ts_match.group(1))\n",
    "                            valid_timesteps.append((timestep, ts_dir))\n",
    "            \n",
    "            if valid_timesteps:\n",
    "                valid_timesteps.sort()  # Sort by timestep\n",
    "                case_name = f\"Reynolds_{reynolds_num}\"\n",
    "                case_dirs.append((vtk_dir, case_name, reynolds_num, valid_timesteps))\n",
    "                print(f\"  {case_name}: {len(valid_timesteps)} timesteps\")\n",
    "            else:\n",
    "                print(f\"  {re_dir.name}: No valid internal.vtu files found\")\n",
    "        \n",
    "        if not case_dirs:\n",
    "            print(\"No valid Reynolds cases found!\")\n",
    "        else:\n",
    "            print(f\"Total valid Reynolds cases: {len(case_dirs)}\")\n",
    "            \n",
    "        return case_dirs\n",
    "    \n",
    "    def read_vtu_data(self, filename):\n",
    "        \"\"\"Read VTU file and extract mesh data and fields.\"\"\"\n",
    "        reader = vtk.vtkXMLUnstructuredGridReader()\n",
    "        reader.SetFileName(str(filename))\n",
    "        reader.Update()\n",
    "        mesh = reader.GetOutput()\n",
    "        \n",
    "        # Extract positions\n",
    "        points = mesh.GetPoints()\n",
    "        pos = np.array([points.GetPoint(i) for i in range(points.GetNumberOfPoints())])\n",
    "        \n",
    "        # Extract point data (physics fields)\n",
    "        point_data = mesh.GetPointData()\n",
    "        fields = {}\n",
    "        \n",
    "        for i in range(point_data.GetNumberOfArrays()):\n",
    "            array = point_data.GetArray(i)\n",
    "            name = array.GetName()\n",
    "            n_components = array.GetNumberOfComponents()\n",
    "            \n",
    "            if name == 'p':  # Pressure field\n",
    "                data = np.array([array.GetValue(j) for j in range(array.GetNumberOfTuples())])\n",
    "                fields['pressure'] = data\n",
    "            elif name == 'U':  # Velocity vector field\n",
    "                data = np.array([array.GetTuple(j) for j in range(array.GetNumberOfTuples())])\n",
    "                fields['velocity_x'] = data[:, 0]\n",
    "                fields['velocity_y'] = data[:, 1] \n",
    "                fields['velocity_z'] = data[:, 2]\n",
    "            elif name == 'vorticity':  # Vorticity vector field\n",
    "                data = np.array([array.GetTuple(j) for j in range(array.GetNumberOfTuples())])\n",
    "                fields['vorticity_x'] = data[:, 0]\n",
    "                fields['vorticity_y'] = data[:, 1]\n",
    "                fields['vorticity_z'] = data[:, 2]\n",
    "            elif n_components == 1:\n",
    "                # Generic scalar field\n",
    "                data = np.array([array.GetValue(j) for j in range(array.GetNumberOfTuples())])\n",
    "                fields[name] = data\n",
    "            else:\n",
    "                # Generic vector field\n",
    "                data = np.array([array.GetTuple(j) for j in range(array.GetNumberOfTuples())])\n",
    "                for comp in range(n_components):\n",
    "                    fields[f\"{name}_{comp}\"] = data[:, comp]\n",
    "        \n",
    "        return pos, fields, mesh\n",
    "    \n",
    "    def extract_mesh_connectivity(self, mesh):\n",
    "        \"\"\"Extract mesh connectivity for graph structure.\"\"\"\n",
    "        edge_set = set()\n",
    "        n_cells = mesh.GetNumberOfCells()\n",
    "        \n",
    "        for i in range(n_cells):\n",
    "            cell = mesh.GetCell(i)\n",
    "            cell_type = cell.GetCellType()\n",
    "            n_points = cell.GetNumberOfPoints()\n",
    "            \n",
    "            point_ids = [cell.GetPointId(j) for j in range(n_points)]\n",
    "            edges = self._get_cell_edges(cell_type, point_ids)\n",
    "            \n",
    "            for edge in edges:\n",
    "                edge_set.add(tuple(sorted(edge)))\n",
    "        \n",
    "        # Convert to directed edges\n",
    "        directed_edges = []\n",
    "        for edge in edge_set:\n",
    "            directed_edges.append([edge[0], edge[1]])\n",
    "            directed_edges.append([edge[1], edge[0]])\n",
    "        \n",
    "        return np.array(directed_edges).T if directed_edges else np.array([[], []])\n",
    "    \n",
    "    def _get_cell_edges(self, cell_type, point_ids):\n",
    "        \"\"\"Get edges for different VTK cell types.\"\"\"\n",
    "        edges = []\n",
    "        \n",
    "        if cell_type == 12:  # Hexahedron\n",
    "            edges = [\n",
    "                # Bottom face\n",
    "                [point_ids[0], point_ids[1]], [point_ids[1], point_ids[2]], \n",
    "                [point_ids[2], point_ids[3]], [point_ids[3], point_ids[0]],\n",
    "                # Top face  \n",
    "                [point_ids[4], point_ids[5]], [point_ids[5], point_ids[6]], \n",
    "                [point_ids[6], point_ids[7]], [point_ids[7], point_ids[4]],\n",
    "                # Vertical edges\n",
    "                [point_ids[0], point_ids[4]], [point_ids[1], point_ids[5]], \n",
    "                [point_ids[2], point_ids[6]], [point_ids[3], point_ids[7]]\n",
    "            ]\n",
    "        elif cell_type == 13:  # Wedge\n",
    "            edges = [\n",
    "                [point_ids[0], point_ids[1]], [point_ids[1], point_ids[2]], [point_ids[2], point_ids[0]],\n",
    "                [point_ids[3], point_ids[4]], [point_ids[4], point_ids[5]], [point_ids[5], point_ids[3]],\n",
    "                [point_ids[0], point_ids[3]], [point_ids[1], point_ids[4]], [point_ids[2], point_ids[5]]\n",
    "            ]\n",
    "        else:  # Other types - connect all pairs (simplified)\n",
    "            for i in range(len(point_ids)):\n",
    "                for j in range(i + 1, len(point_ids)):\n",
    "                    edges.append([point_ids[i], point_ids[j]])\n",
    "        \n",
    "        return edges\n",
    "    \n",
    "    def create_global_parameters(self, reynolds_number):\n",
    "        \"\"\"Create global parameter tensor for the graph.\"\"\"\n",
    "        if not self.include_parameters_as_features:\n",
    "            return None\n",
    "        \n",
    "        global_params = torch.tensor([float(reynolds_number)], dtype=torch.float)\n",
    "        return global_params\n",
    "    \n",
    "    def analyze_variable_ranges_single_case(self, case_dir, case_name, reynolds_number, timesteps):\n",
    "        \"\"\"Analyze variable ranges for a single Reynolds case.\"\"\"\n",
    "        print(f\"  Analyzing variable ranges for {case_name}...\")\n",
    "        \n",
    "        case_stats = {var: {'min': float('inf'), 'max': float('-inf'), \n",
    "                           'all_values': []} for var in self.var_names}\n",
    "        \n",
    "        valid_files_count = 0\n",
    "        \n",
    "        for timestep, ts_dir in timesteps[:10]:  # Sample first 10 timesteps for efficiency\n",
    "            internal_file = ts_dir / \"internal.vtu\"\n",
    "            try:\n",
    "                pos, fields, mesh = self.read_vtu_data(internal_file)\n",
    "                \n",
    "                valid_files_count += 1\n",
    "                \n",
    "                # Analyze positions\n",
    "                for i, pos_var in enumerate(self.position_vars):\n",
    "                    data = pos[:, i]\n",
    "                    valid_data = data[np.isfinite(data)]\n",
    "                    if len(valid_data) > 0:\n",
    "                        case_stats[pos_var]['min'] = min(case_stats[pos_var]['min'], valid_data.min())\n",
    "                        case_stats[pos_var]['max'] = max(case_stats[pos_var]['max'], valid_data.max())\n",
    "                        case_stats[pos_var]['all_values'].extend(valid_data.flatten())\n",
    "                \n",
    "                # Analyze physics variables\n",
    "                for var in self.physics_vars:\n",
    "                    if var in fields:\n",
    "                        data = fields[var]\n",
    "                        valid_data = data[np.isfinite(data)]\n",
    "                        if len(valid_data) > 0:\n",
    "                            case_stats[var]['min'] = min(case_stats[var]['min'], valid_data.min())\n",
    "                            case_stats[var]['max'] = max(case_stats[var]['max'], valid_data.max())\n",
    "                            case_stats[var]['all_values'].extend(valid_data.flatten())\n",
    "                \n",
    "            except Exception as e:\n",
    "                continue  # Skip problematic files silently\n",
    "        \n",
    "        if valid_files_count == 0:\n",
    "            print(f\"    Warning: No valid files could be read for case {case_name}\")\n",
    "            return None\n",
    "        \n",
    "        # Compute additional statistics\n",
    "        final_stats = {}\n",
    "        for var in self.var_names:\n",
    "            if case_stats[var]['all_values'] and case_stats[var]['min'] != float('inf'):\n",
    "                values = np.array(case_stats[var]['all_values'])\n",
    "                final_stats[var] = {\n",
    "                    'min': float(case_stats[var]['min']),\n",
    "                    'max': float(case_stats[var]['max']),\n",
    "                    'mean': float(values.mean()),\n",
    "                    'std': float(values.std()),\n",
    "                    'median': float(np.median(values)),\n",
    "                    'q25': float(np.percentile(values, 25)),\n",
    "                    'q75': float(np.percentile(values, 75))\n",
    "                }\n",
    "        \n",
    "        return final_stats if final_stats else None\n",
    "    \n",
    "    def process_single_case(self, case_dir, case_name, reynolds_number, timesteps, output_path):\n",
    "        \"\"\"Process a single Reynolds case into PyG format - SHOCK TUBE FORMAT.\"\"\"\n",
    "        print(f\"Processing case: {case_name}\")\n",
    "        print(f\"  Reynolds number: {reynolds_number}\")\n",
    "        print(f\"  Timesteps: {len(timesteps)}\")\n",
    "        \n",
    "        if len(timesteps) < 2:\n",
    "            print(f\"  Case {case_name} has insufficient timesteps ({len(timesteps)}). Skipping.\")\n",
    "            return None\n",
    "        \n",
    "        # Store parameter statistics\n",
    "        self.parameter_stats[case_name] = {\n",
    "            'reynolds_number': reynolds_number\n",
    "        }\n",
    "        \n",
    "        # Analyze variable ranges for this case\n",
    "        case_var_stats = self.analyze_variable_ranges_single_case(case_dir, case_name, reynolds_number, timesteps)\n",
    "        \n",
    "        pyg_data_list = []\n",
    "        \n",
    "        # Read first timestep to get mesh structure (static across timesteps)\n",
    "        first_timestep, first_ts_dir = timesteps[0]\n",
    "        first_internal = first_ts_dir / \"internal.vtu\"\n",
    "        pos, _, mesh = self.read_vtu_data(first_internal)\n",
    "        \n",
    "        # Extract connectivity (static)\n",
    "        edge_index = self.extract_mesh_connectivity(mesh)\n",
    "        edge_index_tensor = torch.tensor(edge_index, dtype=torch.long)\n",
    "        \n",
    "        # Create global parameter features\n",
    "        global_params = self.create_global_parameters(reynolds_number)\n",
    "        \n",
    "        # Process consecutive timestep pairs\n",
    "        valid_pairs = 0\n",
    "        total_pairs = len(timesteps) - 1\n",
    "        \n",
    "        for i in range(total_pairs):\n",
    "            if i % 50 == 0:\n",
    "                print(f\"    Processing timestep pairs: {i}/{total_pairs}\")\n",
    "                \n",
    "            current_timestep, current_ts_dir = timesteps[i]\n",
    "            next_timestep, next_ts_dir = timesteps[i + 1]\n",
    "            \n",
    "            try:\n",
    "                # Read current and next timestep data\n",
    "                current_file = current_ts_dir / \"internal.vtu\"\n",
    "                next_file = next_ts_dir / \"internal.vtu\"\n",
    "                \n",
    "                current_pos, current_fields, _ = self.read_vtu_data(current_file)\n",
    "                next_pos, next_fields, _ = self.read_vtu_data(next_file)\n",
    "                \n",
    "                # Create feature arrays - COMBINE POSITIONS + PHYSICS (like shock tube)\n",
    "                available_vars = [var for var in self.physics_vars if var in current_fields]\n",
    "                \n",
    "                # Build current features: [positions | physics]\n",
    "                current_features_list = [current_pos]  # Start with positions\n",
    "                for var in available_vars:\n",
    "                    current_features_list.append(current_fields[var].reshape(-1, 1))\n",
    "                current_features = np.hstack(current_features_list)\n",
    "                \n",
    "                # Build next features (targets are physics only, like shock tube)\n",
    "                next_features = np.stack([next_fields[var] for var in available_vars], axis=-1)\n",
    "                \n",
    "                # Create PyG tensors\n",
    "                x = torch.tensor(current_features, dtype=torch.float)  # [N, 3 positions + 7 physics]\n",
    "                y = torch.tensor(next_features, dtype=torch.float)     # [N, 7 physics only]\n",
    "                \n",
    "                # Also store positions separately (for compatibility)\n",
    "                pos_tensor = torch.tensor(current_pos, dtype=torch.float)\n",
    "                \n",
    "                # Create PyG Data object\n",
    "                data_object = Data(\n",
    "                    x=x,                              # Combined: positions + physics features\n",
    "                    pos=pos_tensor,                   # Also store positions separately\n",
    "                    edge_index=edge_index_tensor,     # Static connectivity\n",
    "                    y=y,                             # Target (next timestep physics only)\n",
    "                    \n",
    "                    # Global parameters\n",
    "                    global_params=global_params if self.include_parameters_as_features else None,\n",
    "                    \n",
    "                    # Metadata\n",
    "                    case_name=case_name,\n",
    "                    reynolds_number=reynolds_number,\n",
    "                    timestep_current=current_timestep,\n",
    "                    timestep_next=next_timestep,\n",
    "                    feature_names=available_vars\n",
    "                )\n",
    "                \n",
    "                pyg_data_list.append(data_object)\n",
    "                valid_pairs += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                continue  # Skip problematic timestep pairs silently\n",
    "        \n",
    "        print(f\"  Successfully processed {valid_pairs} timestep pairs\")\n",
    "        \n",
    "        if pyg_data_list:\n",
    "            # Save this case separately\n",
    "            case_filename = f\"{case_name}_raw_data.pt\"\n",
    "            case_filepath = output_path / \"individual_cases\" / case_filename\n",
    "            case_filepath.parent.mkdir(exist_ok=True)\n",
    "            torch.save(pyg_data_list, case_filepath)\n",
    "            print(f\"  Saved individual case: {case_filepath}\")\n",
    "        \n",
    "        # Store case information\n",
    "        self.case_info[case_name] = {\n",
    "            'total_timesteps': len(timesteps),\n",
    "            'processed_pairs': len(pyg_data_list),\n",
    "            'directory': str(case_dir),\n",
    "            'reynolds_number': reynolds_number,\n",
    "            'variable_statistics': case_var_stats,\n",
    "            'feature_dimensions': {\n",
    "                'node_features_total': len(self.position_vars) + len(available_vars) if pyg_data_list else 0,\n",
    "                'position_features': len(self.position_vars),\n",
    "                'physics_features': len(available_vars) if pyg_data_list else 0,\n",
    "                'global_features': 1 if self.include_parameters_as_features else 0,\n",
    "            },\n",
    "            'saved_file': str(case_filepath) if pyg_data_list else None\n",
    "        }\n",
    "        \n",
    "        return pyg_data_list\n",
    "    \n",
    "    def analyze_global_variable_ranges(self, case_info_list):\n",
    "        \"\"\"Analyze variable ranges across all Reynolds cases.\"\"\"\n",
    "        print(\"Analyzing variable ranges across all Reynolds cases\")\n",
    "        \n",
    "        global_stats = {var: {'min': float('inf'), 'max': float('-inf'), \n",
    "                             'case_mins': [], 'case_maxs': [], 'case_means': []} \n",
    "                       for var in self.var_names}\n",
    "        \n",
    "        case_stats_summary = {}\n",
    "        processed_cases = 0\n",
    "        \n",
    "        for case_dir, case_name, reynolds_number, timesteps in case_info_list:\n",
    "            case_var_stats = self.analyze_variable_ranges_single_case(case_dir, case_name, reynolds_number, timesteps)\n",
    "            \n",
    "            if case_var_stats is not None and case_var_stats:\n",
    "                case_stats_summary[case_name] = case_var_stats\n",
    "                processed_cases += 1\n",
    "                \n",
    "                for var in self.var_names:\n",
    "                    if var in case_var_stats and 'mean' in case_var_stats[var]:\n",
    "                        stats = case_var_stats[var]\n",
    "                        global_stats[var]['min'] = min(global_stats[var]['min'], stats['min'])\n",
    "                        global_stats[var]['max'] = max(global_stats[var]['max'], stats['max'])\n",
    "                        global_stats[var]['case_mins'].append(stats['min'])\n",
    "                        global_stats[var]['case_maxs'].append(stats['max'])\n",
    "                        global_stats[var]['case_means'].append(stats['mean'])\n",
    "        \n",
    "        # Compute global statistics\n",
    "        for var in self.var_names:\n",
    "            if global_stats[var]['case_means']:\n",
    "                global_stats[var]['global_mean_of_means'] = np.mean(global_stats[var]['case_means'])\n",
    "                global_stats[var]['std_of_means'] = np.std(global_stats[var]['case_means'])\n",
    "                global_stats[var]['range'] = global_stats[var]['max'] - global_stats[var]['min']\n",
    "        \n",
    "        print(f\"Global variable statistics summary (from {processed_cases} cases):\")\n",
    "        for var in self.var_names:\n",
    "            stats = global_stats[var]\n",
    "            if stats['case_means']:\n",
    "                print(f\"  {var.upper()}:\")\n",
    "                print(f\"    Global Range: [{stats['min']:.6e}, {stats['max']:.6e}]\")\n",
    "                print(f\"    Range Span: {stats['range']:.6e}\")\n",
    "                print(f\"    Mean across cases: {stats['global_mean_of_means']:.6e} 췀 {stats['std_of_means']:.6e}\")\n",
    "        \n",
    "        self.all_variable_stats = {\n",
    "            'global_statistics': global_stats,\n",
    "            'case_statistics': case_stats_summary\n",
    "        }\n",
    "        \n",
    "        return global_stats, case_stats_summary\n",
    "    \n",
    "    def process_all_cases(self, output_dir=\"processed_karman_new_structure\"):\n",
    "        \"\"\"Process all discovered Reynolds cases.\"\"\"\n",
    "        print(\"BATCH PROCESSING ALL KARMAN VORTEX CASES - NEW STRUCTURE - SHOCK TUBE FORMAT\")\n",
    "        \n",
    "        case_info_list = self.discover_reynolds_cases()\n",
    "        if not case_info_list:\n",
    "            print(\"No cases found to process!\")\n",
    "            return None\n",
    "        \n",
    "        # First, analyze variable ranges across all cases\n",
    "        global_stats, case_stats = self.analyze_global_variable_ranges(case_info_list)\n",
    "        \n",
    "        if global_stats is None:\n",
    "            print(\"Variable range analysis failed!\")\n",
    "            return None\n",
    "        \n",
    "        # Create output directory\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(exist_ok=True)\n",
    "        (output_path / \"individual_cases\").mkdir(exist_ok=True)\n",
    "        \n",
    "        all_data = []\n",
    "        case_sample_counts = {}\n",
    "        reynolds_numbers = []\n",
    "        \n",
    "        print(\"Processing individual Reynolds cases:\")\n",
    "        \n",
    "        # Process each case\n",
    "        for case_dir, case_name, reynolds_number, timesteps in case_info_list:\n",
    "            case_data = self.process_single_case(case_dir, case_name, reynolds_number, timesteps, output_path)\n",
    "            \n",
    "            if case_data:\n",
    "                all_data.extend(case_data)\n",
    "                case_sample_counts[case_name] = len(case_data)\n",
    "                reynolds_numbers.append(reynolds_number)\n",
    "            else:\n",
    "                print(f\"  No data processed for case: {case_name}\")\n",
    "        \n",
    "        if not all_data:\n",
    "            print(\"No data was successfully processed!\")\n",
    "            return None\n",
    "        \n",
    "        # Calculate Reynolds number statistics\n",
    "        reynolds_stats = {\n",
    "            'min': min(reynolds_numbers),\n",
    "            'max': max(reynolds_numbers),\n",
    "            'unique_values': sorted(list(set(reynolds_numbers)))\n",
    "        }\n",
    "        \n",
    "        print(\"Processing summary:\")\n",
    "        print(f\"  Total Reynolds cases processed: {len(case_sample_counts)}\")\n",
    "        print(f\"  Total samples: {len(all_data)}\")\n",
    "        print(f\"  Reynolds numbers: {reynolds_stats['unique_values']}\")\n",
    "        print(f\"  x.shape: [{all_data[0].x.shape[0]}, {all_data[0].x.shape[1]}] (nodes, features)\")\n",
    "        print(f\"    - First 3 columns: positions (x, y, z)\")\n",
    "        print(f\"    - Next 7 columns: physics (pressure, velocity_x/y/z, vorticity_x/y/z)\")\n",
    "        print(f\"  y.shape: [{all_data[0].y.shape[0]}, {all_data[0].y.shape[1]}] (nodes, physics only)\")\n",
    "        print(f\"  global_params.shape: [{all_data[0].global_params.shape[0]}] (Reynolds number)\")\n",
    "        \n",
    "        # Save comprehensive metadata\n",
    "        metadata = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'data_structure': 'NEW - Double-nested Reynolds directories',\n",
    "            'data_format': 'SHOCK_TUBE_COMPATIBLE - positions + physics in x',\n",
    "            'data_processing': 'RAW_DATA_ONLY - No derived features',\n",
    "            'total_samples': len(all_data),\n",
    "            'total_cases': len(case_sample_counts),\n",
    "            'include_parameters_as_features': self.include_parameters_as_features,\n",
    "            'parameter_storage_method': 'global_graph_attributes' if self.include_parameters_as_features else None,\n",
    "            'parameter_names': self.parameter_names if self.include_parameters_as_features else [],\n",
    "            'case_info': self.case_info,\n",
    "            'case_sample_counts': case_sample_counts,\n",
    "            'parameter_statistics': self.parameter_stats,\n",
    "            'variable_statistics': self.all_variable_stats,\n",
    "            'reynolds_summary': reynolds_stats,\n",
    "            'feature_layout': self._get_feature_layout_description(),\n",
    "            'target_layout': self.physics_vars,\n",
    "            'physics_model': 'Incompressible Navier-Stokes with Karman vortex shedding',\n",
    "            'domain_description': 'Flow around cylinder',\n",
    "            'mesh_type': 'Unstructured VTK mesh'\n",
    "        }\n",
    "        \n",
    "        with open(output_path / 'karman_dataset_metadata.json', 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        # Save variable statistics separately\n",
    "        with open(output_path / 'variable_statistics.json', 'w') as f:\n",
    "            json.dump(self.all_variable_stats, f, indent=2)\n",
    "        \n",
    "        return all_data, output_path\n",
    "    \n",
    "    def create_train_test_val_split(self, all_data, output_path, \n",
    "                                  train_ratio=0.7, val_ratio=0.15, test_ratio=0.15,\n",
    "                                  random_state=42, split_by_reynolds=True):\n",
    "        \"\"\"Create train/test/validation splits by Reynolds numbers for extrapolation testing.\"\"\"\n",
    "        print(\"Creating train/test/val split\")\n",
    "        print(f\"  Ratios - Train: {train_ratio}, Val: {val_ratio}, Test: {test_ratio}\")\n",
    "        \n",
    "        if abs(train_ratio + val_ratio + test_ratio - 1.0) > 1e-6:\n",
    "            print(\"Split ratios don't sum to 1.0!\")\n",
    "            return None\n",
    "        \n",
    "        # Group data by Reynolds number\n",
    "        reynolds_groups = {}\n",
    "        for data in all_data:\n",
    "            re_num = data.reynolds_number\n",
    "            if re_num not in reynolds_groups:\n",
    "                reynolds_groups[re_num] = []\n",
    "            reynolds_groups[re_num].append(data)\n",
    "        \n",
    "        print(f\"  Found {len(reynolds_groups)} unique Reynolds numbers\")\n",
    "        reynolds_list = sorted(reynolds_groups.keys())\n",
    "        print(f\"  Reynolds numbers: {reynolds_list}\")\n",
    "        \n",
    "        if split_by_reynolds:\n",
    "            # Split by Reynolds number for physics extrapolation testing\n",
    "            n_reynolds = len(reynolds_list)\n",
    "            train_end = int(n_reynolds * train_ratio)\n",
    "            val_end = train_end + int(n_reynolds * val_ratio)\n",
    "            \n",
    "            train_reynolds = reynolds_list[:train_end]\n",
    "            val_reynolds = reynolds_list[train_end:val_end]\n",
    "            test_reynolds = reynolds_list[val_end:]\n",
    "            \n",
    "            print(f\"  Reynolds split assignment:\")\n",
    "            print(f\"    Train: {train_reynolds}\")\n",
    "            print(f\"    Val:   {val_reynolds}\")\n",
    "            print(f\"    Test:  {test_reynolds}\")\n",
    "            \n",
    "            # Assign ALL samples from each Reynolds to designated split\n",
    "            train_data = []\n",
    "            val_data = []\n",
    "            test_data = []\n",
    "            \n",
    "            for re_num in train_reynolds:\n",
    "                train_data.extend(reynolds_groups[re_num])\n",
    "            for re_num in val_reynolds:\n",
    "                val_data.extend(reynolds_groups[re_num])\n",
    "            for re_num in test_reynolds:\n",
    "                test_data.extend(reynolds_groups[re_num])\n",
    "        else:\n",
    "            # Random split by individual samples\n",
    "            train_data, temp_data = train_test_split(all_data, test_size=(1-train_ratio), random_state=random_state)\n",
    "            val_data, test_data = train_test_split(temp_data, test_size=(test_ratio/(val_ratio+test_ratio)), random_state=random_state)\n",
    "        \n",
    "        print(f\"Final split sizes:\")\n",
    "        print(f\"  Train: {len(train_data)} samples ({len(train_data)/len(all_data)*100:.1f}%)\")\n",
    "        print(f\"  Val:   {len(val_data)} samples ({len(val_data)/len(all_data)*100:.1f}%)\")\n",
    "        print(f\"  Test:  {len(test_data)} samples ({len(test_data)/len(all_data)*100:.1f}%)\")\n",
    "        \n",
    "        # Save the datasets\n",
    "        datasets = {\n",
    "            'train': train_data,\n",
    "            'val': val_data,\n",
    "            'test': test_data\n",
    "        }\n",
    "        \n",
    "        saved_files = {}\n",
    "        for split_name, split_data in datasets.items():\n",
    "            filename = f\"karman_vortex_{split_name}_with_pos.pt\"  # Match shock tube naming\n",
    "            filepath = output_path / filename\n",
    "            torch.save(split_data, filepath)\n",
    "            saved_files[split_name] = str(filepath)\n",
    "            print(f\"  Saved {split_name} split: {filepath}\")\n",
    "        \n",
    "        # Update metadata with split information\n",
    "        split_metadata = {\n",
    "            'split_method': 'reynolds_numbers' if split_by_reynolds else 'random_samples',\n",
    "            'split_ratios': {'train': train_ratio, 'val': val_ratio, 'test': test_ratio},\n",
    "            'split_sizes': {\n",
    "                'train': len(train_data), \n",
    "                'val': len(val_data), \n",
    "                'test': len(test_data)\n",
    "            },\n",
    "            'reynolds_assignment': {\n",
    "                'train_reynolds': train_reynolds if split_by_reynolds else 'mixed',\n",
    "                'val_reynolds': val_reynolds if split_by_reynolds else 'mixed',\n",
    "                'test_reynolds': test_reynolds if split_by_reynolds else 'mixed'\n",
    "            },\n",
    "            'random_state': random_state,\n",
    "            'saved_files': saved_files\n",
    "        }\n",
    "        \n",
    "        # Load existing metadata and add split info\n",
    "        metadata_file = output_path / 'karman_dataset_metadata.json'\n",
    "        with open(metadata_file, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        metadata['train_test_val_split'] = split_metadata\n",
    "        \n",
    "        with open(metadata_file, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"Updated metadata: {metadata_file}\")\n",
    "        \n",
    "        return datasets\n",
    "    \n",
    "    def _get_feature_layout_description(self):\n",
    "        \"\"\"Get description of the feature layout.\"\"\"\n",
    "        if self.include_parameters_as_features:\n",
    "            return {\n",
    "                'x_tensor': '[x_pos, y_pos, z_pos, pressure, velocity_x, velocity_y, velocity_z, vorticity_x, vorticity_y, vorticity_z] (positions + physics)',\n",
    "                'y_tensor': '[pressure, velocity_x, velocity_y, velocity_z, vorticity_x, vorticity_y, vorticity_z] (physics only)',\n",
    "                'global_params': '[reynolds_number]',\n",
    "                'num_static_feats': 3,\n",
    "                'num_dynamic_feats': 7,\n",
    "                'num_global_feats': 1\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'x_tensor': '[x_pos, y_pos, z_pos, pressure, velocity_x, velocity_y, velocity_z, vorticity_x, vorticity_y, vorticity_z] (positions + physics)',\n",
    "                'y_tensor': '[pressure, velocity_x, velocity_y, velocity_z, vorticity_x, vorticity_y, vorticity_z] (physics only)',\n",
    "                'global_params': 'none'\n",
    "            }\n",
    "\n",
    "\n",
    "class KarmanDatasetNormalizerNewStructure:\n",
    "    \"\"\"\n",
    "    Normalizer for Karman vortex street datasets - matches shock tube format.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, source_dir, target_dir, metadata_file=\"karman_dataset_metadata.json\"):\n",
    "        self.source_dir = Path(source_dir)\n",
    "        self.target_dir = Path(target_dir)\n",
    "        self.target_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Variable names - positions + physics (like shock tube)\n",
    "        self.position_vars = ['x_pos', 'y_pos', 'z_pos']\n",
    "        self.physics_vars = ['pressure', 'velocity_x', 'velocity_y', 'velocity_z', \n",
    "                           'vorticity_x', 'vorticity_y', 'vorticity_z']\n",
    "        self.var_names = self.position_vars + self.physics_vars\n",
    "        \n",
    "        # Load metadata\n",
    "        metadata_path = self.source_dir / metadata_file\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            self.metadata = json.load(f)\n",
    "        \n",
    "        # Extract normalization parameters\n",
    "        self.normalization_params = self._compute_normalization_params()\n",
    "        self.global_param_normalization = self._compute_global_param_normalization()\n",
    "        \n",
    "    def _compute_normalization_params(self):\n",
    "        \"\"\"Compute global min/max for each variable from metadata.\"\"\"\n",
    "        print(\"Computing global normalization parameters...\")\n",
    "        \n",
    "        params = {}\n",
    "        global_stats = self.metadata['variable_statistics']['global_statistics']\n",
    "        \n",
    "        for var in self.var_names:\n",
    "            if var in global_stats:\n",
    "                var_min = global_stats[var]['min']\n",
    "                var_max = global_stats[var]['max']\n",
    "                params[var] = {'min': var_min, 'max': var_max}\n",
    "                print(f\"  {var}: [{var_min:.6e}, {var_max:.6e}]\")\n",
    "            else:\n",
    "                print(f\"  Warning: {var} not found in global statistics\")\n",
    "        \n",
    "        return params\n",
    "    \n",
    "    def _compute_global_param_normalization(self):\n",
    "        \"\"\"Compute normalization for global parameters (Reynolds number).\"\"\"\n",
    "        print(\"Computing global parameter normalization...\")\n",
    "        \n",
    "        if 'reynolds_summary' in self.metadata:\n",
    "            reynolds_stats = self.metadata['reynolds_summary']\n",
    "            return {\n",
    "                'reynolds_number': {\n",
    "                    'min': float(reynolds_stats['min']),\n",
    "                    'max': float(reynolds_stats['max'])\n",
    "                }\n",
    "            }\n",
    "        return {}\n",
    "    \n",
    "    def normalize_tensor(self, data, var_name, param_dict=None):\n",
    "        \"\"\"Normalize a tensor using min-max normalization.\"\"\"\n",
    "        if param_dict is None:\n",
    "            param_dict = self.normalization_params\n",
    "        \n",
    "        if var_name not in param_dict:\n",
    "            print(f\"Warning: {var_name} not found in normalization parameters\")\n",
    "            return data.clone()\n",
    "        \n",
    "        params = param_dict[var_name]\n",
    "        var_min, var_max = params['min'], params['max']\n",
    "        \n",
    "        if var_max == var_min:\n",
    "            return torch.zeros_like(data)\n",
    "        \n",
    "        normalized = (data - var_min) / (var_max - var_min)\n",
    "        normalized = torch.clamp(normalized, 0.0, 1.0)\n",
    "        \n",
    "        return normalized\n",
    "    \n",
    "    def normalize_global_params(self, global_params):\n",
    "        \"\"\"Normalize global parameters (Reynolds number).\"\"\"\n",
    "        if global_params is None or len(self.global_param_normalization) == 0:\n",
    "            return global_params.clone() if global_params is not None else None\n",
    "        \n",
    "        normalized_params = global_params.clone()\n",
    "        \n",
    "        if 'reynolds_number' in self.global_param_normalization:\n",
    "            normalized_params[0] = self.normalize_tensor(\n",
    "                global_params[0], 'reynolds_number', self.global_param_normalization\n",
    "            )\n",
    "        \n",
    "        return normalized_params\n",
    "    \n",
    "    def normalize_sample(self, data):\n",
    "        \"\"\"Normalize a single PyG Data object.\"\"\"\n",
    "        # Normalize x (positions + physics)\n",
    "        x_normalized = data.x.clone()\n",
    "        for i, var_name in enumerate(self.var_names):\n",
    "            if i < data.x.shape[1]:\n",
    "                x_normalized[:, i] = self.normalize_tensor(data.x[:, i], var_name)\n",
    "        \n",
    "        # Normalize y (physics only)\n",
    "        y_normalized = data.y.clone()\n",
    "        for i, var_name in enumerate(self.physics_vars):\n",
    "            if i < data.y.shape[1]:\n",
    "                y_normalized[:, i] = self.normalize_tensor(data.y[:, i], var_name)\n",
    "        \n",
    "        # Normalize pos separately (for compatibility)\n",
    "        pos_normalized = data.pos.clone()\n",
    "        for i, var_name in enumerate(self.position_vars):\n",
    "            if i < data.pos.shape[1]:\n",
    "                pos_normalized[:, i] = self.normalize_tensor(data.pos[:, i], var_name)\n",
    "        \n",
    "        # Create normalized data object\n",
    "        normalized_data = data.clone()\n",
    "        normalized_data.x = x_normalized\n",
    "        normalized_data.y = y_normalized\n",
    "        normalized_data.pos = pos_normalized\n",
    "        \n",
    "        # Normalize global parameters\n",
    "        if hasattr(data, 'global_params') and data.global_params is not None:\n",
    "            normalized_data.global_params = self.normalize_global_params(data.global_params)\n",
    "        \n",
    "        return normalized_data\n",
    "    \n",
    "    def normalize_dataset_file(self, input_file, output_file):\n",
    "        \"\"\"Normalize a single dataset file.\"\"\"\n",
    "        print(f\"  Loading: {input_file}\")\n",
    "        dataset = torch.load(input_file, weights_only=False)\n",
    "        \n",
    "        print(f\"  Normalizing {len(dataset)} samples...\")\n",
    "        normalized_dataset = []\n",
    "        \n",
    "        for i, data in enumerate(dataset):\n",
    "            if i % 100 == 0:\n",
    "                print(f\"    Progress: {i}/{len(dataset)}\")\n",
    "            \n",
    "            normalized_data = self.normalize_sample(data)\n",
    "            normalized_dataset.append(normalized_data)\n",
    "        \n",
    "        print(f\"  Saving: {output_file}\")\n",
    "        torch.save(normalized_dataset, output_file)\n",
    "        \n",
    "        return len(normalized_dataset)\n",
    "    \n",
    "    def normalize_all_datasets(self):\n",
    "        \"\"\"Normalize all dataset files.\"\"\"\n",
    "        print(\"NORMALIZING KARMAN VORTEX DATASETS - SHOCK TUBE FORMAT\")\n",
    "        \n",
    "        total_samples = 0\n",
    "        \n",
    "        # Normalize main split files\n",
    "        splits = ['train', 'val', 'test']\n",
    "        for split in splits:\n",
    "            input_file = self.source_dir / f\"karman_vortex_{split}_with_pos.pt\"\n",
    "            output_file = self.target_dir / f\"karman_vortex_{split}_normalized.pt\"\n",
    "            \n",
    "            if input_file.exists():\n",
    "                samples = self.normalize_dataset_file(input_file, output_file)\n",
    "                total_samples += samples\n",
    "                print(f\"  {split}: {samples} samples normalized\")\n",
    "            else:\n",
    "                print(f\"  {split}: File not found\")\n",
    "        \n",
    "        # Normalize individual cases\n",
    "        individual_cases_dir = self.source_dir / \"individual_cases\"\n",
    "        if individual_cases_dir.exists():\n",
    "            normalized_cases_dir = self.target_dir / \"individual_cases_normalized\"\n",
    "            normalized_cases_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            case_files = list(individual_cases_dir.glob(\"*.pt\"))\n",
    "            print(f\"Normalizing {len(case_files)} individual cases...\")\n",
    "            \n",
    "            for case_file in case_files:\n",
    "                output_file = normalized_cases_dir / f\"{case_file.stem}_normalized.pt\"\n",
    "                samples = self.normalize_dataset_file(case_file, output_file)\n",
    "                total_samples += samples\n",
    "        \n",
    "        # Save normalization metadata\n",
    "        norm_metadata = {\n",
    "            'normalization_method': 'min_max_0_to_1',\n",
    "            'data_format': 'shock_tube_compatible',\n",
    "            'normalization_params': self.normalization_params,\n",
    "            'global_param_normalization': self.global_param_normalization,\n",
    "            'original_metadata': self.metadata\n",
    "        }\n",
    "        \n",
    "        with open(self.target_dir / 'normalization_metadata.json', 'w') as f:\n",
    "            json.dump(norm_metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"NORMALIZATION COMPLETE!\")\n",
    "        print(f\"Total samples normalized: {total_samples}\")\n",
    "        \n",
    "        return total_samples\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    BASE_DATA_DIR = \"/standard/sds_baek_energetic/von_karman_vortex/Reynolds 200~\"\n",
    "    OUTPUT_DIR = \"/standard/sds_baek_energetic/von_karman_vortex/Reynolds 200~/processed_parc\"\n",
    "    \n",
    "    print(\"KARMAN VORTEX STREET DATASET PROCESSOR\")\n",
    "    print(\"SHOCK TUBE FORMAT: positions + physics in x tensor\")\n",
    "    print(f\"Source: {BASE_DATA_DIR}\")\n",
    "    print(f\"Output: {OUTPUT_DIR}\")\n",
    "    \n",
    "    # Initialize processor\n",
    "    processor = KarmanVortexProcessorNewStructure(BASE_DATA_DIR)\n",
    "    \n",
    "    # Process all cases\n",
    "    result = processor.process_all_cases(output_dir=OUTPUT_DIR)\n",
    "    \n",
    "    if result is None:\n",
    "        print(\"Processing failed!\")\n",
    "        return\n",
    "    \n",
    "    all_data, output_path = result\n",
    "    \n",
    "    print(\"READY FOR TRAIN/TEST/VAL SPLIT\")\n",
    "    print(\"Variable range analysis complete\")\n",
    "    print(\"Individual simulations saved\")\n",
    "    \n",
    "    # Create train/test/val splits\n",
    "    datasets = processor.create_train_test_val_split(\n",
    "        all_data, \n",
    "        output_path,\n",
    "        train_ratio=0.7,\n",
    "        val_ratio=0.15, \n",
    "        test_ratio=0.15,\n",
    "        split_by_reynolds=True\n",
    "    )\n",
    "    \n",
    "    if datasets:\n",
    "        print(\"NORMALIZING DATASETS\")\n",
    "        normalizer = KarmanDatasetNormalizerNewStructure(output_path, output_path / \"normalized\")\n",
    "        normalizer.normalize_all_datasets()\n",
    "    \n",
    "    print(\"KARMAN VORTEX PROCESSING COMPLETE!\")\n",
    "    print(f\"All outputs saved to: {output_path}\")\n",
    "    print(\"Generated files:\")\n",
    "    print(\"  - karman_vortex_train_with_pos.pt\")\n",
    "    print(\"  - karman_vortex_val_with_pos.pt\")\n",
    "    print(\"  - karman_vortex_test_with_pos.pt\")\n",
    "    print(\"  - normalized/karman_vortex_*_normalized.pt\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338af90b-1082-4f3c-b6e5-bc902214a3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the path to your processed data directory:\n",
      "(Press Enter to use: /standard/sds_baek_energetic/von_karman_vortex/full_data)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ULTRA-FAST STATIC GRAPH ANALYSIS\n",
      "(Only sampling 1 timestep per Reynolds case)\n",
      "================================================================================\n",
      "\n",
      "1. INDIVIDUAL REYNOLDS CASES - GRAPH SIZE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Analyzing individual_cases: 23 files\n",
      "Reynolds     Nodes      Edges      Timesteps    Size (MB)    MB/timestep \n",
      "--------------------------------------------------------------------------------\n",
      "1000         165082     865226     400          5051.5       12.629      \n",
      "100          60746      318178     400          1859.0       4.648       \n",
      "150          60746      318178     400          1859.0       4.648       \n",
      "1            60746      318178     400          1859.0       4.648       \n",
      "200          120696     623052     400          3693.2       9.233       \n",
      "20           60746      318178     400          1859.0       4.648       \n",
      "250          120696     623052     400          3693.2       9.233       \n",
      "300          160556     823104     400          4912.7       12.282      \n",
      "350          120696     623052     400          3693.2       9.233       \n",
      "400          160556     823104     400          4912.7       12.282      \n",
      "40           60746      318178     400          1859.0       4.648       \n",
      "450          160556     823104     400          4912.7       12.282      \n",
      "500          120696     623052     400          3693.2       9.233       \n",
      "50           60746      318178     400          1859.0       4.648       \n",
      "550          120696     623052     400          3693.2       9.233       \n",
      "600          160556     823104     400          4912.7       12.282      \n",
      "650          160556     823104     400          4912.7       12.282      \n",
      "700          160556     823104     400          4912.7       12.282      \n",
      "750          165082     865226     400          5051.5       12.629      \n",
      "800          165082     865226     400          5051.5       12.629      \n",
      "850          165082     865226     400          5051.5       12.629      \n",
      "900          165082     865226     400          5051.5       12.629      \n",
      "950          165082     865226     400          5051.5       12.629      \n",
      "\n",
      "\n",
      "2. SPLIT FILES - SAMPLING STRUCTURE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "TRAIN:\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def ultra_fast_static_graph_analysis(data_dir):\n",
    "    \"\"\"\n",
    "    Ultra-fast analysis for STATIC GRAPHS.\n",
    "    Since mesh doesn't change, we only need 1 sample per Reynolds number.\n",
    "    \"\"\"\n",
    "    \n",
    "    data_path = Path(data_dir)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"ULTRA-FAST STATIC GRAPH ANALYSIS\")\n",
    "    print(\"(Only sampling 1 timestep per Reynolds case)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Part 1: Individual cases - just look at first sample of each\n",
    "    print(\"\\n1. INDIVIDUAL REYNOLDS CASES - GRAPH SIZE\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    individual_dirs = [\n",
    "        data_path / \"individual_cases\",\n",
    "        data_path / \"normalized\" / \"individual_cases_normalized\"\n",
    "    ]\n",
    "    \n",
    "    reynolds_info = {}\n",
    "    \n",
    "    for ind_dir in individual_dirs:\n",
    "        if not ind_dir.exists():\n",
    "            continue\n",
    "        \n",
    "        case_files = sorted(ind_dir.glob(\"*.pt\"))\n",
    "        print(f\"\\nAnalyzing {ind_dir.name}: {len(case_files)} files\")\n",
    "        print(f\"{'Reynolds':<12} {'Nodes':<10} {'Edges':<10} {'Timesteps':<12} {'Size (MB)':<12} {'MB/timestep':<12}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for case_file in case_files:\n",
    "            # Extract Reynolds number\n",
    "            parts = case_file.stem.split('_')\n",
    "            reynolds = None\n",
    "            for i, part in enumerate(parts):\n",
    "                if part == 'Reynolds' and i+1 < len(parts):\n",
    "                    reynolds_str = parts[i+1].replace('dir0', '').replace('dir1', '')\n",
    "                    try:\n",
    "                        reynolds = int(reynolds_str)\n",
    "                    except:\n",
    "                        pass\n",
    "                    break\n",
    "            \n",
    "            if reynolds is None:\n",
    "                continue\n",
    "            \n",
    "            # Get file size\n",
    "            size_mb = case_file.stat().st_size / (1024**2)\n",
    "            \n",
    "            # Load ONLY first sample to get graph structure\n",
    "            try:\n",
    "                dataset = torch.load(case_file, weights_only=False)\n",
    "                n_timesteps = len(dataset)\n",
    "                \n",
    "                if n_timesteps > 0:\n",
    "                    first_sample = dataset[0]\n",
    "                    n_nodes = first_sample.x.shape[0]\n",
    "                    n_edges = first_sample.edge_index.shape[1]\n",
    "                    \n",
    "                    mb_per_timestep = size_mb / n_timesteps\n",
    "                    \n",
    "                    # Store info\n",
    "                    if reynolds not in reynolds_info or 'normalized' not in str(case_file):\n",
    "                        reynolds_info[reynolds] = {\n",
    "                            'nodes': n_nodes,\n",
    "                            'edges': n_edges,\n",
    "                            'timesteps': n_timesteps,\n",
    "                            'size_mb': size_mb,\n",
    "                            'mb_per_timestep': mb_per_timestep,\n",
    "                            'file': case_file.name\n",
    "                        }\n",
    "                    \n",
    "                    print(f\"{reynolds:<12} {n_nodes:<10} {n_edges:<10} {n_timesteps:<12} {size_mb:<12.1f} {mb_per_timestep:<12.3f}\")\n",
    "                    \n",
    "                    # Free memory immediately\n",
    "                    del dataset, first_sample\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {case_file.name}: {e}\")\n",
    "    \n",
    "    # Part 2: Analyze split files - sample first few\n",
    "    print(\"\\n\\n2. SPLIT FILES - SAMPLING STRUCTURE\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    split_files = ['train', 'val', 'test']\n",
    "    split_info = {}\n",
    "    \n",
    "    for split in split_files:\n",
    "        filename = f\"karman_vortex_{split}_with_pos.pt\"\n",
    "        filepath = data_path / filename\n",
    "        \n",
    "        if not filepath.exists():\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n{split.upper()}:\")\n",
    "        \n",
    "        try:\n",
    "            # Load file\n",
    "            dataset = torch.load(filepath, weights_only=False)\n",
    "            n_total = len(dataset)\n",
    "            file_size_gb = filepath.stat().st_size / (1024**3)\n",
    "            \n",
    "            # Sample first 10 to identify unique Reynolds numbers\n",
    "            sample_size = min(10, n_total)\n",
    "            reynolds_in_split = set()\n",
    "            nodes_by_reynolds = {}\n",
    "            \n",
    "            for i in range(sample_size):\n",
    "                sample = dataset[i]\n",
    "                \n",
    "                # Get Reynolds number\n",
    "                re = None\n",
    "                if hasattr(sample, 'global_params') and sample.global_params is not None:\n",
    "                    re = int(sample.global_params[0].item())\n",
    "                elif hasattr(sample, 'reynolds_number'):\n",
    "                    re = int(sample.reynolds_number)\n",
    "                \n",
    "                if re is not None:\n",
    "                    reynolds_in_split.add(re)\n",
    "                    if re not in nodes_by_reynolds:\n",
    "                        nodes_by_reynolds[re] = sample.x.shape[0]\n",
    "            \n",
    "            # Estimate based on known Reynolds info\n",
    "            estimated_samples_per_re = {}\n",
    "            if reynolds_info:\n",
    "                for re in reynolds_in_split:\n",
    "                    if re in reynolds_info:\n",
    "                        # File size / (samples * graph_size) to estimate samples per Re\n",
    "                        estimated_samples_per_re[re] = reynolds_info[re]['timesteps']\n",
    "            \n",
    "            split_info[split] = {\n",
    "                'total_samples': n_total,\n",
    "                'file_size_gb': file_size_gb,\n",
    "                'reynolds_numbers': sorted(reynolds_in_split),\n",
    "                'n_reynolds': len(reynolds_in_split),\n",
    "                'mb_per_sample': (file_size_gb * 1024) / n_total\n",
    "            }\n",
    "            \n",
    "            print(f\"  Total samples: {n_total}\")\n",
    "            print(f\"  File size: {file_size_gb:.2f} GB\")\n",
    "            print(f\"  MB per sample: {split_info[split]['mb_per_sample']:.3f}\")\n",
    "            print(f\"  Unique Reynolds #s: {len(reynolds_in_split)}\")\n",
    "            print(f\"  Reynolds numbers: {sorted(reynolds_in_split)}\")\n",
    "            \n",
    "            # Free memory\n",
    "            del dataset\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {e}\")\n",
    "    \n",
    "    # Part 3: Analysis - why files differ\n",
    "    print(\"\\n\\n3. WHY FILE SIZES DIFFER - ROOT CAUSE ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if reynolds_info:\n",
    "        # Sort by various metrics\n",
    "        by_nodes = sorted(reynolds_info.items(), key=lambda x: x[1]['nodes'], reverse=True)\n",
    "        by_timesteps = sorted(reynolds_info.items(), key=lambda x: x[1]['timesteps'], reverse=True)\n",
    "        by_size = sorted(reynolds_info.items(), key=lambda x: x[1]['size_mb'], reverse=True)\n",
    "        \n",
    "        # Factor 1: Graph size (nodes)\n",
    "        print(\"\\nFACTOR 1: GRAPH SIZE (Number of Nodes)\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"{'Reynolds':<12} {'Nodes':<10} {'Ratio to Min':<12}\")\n",
    "        print(\"-\" * 80)\n",
    "        min_nodes = min(info['nodes'] for info in reynolds_info.values())\n",
    "        for re, info in by_nodes[:10]:\n",
    "            ratio = info['nodes'] / min_nodes\n",
    "            print(f\"{re:<12} {info['nodes']:<10} {ratio:<12.2f}x\")\n",
    "        \n",
    "        nodes_list = [info['nodes'] for info in reynolds_info.values()]\n",
    "        print(f\"\\nNodes range: {min(nodes_list)} - {max(nodes_list)}\")\n",
    "        print(f\"Ratio (max/min): {max(nodes_list) / min(nodes_list):.2f}x\")\n",
    "        \n",
    "        # Factor 2: Number of timesteps\n",
    "        print(\"\\n\\nFACTOR 2: NUMBER OF TIMESTEPS\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"{'Reynolds':<12} {'Timesteps':<12} {'Ratio to Min':<12}\")\n",
    "        print(\"-\" * 80)\n",
    "        min_timesteps = min(info['timesteps'] for info in reynolds_info.values())\n",
    "        for re, info in by_timesteps[:10]:\n",
    "            ratio = info['timesteps'] / min_timesteps\n",
    "            print(f\"{re:<12} {info['timesteps']:<12} {ratio:<12.2f}x\")\n",
    "        \n",
    "        timesteps_list = [info['timesteps'] for info in reynolds_info.values()]\n",
    "        print(f\"\\nTimesteps range: {min(timesteps_list)} - {max(timesteps_list)}\")\n",
    "        print(f\"Ratio (max/min): {max(timesteps_list) / min(timesteps_list):.2f}x\")\n",
    "        \n",
    "        # Combined effect\n",
    "        print(\"\\n\\nCOMBINED EFFECT: LARGEST FILES\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"{'Reynolds':<12} {'Size (MB)':<12} {'Nodes':<10} {'Timesteps':<12} {'Explanation'}\")\n",
    "        print(\"-\" * 80)\n",
    "        for re, info in by_size[:10]:\n",
    "            explanation = []\n",
    "            if info['nodes'] > np.median(nodes_list):\n",
    "                explanation.append(\"Large mesh\")\n",
    "            if info['timesteps'] > np.median(timesteps_list):\n",
    "                explanation.append(\"Many timesteps\")\n",
    "            exp_str = \", \".join(explanation) if explanation else \"Small mesh + few timesteps\"\n",
    "            print(f\"{re:<12} {info['size_mb']:<12.1f} {info['nodes']:<10} {info['timesteps']:<12} {exp_str}\")\n",
    "        \n",
    "        # Correlations\n",
    "        print(\"\\n\\nCORRELATIONS WITH FILE SIZE:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        reynolds_nums = list(reynolds_info.keys())\n",
    "        sizes = [info['size_mb'] for info in reynolds_info.values()]\n",
    "        nodes = [info['nodes'] for info in reynolds_info.values()]\n",
    "        timesteps = [info['timesteps'] for info in reynolds_info.values()]\n",
    "        \n",
    "        if len(reynolds_nums) > 2:\n",
    "            corr_re = np.corrcoef(reynolds_nums, sizes)[0, 1]\n",
    "            corr_nodes = np.corrcoef(nodes, sizes)[0, 1]\n",
    "            corr_timesteps = np.corrcoef(timesteps, sizes)[0, 1]\n",
    "            \n",
    "            print(f\"Reynolds number:  {corr_re:>6.3f} \", end=\"\")\n",
    "            print(\" Higher Re = larger files\" if corr_re > 0.5 else \" Weak relationship\")\n",
    "            \n",
    "            print(f\"Number of nodes:  {corr_nodes:>6.3f} \", end=\"\")\n",
    "            print(\" STRONG: More nodes = larger files\" if corr_nodes > 0.7 else \" Moderate effect\")\n",
    "            \n",
    "            print(f\"Number of timesteps: {corr_timesteps:>6.3f} \", end=\"\")\n",
    "            print(\" STRONG: More timesteps = larger files\" if corr_timesteps > 0.7 else \" Moderate effect\")\n",
    "    \n",
    "    # Part 4: Split comparison\n",
    "    if split_info:\n",
    "        print(\"\\n\\n4. TRAIN/VAL/TEST SPLIT COMPARISON\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for split in ['train', 'val', 'test']:\n",
    "            if split in split_info:\n",
    "                info = split_info[split]\n",
    "                print(f\"\\n{split.upper()}:\")\n",
    "                print(f\"  Samples: {info['total_samples']}\")\n",
    "                print(f\"  Size: {info['file_size_gb']:.2f} GB\")\n",
    "                print(f\"  Reynolds cases: {info['n_reynolds']}\")\n",
    "                print(f\"  Samples per Reynolds: {info['total_samples'] / info['n_reynolds']:.1f}\")\n",
    "                print(f\"  Reynolds numbers: {info['reynolds_numbers']}\")\n",
    "        \n",
    "        # Why splits differ\n",
    "        if 'train' in split_info and 'val' in split_info:\n",
    "            print(f\"\\nWHY TRAIN IS LARGER THAN VAL:\")\n",
    "            train_samples = split_info['train']['total_samples']\n",
    "            val_samples = split_info['val']['total_samples']\n",
    "            ratio = train_samples / val_samples\n",
    "            print(f\"  Sample ratio (train/val): {ratio:.2f}x\")\n",
    "            print(f\"  Train has {split_info['train']['n_reynolds']} Reynolds cases\")\n",
    "            print(f\"  Val has {split_info['val']['n_reynolds']} Reynolds cases\")\n",
    "            \n",
    "            if reynolds_info:\n",
    "                train_re = split_info['train']['reynolds_numbers']\n",
    "                val_re = split_info['val']['reynolds_numbers']\n",
    "                \n",
    "                train_avg_nodes = np.mean([reynolds_info[re]['nodes'] for re in train_re if re in reynolds_info])\n",
    "                val_avg_nodes = np.mean([reynolds_info[re]['nodes'] for re in val_re if re in reynolds_info])\n",
    "                \n",
    "                print(f\"  Avg nodes in train cases: {train_avg_nodes:.0f}\")\n",
    "                print(f\"  Avg nodes in val cases: {val_avg_nodes:.0f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ANALYSIS COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return reynolds_info, split_info\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    DATA_DIR = \"/standard/sds_baek_energetic/von_karman_vortex/full_data\"\n",
    "    \n",
    "    print(\"Enter the path to your processed data directory:\")\n",
    "    print(f\"(Press Enter to use: {DATA_DIR})\")\n",
    "    \n",
    "    user_input = input().strip()\n",
    "    if user_input:\n",
    "        DATA_DIR = user_input\n",
    "    \n",
    "    if not os.path.exists(DATA_DIR):\n",
    "        print(f\"\\nERROR: Directory not found: {DATA_DIR}\")\n",
    "    else:\n",
    "        ultra_fast_static_graph_analysis(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b792ecdf-40dd-451d-a26c-8dc73f950460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KARMAN VORTEX STREET DATASET PROCESSOR - MULTIPLE DIRECTORIES\n",
      "SHOCK TUBE FORMAT: positions + physics in x tensor\n",
      "Sources:\n",
      "  1. /standard/sds_baek_energetic/von_karman_vortex/Reynolds 200~\n",
      "  2. /standard/sds_baek_energetic/von_karman_vortex/Reynolds 1~150\n",
      "Output: /standard/sds_baek_energetic/von_karman_vortex/full_data\n",
      "BATCH PROCESSING ALL KARMAN VORTEX CASES FROM MULTIPLE DIRECTORIES\n",
      "Processing from 2 source directories\n",
      "Discovering Karman vortex cases from 2 directories:\n",
      "  - /standard/sds_baek_energetic/von_karman_vortex/Reynolds 200~\n",
      "  - /standard/sds_baek_energetic/von_karman_vortex/Reynolds 1~150\n",
      "\n",
      "Processing directory 1/2: /standard/sds_baek_energetic/von_karman_vortex/Reynolds 200~\n",
      "  Found 17 Reynolds directories\n",
      "    Reynolds_1000: 401 timesteps (labeled as Reynolds_1000_dir0)\n",
      "    Reynolds_200: 401 timesteps (labeled as Reynolds_200_dir0)\n",
      "    Reynolds_250: 401 timesteps (labeled as Reynolds_250_dir0)\n",
      "    Reynolds_300: 401 timesteps (labeled as Reynolds_300_dir0)\n",
      "    Reynolds_350: 401 timesteps (labeled as Reynolds_350_dir0)\n",
      "    Reynolds_400: 401 timesteps (labeled as Reynolds_400_dir0)\n",
      "    Reynolds_450: 401 timesteps (labeled as Reynolds_450_dir0)\n",
      "    Reynolds_500: 401 timesteps (labeled as Reynolds_500_dir0)\n",
      "    Reynolds_550: 401 timesteps (labeled as Reynolds_550_dir0)\n",
      "    Reynolds_600: 401 timesteps (labeled as Reynolds_600_dir0)\n",
      "    Reynolds_650: 401 timesteps (labeled as Reynolds_650_dir0)\n",
      "    Reynolds_700: 401 timesteps (labeled as Reynolds_700_dir0)\n",
      "    Reynolds_750: 401 timesteps (labeled as Reynolds_750_dir0)\n",
      "    Reynolds_800: 401 timesteps (labeled as Reynolds_800_dir0)\n",
      "    Reynolds_850: 401 timesteps (labeled as Reynolds_850_dir0)\n",
      "    Reynolds_900: 401 timesteps (labeled as Reynolds_900_dir0)\n",
      "    Reynolds_950: 401 timesteps (labeled as Reynolds_950_dir0)\n",
      "\n",
      "Processing directory 2/2: /standard/sds_baek_energetic/von_karman_vortex/Reynolds 1~150\n",
      "  Found 6 Reynolds directories\n",
      "    Reynolds_1: 401 timesteps (labeled as Reynolds_1_dir1)\n",
      "    Reynolds_100: 401 timesteps (labeled as Reynolds_100_dir1)\n",
      "    Reynolds_150: 401 timesteps (labeled as Reynolds_150_dir1)\n",
      "    Reynolds_20: 401 timesteps (labeled as Reynolds_20_dir1)\n",
      "    Reynolds_40: 401 timesteps (labeled as Reynolds_40_dir1)\n",
      "    Reynolds_50: 401 timesteps (labeled as Reynolds_50_dir1)\n",
      "\n",
      "Total valid Reynolds cases across all directories: 23\n",
      "  Directory 1 (Reynolds 200~): 17 cases\n",
      "  Directory 2 (Reynolds 1~150): 6 cases\n",
      "Analyzing variable ranges across ALL Reynolds cases from ALL directories\n",
      "  Analyzing variable ranges for Reynolds_1000_dir0...\n",
      "  Analyzing variable ranges for Reynolds_200_dir0...\n",
      "  Analyzing variable ranges for Reynolds_250_dir0...\n",
      "  Analyzing variable ranges for Reynolds_300_dir0...\n",
      "  Analyzing variable ranges for Reynolds_350_dir0...\n",
      "  Analyzing variable ranges for Reynolds_400_dir0...\n",
      "  Analyzing variable ranges for Reynolds_450_dir0...\n",
      "  Analyzing variable ranges for Reynolds_500_dir0...\n",
      "  Analyzing variable ranges for Reynolds_550_dir0...\n",
      "  Analyzing variable ranges for Reynolds_600_dir0...\n",
      "  Analyzing variable ranges for Reynolds_650_dir0...\n",
      "  Analyzing variable ranges for Reynolds_700_dir0...\n",
      "  Analyzing variable ranges for Reynolds_750_dir0...\n",
      "  Analyzing variable ranges for Reynolds_800_dir0...\n",
      "  Analyzing variable ranges for Reynolds_850_dir0...\n",
      "  Analyzing variable ranges for Reynolds_900_dir0...\n",
      "  Analyzing variable ranges for Reynolds_950_dir0...\n",
      "  Analyzing variable ranges for Reynolds_1_dir1...\n",
      "  Analyzing variable ranges for Reynolds_100_dir1...\n",
      "  Analyzing variable ranges for Reynolds_150_dir1...\n",
      "  Analyzing variable ranges for Reynolds_20_dir1...\n",
      "  Analyzing variable ranges for Reynolds_40_dir1...\n",
      "  Analyzing variable ranges for Reynolds_50_dir1...\n",
      "Global variable statistics summary (from 23 cases across 2 directories):\n",
      "  X_POS:\n",
      "    Global Range: [-5.000000e-02, 1.500000e-01]\n",
      "    Range Span: 2.000000e-01\n",
      "    Mean across cases: 5.442107e-02 췀 6.932533e-03\n",
      "  Y_POS:\n",
      "    Global Range: [-5.000000e-02, 5.000000e-02]\n",
      "    Range Span: 1.000000e-01\n",
      "    Mean across cases: -2.488630e-07 췀 3.465761e-07\n",
      "  Z_POS:\n",
      "    Global Range: [0.000000e+00, 1.000000e-02]\n",
      "    Range Span: 1.000000e-02\n",
      "    Mean across cases: 5.000000e-03 췀 0.000000e+00\n",
      "  PRESSURE:\n",
      "    Global Range: [-4.066926e+00, 1.581088e+00]\n",
      "    Range Span: 5.648015e+00\n",
      "    Mean across cases: -2.629139e-02 췀 3.207653e-02\n",
      "  VELOCITY_X:\n",
      "    Global Range: [-2.089695e+00, 3.098995e+00]\n",
      "    Range Span: 5.188691e+00\n",
      "    Mean across cases: 6.150201e-01 췀 4.247079e-01\n",
      "  VELOCITY_Y:\n",
      "    Global Range: [-2.557230e+00, 2.146482e+00]\n",
      "    Range Span: 4.703712e+00\n",
      "    Mean across cases: 1.995986e-02 췀 1.181076e-02\n",
      "  VELOCITY_Z:\n",
      "    Global Range: [-3.454671e-19, 3.312376e-19]\n",
      "    Range Span: 6.767047e-19\n",
      "    Mean across cases: -2.356431e-24 췀 8.570545e-24\n",
      "  VORTICITY_X:\n",
      "    Global Range: [-2.844113e-14, 5.295167e-14]\n",
      "    Range Span: 8.139280e-14\n",
      "    Mean across cases: 1.341511e-19 췀 3.762512e-19\n",
      "  VORTICITY_Y:\n",
      "    Global Range: [-3.342095e-14, 2.517809e-14]\n",
      "    Range Span: 5.859904e-14\n",
      "    Mean across cases: -1.308296e-19 췀 3.148342e-19\n",
      "  VORTICITY_Z:\n",
      "    Global Range: [-2.488884e+04, 2.035310e+04]\n",
      "    Range Span: 4.524194e+04\n",
      "    Mean across cases: -1.196720e+00 췀 5.861250e-01\n",
      "Processing individual Reynolds cases:\n",
      "Processing case: Reynolds_1000_dir0\n",
      "  Reynolds number: 1000\n",
      "  Source directory: /standard/sds_baek_energetic/von_karman_vortex/Reynolds 200~\n",
      "  Timesteps: 401\n",
      "  Analyzing variable ranges for Reynolds_1000_dir0...\n",
      "    Processing timestep pairs: 0/400\n",
      "    Processing timestep pairs: 50/400\n",
      "    Processing timestep pairs: 100/400\n",
      "    Processing timestep pairs: 150/400\n",
      "    Processing timestep pairs: 200/400\n",
      "    Processing timestep pairs: 250/400\n",
      "    Processing timestep pairs: 300/400\n",
      "    Processing timestep pairs: 350/400\n",
      "  Successfully processed 400 timestep pairs\n",
      "  Saved individual case: /standard/sds_baek_energetic/von_karman_vortex/full_data/individual_cases/Reynolds_1000_dir0_raw_data.pt\n",
      "Processing case: Reynolds_200_dir0\n",
      "  Reynolds number: 200\n",
      "  Source directory: /standard/sds_baek_energetic/von_karman_vortex/Reynolds 200~\n",
      "  Timesteps: 401\n",
      "  Analyzing variable ranges for Reynolds_200_dir0...\n",
      "    Processing timestep pairs: 0/400\n",
      "    Processing timestep pairs: 50/400\n",
      "    Processing timestep pairs: 100/400\n",
      "    Processing timestep pairs: 150/400\n",
      "    Processing timestep pairs: 200/400\n",
      "    Processing timestep pairs: 250/400\n",
      "    Processing timestep pairs: 300/400\n",
      "    Processing timestep pairs: 350/400\n",
      "  Successfully processed 400 timestep pairs\n",
      "  Saved individual case: /standard/sds_baek_energetic/von_karman_vortex/full_data/individual_cases/Reynolds_200_dir0_raw_data.pt\n",
      "Processing case: Reynolds_250_dir0\n",
      "  Reynolds number: 250\n",
      "  Source directory: /standard/sds_baek_energetic/von_karman_vortex/Reynolds 200~\n",
      "  Timesteps: 401\n",
      "  Analyzing variable ranges for Reynolds_250_dir0...\n",
      "    Processing timestep pairs: 0/400\n",
      "    Processing timestep pairs: 50/400\n",
      "    Processing timestep pairs: 100/400\n",
      "    Processing timestep pairs: 150/400\n",
      "    Processing timestep pairs: 200/400\n",
      "    Processing timestep pairs: 250/400\n",
      "    Processing timestep pairs: 300/400\n",
      "    Processing timestep pairs: 350/400\n",
      "  Successfully processed 400 timestep pairs\n",
      "  Saved individual case: /standard/sds_baek_energetic/von_karman_vortex/full_data/individual_cases/Reynolds_250_dir0_raw_data.pt\n",
      "Processing case: Reynolds_300_dir0\n",
      "  Reynolds number: 300\n",
      "  Source directory: /standard/sds_baek_energetic/von_karman_vortex/Reynolds 200~\n",
      "  Timesteps: 401\n",
      "  Analyzing variable ranges for Reynolds_300_dir0...\n",
      "    Processing timestep pairs: 0/400\n",
      "    Processing timestep pairs: 50/400\n",
      "    Processing timestep pairs: 100/400\n",
      "    Processing timestep pairs: 150/400\n",
      "    Processing timestep pairs: 200/400\n",
      "    Processing timestep pairs: 250/400\n",
      "    Processing timestep pairs: 300/400\n",
      "    Processing timestep pairs: 350/400\n",
      "  Successfully processed 400 timestep pairs\n",
      "  Saved individual case: /standard/sds_baek_energetic/von_karman_vortex/full_data/individual_cases/Reynolds_300_dir0_raw_data.pt\n",
      "Processing case: Reynolds_350_dir0\n",
      "  Reynolds number: 350\n",
      "  Source directory: /standard/sds_baek_energetic/von_karman_vortex/Reynolds 200~\n",
      "  Timesteps: 401\n",
      "  Analyzing variable ranges for Reynolds_350_dir0...\n",
      "    Processing timestep pairs: 0/400\n",
      "    Processing timestep pairs: 50/400\n",
      "    Processing timestep pairs: 100/400\n",
      "    Processing timestep pairs: 150/400\n",
      "    Processing timestep pairs: 200/400\n",
      "    Processing timestep pairs: 250/400\n",
      "    Processing timestep pairs: 300/400\n",
      "    Processing timestep pairs: 350/400\n",
      "  Successfully processed 400 timestep pairs\n",
      "  Saved individual case: /standard/sds_baek_energetic/von_karman_vortex/full_data/individual_cases/Reynolds_350_dir0_raw_data.pt\n",
      "Processing case: Reynolds_400_dir0\n",
      "  Reynolds number: 400\n",
      "  Source directory: /standard/sds_baek_energetic/von_karman_vortex/Reynolds 200~\n",
      "  Timesteps: 401\n",
      "  Analyzing variable ranges for Reynolds_400_dir0...\n",
      "    Processing timestep pairs: 0/400\n",
      "    Processing timestep pairs: 50/400\n",
      "    Processing timestep pairs: 100/400\n",
      "    Processing timestep pairs: 150/400\n",
      "    Processing timestep pairs: 200/400\n",
      "    Processing timestep pairs: 250/400\n",
      "    Processing timestep pairs: 300/400\n",
      "    Processing timestep pairs: 350/400\n",
      "  Successfully processed 400 timestep pairs\n",
      "  Saved individual case: /standard/sds_baek_energetic/von_karman_vortex/full_data/individual_cases/Reynolds_400_dir0_raw_data.pt\n",
      "Processing case: Reynolds_450_dir0\n",
      "  Reynolds number: 450\n",
      "  Source directory: /standard/sds_baek_energetic/von_karman_vortex/Reynolds 200~\n",
      "  Timesteps: 401\n",
      "  Analyzing variable ranges for Reynolds_450_dir0...\n",
      "    Processing timestep pairs: 0/400\n",
      "    Processing timestep pairs: 50/400\n",
      "    Processing timestep pairs: 100/400\n",
      "    Processing timestep pairs: 150/400\n",
      "    Processing timestep pairs: 200/400\n",
      "    Processing timestep pairs: 250/400\n",
      "    Processing timestep pairs: 300/400\n",
      "    Processing timestep pairs: 350/400\n",
      "  Successfully processed 400 timestep pairs\n",
      "  Saved individual case: /standard/sds_baek_energetic/von_karman_vortex/full_data/individual_cases/Reynolds_450_dir0_raw_data.pt\n",
      "Processing case: Reynolds_500_dir0\n",
      "  Reynolds number: 500\n",
      "  Source directory: /standard/sds_baek_energetic/von_karman_vortex/Reynolds 200~\n",
      "  Timesteps: 401\n",
      "  Analyzing variable ranges for Reynolds_500_dir0...\n",
      "    Processing timestep pairs: 0/400\n",
      "    Processing timestep pairs: 50/400\n",
      "    Processing timestep pairs: 100/400\n",
      "    Processing timestep pairs: 150/400\n",
      "    Processing timestep pairs: 200/400\n",
      "    Processing timestep pairs: 250/400\n",
      "    Processing timestep pairs: 300/400\n",
      "    Processing timestep pairs: 350/400\n",
      "  Successfully processed 400 timestep pairs\n",
      "  Saved individual case: /standard/sds_baek_energetic/von_karman_vortex/full_data/individual_cases/Reynolds_500_dir0_raw_data.pt\n",
      "Processing case: Reynolds_550_dir0\n",
      "  Reynolds number: 550\n",
      "  Source directory: /standard/sds_baek_energetic/von_karman_vortex/Reynolds 200~\n",
      "  Timesteps: 401\n",
      "  Analyzing variable ranges for Reynolds_550_dir0...\n",
      "    Processing timestep pairs: 0/400\n",
      "    Processing timestep pairs: 50/400\n",
      "    Processing timestep pairs: 100/400\n",
      "    Processing timestep pairs: 150/400\n",
      "    Processing timestep pairs: 200/400\n",
      "    Processing timestep pairs: 250/400\n",
      "    Processing timestep pairs: 300/400\n",
      "    Processing timestep pairs: 350/400\n",
      "  Successfully processed 400 timestep pairs\n",
      "  Saved individual case: /standard/sds_baek_energetic/von_karman_vortex/full_data/individual_cases/Reynolds_550_dir0_raw_data.pt\n",
      "Processing case: Reynolds_600_dir0\n",
      "  Reynolds number: 600\n",
      "  Source directory: /standard/sds_baek_energetic/von_karman_vortex/Reynolds 200~\n",
      "  Timesteps: 401\n",
      "  Analyzing variable ranges for Reynolds_600_dir0...\n",
      "    Processing timestep pairs: 0/400\n",
      "    Processing timestep pairs: 50/400\n",
      "    Processing timestep pairs: 100/400\n",
      "    Processing timestep pairs: 150/400\n",
      "    Processing timestep pairs: 200/400\n",
      "    Processing timestep pairs: 250/400\n",
      "    Processing timestep pairs: 300/400\n",
      "    Processing timestep pairs: 350/400\n",
      "  Successfully processed 400 timestep pairs\n",
      "  Saved individual case: /standard/sds_baek_energetic/von_karman_vortex/full_data/individual_cases/Reynolds_600_dir0_raw_data.pt\n",
      "Processing case: Reynolds_650_dir0\n",
      "  Reynolds number: 650\n",
      "  Source directory: /standard/sds_baek_energetic/von_karman_vortex/Reynolds 200~\n",
      "  Timesteps: 401\n",
      "  Analyzing variable ranges for Reynolds_650_dir0...\n",
      "    Processing timestep pairs: 0/400\n",
      "    Processing timestep pairs: 50/400\n",
      "    Processing timestep pairs: 100/400\n",
      "    Processing timestep pairs: 150/400\n",
      "    Processing timestep pairs: 200/400\n",
      "    Processing timestep pairs: 250/400\n",
      "    Processing timestep pairs: 300/400\n",
      "    Processing timestep pairs: 350/400\n",
      "  Successfully processed 400 timestep pairs\n",
      "  Saved individual case: /standard/sds_baek_energetic/von_karman_vortex/full_data/individual_cases/Reynolds_650_dir0_raw_data.pt\n",
      "Processing case: Reynolds_700_dir0\n",
      "  Reynolds number: 700\n",
      "  Source directory: /standard/sds_baek_energetic/von_karman_vortex/Reynolds 200~\n",
      "  Timesteps: 401\n",
      "  Analyzing variable ranges for Reynolds_700_dir0...\n",
      "    Processing timestep pairs: 0/400\n",
      "    Processing timestep pairs: 50/400\n",
      "    Processing timestep pairs: 100/400\n",
      "    Processing timestep pairs: 150/400\n",
      "    Processing timestep pairs: 200/400\n",
      "    Processing timestep pairs: 250/400\n",
      "    Processing timestep pairs: 300/400\n",
      "    Processing timestep pairs: 350/400\n",
      "  Successfully processed 400 timestep pairs\n",
      "  Saved individual case: /standard/sds_baek_energetic/von_karman_vortex/full_data/individual_cases/Reynolds_700_dir0_raw_data.pt\n",
      "Processing case: Reynolds_750_dir0\n",
      "  Reynolds number: 750\n",
      "  Source directory: /standard/sds_baek_energetic/von_karman_vortex/Reynolds 200~\n",
      "  Timesteps: 401\n",
      "  Analyzing variable ranges for Reynolds_750_dir0...\n",
      "    Processing timestep pairs: 0/400\n",
      "    Processing timestep pairs: 50/400\n",
      "    Processing timestep pairs: 100/400\n",
      "    Processing timestep pairs: 150/400\n",
      "    Processing timestep pairs: 200/400\n",
      "    Processing timestep pairs: 250/400\n",
      "    Processing timestep pairs: 300/400\n",
      "    Processing timestep pairs: 350/400\n",
      "  Successfully processed 400 timestep pairs\n",
      "  Saved individual case: /standard/sds_baek_energetic/von_karman_vortex/full_data/individual_cases/Reynolds_750_dir0_raw_data.pt\n",
      "Processing case: Reynolds_800_dir0\n",
      "  Reynolds number: 800\n",
      "  Source directory: /standard/sds_baek_energetic/von_karman_vortex/Reynolds 200~\n",
      "  Timesteps: 401\n",
      "  Analyzing variable ranges for Reynolds_800_dir0...\n",
      "    Processing timestep pairs: 0/400\n",
      "    Processing timestep pairs: 50/400\n",
      "    Processing timestep pairs: 100/400\n",
      "    Processing timestep pairs: 150/400\n",
      "    Processing timestep pairs: 200/400\n",
      "    Processing timestep pairs: 250/400\n",
      "    Processing timestep pairs: 300/400\n",
      "    Processing timestep pairs: 350/400\n",
      "  Successfully processed 400 timestep pairs\n",
      "  Saved individual case: /standard/sds_baek_energetic/von_karman_vortex/full_data/individual_cases/Reynolds_800_dir0_raw_data.pt\n",
      "Processing case: Reynolds_850_dir0\n",
      "  Reynolds number: 850\n",
      "  Source directory: /standard/sds_baek_energetic/von_karman_vortex/Reynolds 200~\n",
      "  Timesteps: 401\n",
      "  Analyzing variable ranges for Reynolds_850_dir0...\n",
      "    Processing timestep pairs: 0/400\n",
      "    Processing timestep pairs: 50/400\n",
      "    Processing timestep pairs: 100/400\n",
      "    Processing timestep pairs: 150/400\n",
      "    Processing timestep pairs: 200/400\n",
      "    Processing timestep pairs: 250/400\n",
      "    Processing timestep pairs: 300/400\n",
      "    Processing timestep pairs: 350/400\n",
      "  Successfully processed 400 timestep pairs\n",
      "  Saved individual case: /standard/sds_baek_energetic/von_karman_vortex/full_data/individual_cases/Reynolds_850_dir0_raw_data.pt\n",
      "Processing case: Reynolds_900_dir0\n",
      "  Reynolds number: 900\n",
      "  Source directory: /standard/sds_baek_energetic/von_karman_vortex/Reynolds 200~\n",
      "  Timesteps: 401\n",
      "  Analyzing variable ranges for Reynolds_900_dir0...\n",
      "    Processing timestep pairs: 0/400\n",
      "    Processing timestep pairs: 50/400\n",
      "    Processing timestep pairs: 100/400\n",
      "    Processing timestep pairs: 150/400\n",
      "    Processing timestep pairs: 200/400\n",
      "    Processing timestep pairs: 250/400\n",
      "    Processing timestep pairs: 300/400\n",
      "    Processing timestep pairs: 350/400\n",
      "  Successfully processed 400 timestep pairs\n",
      "  Saved individual case: /standard/sds_baek_energetic/von_karman_vortex/full_data/individual_cases/Reynolds_900_dir0_raw_data.pt\n",
      "Processing case: Reynolds_950_dir0\n",
      "  Reynolds number: 950\n",
      "  Source directory: /standard/sds_baek_energetic/von_karman_vortex/Reynolds 200~\n",
      "  Timesteps: 401\n",
      "  Analyzing variable ranges for Reynolds_950_dir0...\n",
      "    Processing timestep pairs: 0/400\n",
      "    Processing timestep pairs: 50/400\n",
      "    Processing timestep pairs: 100/400\n",
      "    Processing timestep pairs: 150/400\n",
      "    Processing timestep pairs: 200/400\n",
      "    Processing timestep pairs: 250/400\n",
      "    Processing timestep pairs: 300/400\n",
      "    Processing timestep pairs: 350/400\n",
      "  Successfully processed 400 timestep pairs\n",
      "  Saved individual case: /standard/sds_baek_energetic/von_karman_vortex/full_data/individual_cases/Reynolds_950_dir0_raw_data.pt\n",
      "Processing case: Reynolds_1_dir1\n",
      "  Reynolds number: 1\n",
      "  Source directory: /standard/sds_baek_energetic/von_karman_vortex/Reynolds 1~150\n",
      "  Timesteps: 401\n",
      "  Analyzing variable ranges for Reynolds_1_dir1...\n",
      "    Processing timestep pairs: 0/400\n",
      "    Processing timestep pairs: 50/400\n",
      "    Processing timestep pairs: 100/400\n",
      "    Processing timestep pairs: 150/400\n",
      "    Processing timestep pairs: 200/400\n",
      "    Processing timestep pairs: 250/400\n",
      "    Processing timestep pairs: 300/400\n",
      "    Processing timestep pairs: 350/400\n",
      "  Successfully processed 400 timestep pairs\n",
      "  Saved individual case: /standard/sds_baek_energetic/von_karman_vortex/full_data/individual_cases/Reynolds_1_dir1_raw_data.pt\n",
      "Processing case: Reynolds_100_dir1\n",
      "  Reynolds number: 100\n",
      "  Source directory: /standard/sds_baek_energetic/von_karman_vortex/Reynolds 1~150\n",
      "  Timesteps: 401\n",
      "  Analyzing variable ranges for Reynolds_100_dir1...\n",
      "    Processing timestep pairs: 0/400\n",
      "    Processing timestep pairs: 50/400\n",
      "    Processing timestep pairs: 100/400\n",
      "    Processing timestep pairs: 150/400\n",
      "    Processing timestep pairs: 200/400\n",
      "    Processing timestep pairs: 250/400\n",
      "    Processing timestep pairs: 300/400\n",
      "    Processing timestep pairs: 350/400\n",
      "  Successfully processed 400 timestep pairs\n",
      "  Saved individual case: /standard/sds_baek_energetic/von_karman_vortex/full_data/individual_cases/Reynolds_100_dir1_raw_data.pt\n",
      "Processing case: Reynolds_150_dir1\n",
      "  Reynolds number: 150\n",
      "  Source directory: /standard/sds_baek_energetic/von_karman_vortex/Reynolds 1~150\n",
      "  Timesteps: 401\n",
      "  Analyzing variable ranges for Reynolds_150_dir1...\n",
      "    Processing timestep pairs: 0/400\n",
      "    Processing timestep pairs: 50/400\n",
      "    Processing timestep pairs: 100/400\n",
      "    Processing timestep pairs: 150/400\n",
      "    Processing timestep pairs: 200/400\n",
      "    Processing timestep pairs: 250/400\n",
      "    Processing timestep pairs: 300/400\n",
      "    Processing timestep pairs: 350/400\n",
      "  Successfully processed 400 timestep pairs\n",
      "  Saved individual case: /standard/sds_baek_energetic/von_karman_vortex/full_data/individual_cases/Reynolds_150_dir1_raw_data.pt\n",
      "Processing case: Reynolds_20_dir1\n",
      "  Reynolds number: 20\n",
      "  Source directory: /standard/sds_baek_energetic/von_karman_vortex/Reynolds 1~150\n",
      "  Timesteps: 401\n",
      "  Analyzing variable ranges for Reynolds_20_dir1...\n",
      "    Processing timestep pairs: 0/400\n",
      "    Processing timestep pairs: 50/400\n",
      "    Processing timestep pairs: 100/400\n",
      "    Processing timestep pairs: 150/400\n",
      "    Processing timestep pairs: 200/400\n",
      "    Processing timestep pairs: 250/400\n",
      "    Processing timestep pairs: 300/400\n",
      "    Processing timestep pairs: 350/400\n",
      "  Successfully processed 400 timestep pairs\n",
      "  Saved individual case: /standard/sds_baek_energetic/von_karman_vortex/full_data/individual_cases/Reynolds_20_dir1_raw_data.pt\n",
      "Processing case: Reynolds_40_dir1\n",
      "  Reynolds number: 40\n",
      "  Source directory: /standard/sds_baek_energetic/von_karman_vortex/Reynolds 1~150\n",
      "  Timesteps: 401\n",
      "  Analyzing variable ranges for Reynolds_40_dir1...\n",
      "    Processing timestep pairs: 0/400\n",
      "    Processing timestep pairs: 50/400\n",
      "    Processing timestep pairs: 100/400\n",
      "    Processing timestep pairs: 150/400\n",
      "    Processing timestep pairs: 200/400\n",
      "    Processing timestep pairs: 250/400\n",
      "    Processing timestep pairs: 300/400\n",
      "    Processing timestep pairs: 350/400\n",
      "  Successfully processed 400 timestep pairs\n",
      "  Saved individual case: /standard/sds_baek_energetic/von_karman_vortex/full_data/individual_cases/Reynolds_40_dir1_raw_data.pt\n",
      "Processing case: Reynolds_50_dir1\n",
      "  Reynolds number: 50\n",
      "  Source directory: /standard/sds_baek_energetic/von_karman_vortex/Reynolds 1~150\n",
      "  Timesteps: 401\n",
      "  Analyzing variable ranges for Reynolds_50_dir1...\n",
      "    Processing timestep pairs: 0/400\n",
      "    Processing timestep pairs: 50/400\n",
      "    Processing timestep pairs: 100/400\n",
      "    Processing timestep pairs: 150/400\n",
      "    Processing timestep pairs: 200/400\n",
      "    Processing timestep pairs: 250/400\n",
      "    Processing timestep pairs: 300/400\n",
      "    Processing timestep pairs: 350/400\n",
      "  Successfully processed 400 timestep pairs\n",
      "  Saved individual case: /standard/sds_baek_energetic/von_karman_vortex/full_data/individual_cases/Reynolds_50_dir1_raw_data.pt\n",
      "Processing summary:\n",
      "  Total Reynolds cases processed: 23\n",
      "  Total samples: 9200\n",
      "  Reynolds numbers: [1, 20, 40, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000]\n",
      "  x.shape: [165082, 10] (nodes, features)\n",
      "    - First 3 columns: positions (x, y, z)\n",
      "    - Next 7 columns: physics (pressure, velocity_x/y/z, vorticity_x/y/z)\n",
      "  y.shape: [165082, 7] (nodes, physics only)\n",
      "  global_params.shape: [1] (Reynolds number)\n",
      "READY FOR TRAIN/TEST/VAL SPLIT\n",
      "Variable range analysis complete\n",
      "Individual simulations saved\n",
      "\n",
      "============================================================\n",
      "USING MANUAL REYNOLDS NUMBER SPLIT\n",
      "============================================================\n",
      "Creating train/test/val split\n",
      "  Found 23 unique Reynolds numbers\n",
      "  Available Reynolds numbers: [1, 20, 40, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000]\n",
      "\n",
      "  Using MANUAL Reynolds number assignment:\n",
      "    Train Reynolds: [40, 50, 100, 150, 250, 400, 450, 500, 550, 700, 750, 850, 950]\n",
      "    Val Reynolds:   [20, 200, 300, 650, 1000]\n",
      "    Test Reynolds:  [1, 350, 600, 800, 900]\n",
      "\n",
      "Final split sizes:\n",
      "  Train: 5200 samples (56.5%)\n",
      "  Val:   2000 samples (21.7%)\n",
      "  Test:  2000 samples (21.7%)\n",
      "  Saved train split: /standard/sds_baek_energetic/von_karman_vortex/full_data/karman_vortex_train_with_pos.pt\n",
      "  Saved val split: /standard/sds_baek_energetic/von_karman_vortex/full_data/karman_vortex_val_with_pos.pt\n",
      "  Saved test split: /standard/sds_baek_energetic/von_karman_vortex/full_data/karman_vortex_test_with_pos.pt\n",
      "\n",
      "Updated metadata: /standard/sds_baek_energetic/von_karman_vortex/full_data/karman_dataset_metadata.json\n",
      "\n",
      "NORMALIZING DATASETS\n",
      "Computing global normalization parameters...\n",
      "  x_pos: [-5.000000e-02, 1.500000e-01]\n",
      "  y_pos: [-5.000000e-02, 5.000000e-02]\n",
      "  z_pos: [0.000000e+00, 1.000000e-02]\n",
      "  pressure: [-4.066926e+00, 1.581088e+00]\n",
      "  velocity_x: [-2.089695e+00, 3.098995e+00]\n",
      "  velocity_y: [-2.557230e+00, 2.146482e+00]\n",
      "  velocity_z: [-3.454671e-19, 3.312376e-19]\n",
      "  vorticity_x: [-2.844113e-14, 5.295167e-14]\n",
      "  vorticity_y: [-3.342095e-14, 2.517809e-14]\n",
      "  vorticity_z: [-2.488884e+04, 2.035310e+04]\n",
      "Computing global parameter normalization...\n",
      "NORMALIZING KARMAN VORTEX DATASETS - SHOCK TUBE FORMAT\n",
      "  Loading: /standard/sds_baek_energetic/von_karman_vortex/full_data/karman_vortex_train_with_pos.pt\n"
     ]
    }
   ],
   "source": [
    "import vtk\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class KarmanVortexProcessorNewStructure:\n",
    "    \"\"\"\n",
    "    Processor for Karman vortex street simulations - NEW DATA STRUCTURE.\n",
    "    Modified to handle MULTIPLE base directories.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_data_dirs, include_parameters_as_features=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            base_data_dirs: List of base directory paths or single path\n",
    "            include_parameters_as_features: Whether to include Reynolds number as global feature\n",
    "        \"\"\"\n",
    "        # Convert single path to list for uniform handling\n",
    "        if isinstance(base_data_dirs, (str, Path)):\n",
    "            base_data_dirs = [base_data_dirs]\n",
    "        \n",
    "        self.base_data_dirs = [Path(d) for d in base_data_dirs]\n",
    "        self.include_parameters_as_features = include_parameters_as_features\n",
    "        \n",
    "        # Position variables (will be first in x tensor)\n",
    "        self.position_vars = ['x_pos', 'y_pos', 'z_pos']\n",
    "        \n",
    "        # Physical variable names from VTK files - RAW DATA ONLY\n",
    "        self.physics_vars = ['pressure', 'velocity_x', 'velocity_y', 'velocity_z', \n",
    "                           'vorticity_x', 'vorticity_y', 'vorticity_z']\n",
    "        \n",
    "        # All variables in x tensor (positions + physics)\n",
    "        self.var_names = self.position_vars + self.physics_vars\n",
    "        \n",
    "        # Global parameters\n",
    "        self.parameter_names = ['reynolds_number']\n",
    "        \n",
    "        # For tracking processed cases\n",
    "        self.case_info = {}\n",
    "        self.all_variable_stats = {}\n",
    "        self.parameter_stats = {}\n",
    "        \n",
    "    def discover_reynolds_cases(self):\n",
    "        \"\"\"Discover all available Reynolds number cases from ALL base directories.\"\"\"\n",
    "        print(f\"Discovering Karman vortex cases from {len(self.base_data_dirs)} directories:\")\n",
    "        for dir_path in self.base_data_dirs:\n",
    "            print(f\"  - {dir_path}\")\n",
    "        \n",
    "        all_case_dirs = []\n",
    "        \n",
    "        # Process each base directory\n",
    "        for base_dir_idx, base_data_dir in enumerate(self.base_data_dirs):\n",
    "            print(f\"\\nProcessing directory {base_dir_idx + 1}/{len(self.base_data_dirs)}: {base_data_dir}\")\n",
    "            \n",
    "            if not base_data_dir.exists():\n",
    "                print(f\"  WARNING: Directory {base_data_dir} does not exist! Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Find all Reynolds_XX directories at the base level\n",
    "            reynolds_dirs = sorted([d for d in base_data_dir.iterdir() \n",
    "                                   if d.is_dir() and d.name.startswith('Reynolds_')])\n",
    "            \n",
    "            print(f\"  Found {len(reynolds_dirs)} Reynolds directories\")\n",
    "            \n",
    "            for re_dir in reynolds_dirs:\n",
    "                # Extract Reynolds number from directory name\n",
    "                re_match = re.search(r'Reynolds_(\\d+)', re_dir.name)\n",
    "                if not re_match:\n",
    "                    print(f\"    {re_dir.name}: Could not extract Reynolds number\")\n",
    "                    continue\n",
    "                    \n",
    "                reynolds_num = int(re_match.group(1))\n",
    "                \n",
    "                # NEW STRUCTURE: Look for double-nested Reynolds directory\n",
    "                inner_re_dir = re_dir / re_dir.name\n",
    "                \n",
    "                if not inner_re_dir.exists():\n",
    "                    print(f\"    {re_dir.name}: No double-nested directory found\")\n",
    "                    continue\n",
    "                \n",
    "                # Look for VTK directory\n",
    "                vtk_dir = inner_re_dir / \"VTK\"\n",
    "                \n",
    "                if not vtk_dir.exists():\n",
    "                    print(f\"    {re_dir.name}: No VTK directory found\")\n",
    "                    continue\n",
    "                \n",
    "                # Look for timestep directories\n",
    "                timestep_dirs = [d for d in vtk_dir.iterdir() if d.is_dir()]\n",
    "                \n",
    "                if not timestep_dirs:\n",
    "                    print(f\"    {re_dir.name}: No timestep directories in VTK\")\n",
    "                    continue\n",
    "                \n",
    "                # Check for internal.vtu files in timestep directories\n",
    "                valid_timesteps = []\n",
    "                for ts_dir in timestep_dirs:\n",
    "                    internal_file = ts_dir / \"internal.vtu\"\n",
    "                    if internal_file.exists():\n",
    "                        # Extract timestep number from directory name\n",
    "                        ts_match = re.search(rf'Reynolds_{reynolds_num}_(\\d+)', ts_dir.name)\n",
    "                        if ts_match:\n",
    "                            timestep = int(ts_match.group(1))\n",
    "                            valid_timesteps.append((timestep, ts_dir))\n",
    "                        else:\n",
    "                            # Try to extract just a number if the pattern doesn't match\n",
    "                            ts_match = re.search(r'(\\d+)$', ts_dir.name)\n",
    "                            if ts_match:\n",
    "                                timestep = int(ts_match.group(1))\n",
    "                                valid_timesteps.append((timestep, ts_dir))\n",
    "                \n",
    "                if valid_timesteps:\n",
    "                    valid_timesteps.sort()  # Sort by timestep\n",
    "                    # Include source directory index in case name to avoid conflicts\n",
    "                    case_name = f\"Reynolds_{reynolds_num}_dir{base_dir_idx}\"\n",
    "                    all_case_dirs.append((vtk_dir, case_name, reynolds_num, valid_timesteps, base_dir_idx))\n",
    "                    print(f\"    {re_dir.name}: {len(valid_timesteps)} timesteps (labeled as {case_name})\")\n",
    "                else:\n",
    "                    print(f\"    {re_dir.name}: No valid internal.vtu files found\")\n",
    "        \n",
    "        if not all_case_dirs:\n",
    "            print(\"\\nNo valid Reynolds cases found across all directories!\")\n",
    "        else:\n",
    "            print(f\"\\nTotal valid Reynolds cases across all directories: {len(all_case_dirs)}\")\n",
    "            \n",
    "            # Summary by directory\n",
    "            for dir_idx, base_dir in enumerate(self.base_data_dirs):\n",
    "                dir_cases = [c for c in all_case_dirs if c[4] == dir_idx]\n",
    "                print(f\"  Directory {dir_idx + 1} ({base_dir.name}): {len(dir_cases)} cases\")\n",
    "            \n",
    "        return all_case_dirs\n",
    "    \n",
    "    def read_vtu_data(self, filename):\n",
    "        \"\"\"Read VTU file and extract mesh data and fields.\"\"\"\n",
    "        reader = vtk.vtkXMLUnstructuredGridReader()\n",
    "        reader.SetFileName(str(filename))\n",
    "        reader.Update()\n",
    "        mesh = reader.GetOutput()\n",
    "        \n",
    "        # Extract positions\n",
    "        points = mesh.GetPoints()\n",
    "        pos = np.array([points.GetPoint(i) for i in range(points.GetNumberOfPoints())])\n",
    "        \n",
    "        # Extract point data (physics fields)\n",
    "        point_data = mesh.GetPointData()\n",
    "        fields = {}\n",
    "        \n",
    "        for i in range(point_data.GetNumberOfArrays()):\n",
    "            array = point_data.GetArray(i)\n",
    "            name = array.GetName()\n",
    "            n_components = array.GetNumberOfComponents()\n",
    "            \n",
    "            if name == 'p':  # Pressure field\n",
    "                data = np.array([array.GetValue(j) for j in range(array.GetNumberOfTuples())])\n",
    "                fields['pressure'] = data\n",
    "            elif name == 'U':  # Velocity vector field\n",
    "                data = np.array([array.GetTuple(j) for j in range(array.GetNumberOfTuples())])\n",
    "                fields['velocity_x'] = data[:, 0]\n",
    "                fields['velocity_y'] = data[:, 1] \n",
    "                fields['velocity_z'] = data[:, 2]\n",
    "            elif name == 'vorticity':  # Vorticity vector field\n",
    "                data = np.array([array.GetTuple(j) for j in range(array.GetNumberOfTuples())])\n",
    "                fields['vorticity_x'] = data[:, 0]\n",
    "                fields['vorticity_y'] = data[:, 1]\n",
    "                fields['vorticity_z'] = data[:, 2]\n",
    "            elif n_components == 1:\n",
    "                # Generic scalar field\n",
    "                data = np.array([array.GetValue(j) for j in range(array.GetNumberOfTuples())])\n",
    "                fields[name] = data\n",
    "            else:\n",
    "                # Generic vector field\n",
    "                data = np.array([array.GetTuple(j) for j in range(array.GetNumberOfTuples())])\n",
    "                for comp in range(n_components):\n",
    "                    fields[f\"{name}_{comp}\"] = data[:, comp]\n",
    "        \n",
    "        return pos, fields, mesh\n",
    "    \n",
    "    def extract_mesh_connectivity(self, mesh):\n",
    "        \"\"\"Extract mesh connectivity for graph structure.\"\"\"\n",
    "        edge_set = set()\n",
    "        n_cells = mesh.GetNumberOfCells()\n",
    "        \n",
    "        for i in range(n_cells):\n",
    "            cell = mesh.GetCell(i)\n",
    "            cell_type = cell.GetCellType()\n",
    "            n_points = cell.GetNumberOfPoints()\n",
    "            \n",
    "            point_ids = [cell.GetPointId(j) for j in range(n_points)]\n",
    "            edges = self._get_cell_edges(cell_type, point_ids)\n",
    "            \n",
    "            for edge in edges:\n",
    "                edge_set.add(tuple(sorted(edge)))\n",
    "        \n",
    "        # Convert to directed edges\n",
    "        directed_edges = []\n",
    "        for edge in edge_set:\n",
    "            directed_edges.append([edge[0], edge[1]])\n",
    "            directed_edges.append([edge[1], edge[0]])\n",
    "        \n",
    "        return np.array(directed_edges).T if directed_edges else np.array([[], []])\n",
    "    \n",
    "    def _get_cell_edges(self, cell_type, point_ids):\n",
    "        \"\"\"Get edges for different VTK cell types.\"\"\"\n",
    "        edges = []\n",
    "        \n",
    "        if cell_type == 12:  # Hexahedron\n",
    "            edges = [\n",
    "                # Bottom face\n",
    "                [point_ids[0], point_ids[1]], [point_ids[1], point_ids[2]], \n",
    "                [point_ids[2], point_ids[3]], [point_ids[3], point_ids[0]],\n",
    "                # Top face  \n",
    "                [point_ids[4], point_ids[5]], [point_ids[5], point_ids[6]], \n",
    "                [point_ids[6], point_ids[7]], [point_ids[7], point_ids[4]],\n",
    "                # Vertical edges\n",
    "                [point_ids[0], point_ids[4]], [point_ids[1], point_ids[5]], \n",
    "                [point_ids[2], point_ids[6]], [point_ids[3], point_ids[7]]\n",
    "            ]\n",
    "        elif cell_type == 13:  # Wedge\n",
    "            edges = [\n",
    "                [point_ids[0], point_ids[1]], [point_ids[1], point_ids[2]], [point_ids[2], point_ids[0]],\n",
    "                [point_ids[3], point_ids[4]], [point_ids[4], point_ids[5]], [point_ids[5], point_ids[3]],\n",
    "                [point_ids[0], point_ids[3]], [point_ids[1], point_ids[4]], [point_ids[2], point_ids[5]]\n",
    "            ]\n",
    "        else:  # Other types - connect all pairs (simplified)\n",
    "            for i in range(len(point_ids)):\n",
    "                for j in range(i + 1, len(point_ids)):\n",
    "                    edges.append([point_ids[i], point_ids[j]])\n",
    "        \n",
    "        return edges\n",
    "    \n",
    "    def create_global_parameters(self, reynolds_number):\n",
    "        \"\"\"Create global parameter tensor for the graph.\"\"\"\n",
    "        if not self.include_parameters_as_features:\n",
    "            return None\n",
    "        \n",
    "        global_params = torch.tensor([float(reynolds_number)], dtype=torch.float)\n",
    "        return global_params\n",
    "    \n",
    "    def analyze_variable_ranges_single_case(self, case_dir, case_name, reynolds_number, timesteps):\n",
    "        \"\"\"Analyze variable ranges for a single Reynolds case.\"\"\"\n",
    "        print(f\"  Analyzing variable ranges for {case_name}...\")\n",
    "        \n",
    "        case_stats = {var: {'min': float('inf'), 'max': float('-inf'), \n",
    "                           'all_values': []} for var in self.var_names}\n",
    "        \n",
    "        valid_files_count = 0\n",
    "        \n",
    "        for timestep, ts_dir in timesteps[:10]:  # Sample first 10 timesteps for efficiency\n",
    "            internal_file = ts_dir / \"internal.vtu\"\n",
    "            try:\n",
    "                pos, fields, mesh = self.read_vtu_data(internal_file)\n",
    "                \n",
    "                valid_files_count += 1\n",
    "                \n",
    "                # Analyze positions\n",
    "                for i, pos_var in enumerate(self.position_vars):\n",
    "                    data = pos[:, i]\n",
    "                    valid_data = data[np.isfinite(data)]\n",
    "                    if len(valid_data) > 0:\n",
    "                        case_stats[pos_var]['min'] = min(case_stats[pos_var]['min'], valid_data.min())\n",
    "                        case_stats[pos_var]['max'] = max(case_stats[pos_var]['max'], valid_data.max())\n",
    "                        case_stats[pos_var]['all_values'].extend(valid_data.flatten())\n",
    "                \n",
    "                # Analyze physics variables\n",
    "                for var in self.physics_vars:\n",
    "                    if var in fields:\n",
    "                        data = fields[var]\n",
    "                        valid_data = data[np.isfinite(data)]\n",
    "                        if len(valid_data) > 0:\n",
    "                            case_stats[var]['min'] = min(case_stats[var]['min'], valid_data.min())\n",
    "                            case_stats[var]['max'] = max(case_stats[var]['max'], valid_data.max())\n",
    "                            case_stats[var]['all_values'].extend(valid_data.flatten())\n",
    "                \n",
    "            except Exception as e:\n",
    "                continue  # Skip problematic files silently\n",
    "        \n",
    "        if valid_files_count == 0:\n",
    "            print(f\"    Warning: No valid files could be read for case {case_name}\")\n",
    "            return None\n",
    "        \n",
    "        # Compute additional statistics\n",
    "        final_stats = {}\n",
    "        for var in self.var_names:\n",
    "            if case_stats[var]['all_values'] and case_stats[var]['min'] != float('inf'):\n",
    "                values = np.array(case_stats[var]['all_values'])\n",
    "                final_stats[var] = {\n",
    "                    'min': float(case_stats[var]['min']),\n",
    "                    'max': float(case_stats[var]['max']),\n",
    "                    'mean': float(values.mean()),\n",
    "                    'std': float(values.std()),\n",
    "                    'median': float(np.median(values)),\n",
    "                    'q25': float(np.percentile(values, 25)),\n",
    "                    'q75': float(np.percentile(values, 75))\n",
    "                }\n",
    "        \n",
    "        return final_stats if final_stats else None\n",
    "    \n",
    "    def process_single_case(self, case_dir, case_name, reynolds_number, timesteps, output_path, source_dir_idx):\n",
    "        \"\"\"Process a single Reynolds case into PyG format - SHOCK TUBE FORMAT.\"\"\"\n",
    "        print(f\"Processing case: {case_name}\")\n",
    "        print(f\"  Reynolds number: {reynolds_number}\")\n",
    "        print(f\"  Source directory: {self.base_data_dirs[source_dir_idx]}\")\n",
    "        print(f\"  Timesteps: {len(timesteps)}\")\n",
    "        \n",
    "        if len(timesteps) < 2:\n",
    "            print(f\"  Case {case_name} has insufficient timesteps ({len(timesteps)}). Skipping.\")\n",
    "            return None\n",
    "        \n",
    "        # Store parameter statistics\n",
    "        self.parameter_stats[case_name] = {\n",
    "            'reynolds_number': reynolds_number,\n",
    "            'source_directory_index': source_dir_idx,\n",
    "            'source_directory': str(self.base_data_dirs[source_dir_idx])\n",
    "        }\n",
    "        \n",
    "        # Analyze variable ranges for this case\n",
    "        case_var_stats = self.analyze_variable_ranges_single_case(case_dir, case_name, reynolds_number, timesteps)\n",
    "        \n",
    "        pyg_data_list = []\n",
    "        \n",
    "        # Read first timestep to get mesh structure (static across timesteps)\n",
    "        first_timestep, first_ts_dir = timesteps[0]\n",
    "        first_internal = first_ts_dir / \"internal.vtu\"\n",
    "        pos, _, mesh = self.read_vtu_data(first_internal)\n",
    "        \n",
    "        # Extract connectivity (static)\n",
    "        edge_index = self.extract_mesh_connectivity(mesh)\n",
    "        edge_index_tensor = torch.tensor(edge_index, dtype=torch.long)\n",
    "        \n",
    "        # Create global parameter features\n",
    "        global_params = self.create_global_parameters(reynolds_number)\n",
    "        \n",
    "        # Process consecutive timestep pairs\n",
    "        valid_pairs = 0\n",
    "        total_pairs = len(timesteps) - 1\n",
    "        \n",
    "        for i in range(total_pairs):\n",
    "            if i % 50 == 0:\n",
    "                print(f\"    Processing timestep pairs: {i}/{total_pairs}\")\n",
    "                \n",
    "            current_timestep, current_ts_dir = timesteps[i]\n",
    "            next_timestep, next_ts_dir = timesteps[i + 1]\n",
    "            \n",
    "            try:\n",
    "                # Read current and next timestep data\n",
    "                current_file = current_ts_dir / \"internal.vtu\"\n",
    "                next_file = next_ts_dir / \"internal.vtu\"\n",
    "                \n",
    "                current_pos, current_fields, _ = self.read_vtu_data(current_file)\n",
    "                next_pos, next_fields, _ = self.read_vtu_data(next_file)\n",
    "                \n",
    "                # Create feature arrays - COMBINE POSITIONS + PHYSICS (like shock tube)\n",
    "                available_vars = [var for var in self.physics_vars if var in current_fields]\n",
    "                \n",
    "                # Build current features: [positions | physics]\n",
    "                current_features_list = [current_pos]  # Start with positions\n",
    "                for var in available_vars:\n",
    "                    current_features_list.append(current_fields[var].reshape(-1, 1))\n",
    "                current_features = np.hstack(current_features_list)\n",
    "                \n",
    "                # Build next features (targets are physics only, like shock tube)\n",
    "                next_features = np.stack([next_fields[var] for var in available_vars], axis=-1)\n",
    "                \n",
    "                # Create PyG tensors\n",
    "                x = torch.tensor(current_features, dtype=torch.float)  # [N, 3 positions + 7 physics]\n",
    "                y = torch.tensor(next_features, dtype=torch.float)     # [N, 7 physics only]\n",
    "                \n",
    "                # Also store positions separately (for compatibility)\n",
    "                pos_tensor = torch.tensor(current_pos, dtype=torch.float)\n",
    "                \n",
    "                # Create PyG Data object\n",
    "                data_object = Data(\n",
    "                    x=x,                              # Combined: positions + physics features\n",
    "                    pos=pos_tensor,                   # Also store positions separately\n",
    "                    edge_index=edge_index_tensor,     # Static connectivity\n",
    "                    y=y,                             # Target (next timestep physics only)\n",
    "                    \n",
    "                    # Global parameters\n",
    "                    global_params=global_params if self.include_parameters_as_features else None,\n",
    "                    \n",
    "                    # Metadata\n",
    "                    case_name=case_name,\n",
    "                    reynolds_number=reynolds_number,\n",
    "                    source_directory_index=source_dir_idx,\n",
    "                    timestep_current=current_timestep,\n",
    "                    timestep_next=next_timestep,\n",
    "                    feature_names=available_vars\n",
    "                )\n",
    "                \n",
    "                pyg_data_list.append(data_object)\n",
    "                valid_pairs += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                continue  # Skip problematic timestep pairs silently\n",
    "        \n",
    "        print(f\"  Successfully processed {valid_pairs} timestep pairs\")\n",
    "        \n",
    "        if pyg_data_list:\n",
    "            # Save this case separately\n",
    "            case_filename = f\"{case_name}_raw_data.pt\"\n",
    "            case_filepath = output_path / \"individual_cases\" / case_filename\n",
    "            case_filepath.parent.mkdir(exist_ok=True)\n",
    "            torch.save(pyg_data_list, case_filepath)\n",
    "            print(f\"  Saved individual case: {case_filepath}\")\n",
    "        \n",
    "        # Store case information\n",
    "        self.case_info[case_name] = {\n",
    "            'total_timesteps': len(timesteps),\n",
    "            'processed_pairs': len(pyg_data_list),\n",
    "            'directory': str(case_dir),\n",
    "            'reynolds_number': reynolds_number,\n",
    "            'source_directory_index': source_dir_idx,\n",
    "            'source_directory': str(self.base_data_dirs[source_dir_idx]),\n",
    "            'variable_statistics': case_var_stats,\n",
    "            'feature_dimensions': {\n",
    "                'node_features_total': len(self.position_vars) + len(available_vars) if pyg_data_list else 0,\n",
    "                'position_features': len(self.position_vars),\n",
    "                'physics_features': len(available_vars) if pyg_data_list else 0,\n",
    "                'global_features': 1 if self.include_parameters_as_features else 0,\n",
    "            },\n",
    "            'saved_file': str(case_filepath) if pyg_data_list else None\n",
    "        }\n",
    "        \n",
    "        return pyg_data_list\n",
    "    \n",
    "    def analyze_global_variable_ranges(self, case_info_list):\n",
    "        \"\"\"Analyze variable ranges across all Reynolds cases from all directories.\"\"\"\n",
    "        print(\"Analyzing variable ranges across ALL Reynolds cases from ALL directories\")\n",
    "        \n",
    "        global_stats = {var: {'min': float('inf'), 'max': float('-inf'), \n",
    "                             'case_mins': [], 'case_maxs': [], 'case_means': []} \n",
    "                       for var in self.var_names}\n",
    "        \n",
    "        case_stats_summary = {}\n",
    "        processed_cases = 0\n",
    "        \n",
    "        for case_dir, case_name, reynolds_number, timesteps, source_dir_idx in case_info_list:\n",
    "            case_var_stats = self.analyze_variable_ranges_single_case(case_dir, case_name, reynolds_number, timesteps)\n",
    "            \n",
    "            if case_var_stats is not None and case_var_stats:\n",
    "                case_stats_summary[case_name] = case_var_stats\n",
    "                processed_cases += 1\n",
    "                \n",
    "                for var in self.var_names:\n",
    "                    if var in case_var_stats and 'mean' in case_var_stats[var]:\n",
    "                        stats = case_var_stats[var]\n",
    "                        global_stats[var]['min'] = min(global_stats[var]['min'], stats['min'])\n",
    "                        global_stats[var]['max'] = max(global_stats[var]['max'], stats['max'])\n",
    "                        global_stats[var]['case_mins'].append(stats['min'])\n",
    "                        global_stats[var]['case_maxs'].append(stats['max'])\n",
    "                        global_stats[var]['case_means'].append(stats['mean'])\n",
    "        \n",
    "        # Compute global statistics\n",
    "        for var in self.var_names:\n",
    "            if global_stats[var]['case_means']:\n",
    "                global_stats[var]['global_mean_of_means'] = np.mean(global_stats[var]['case_means'])\n",
    "                global_stats[var]['std_of_means'] = np.std(global_stats[var]['case_means'])\n",
    "                global_stats[var]['range'] = global_stats[var]['max'] - global_stats[var]['min']\n",
    "        \n",
    "        print(f\"Global variable statistics summary (from {processed_cases} cases across {len(self.base_data_dirs)} directories):\")\n",
    "        for var in self.var_names:\n",
    "            stats = global_stats[var]\n",
    "            if stats['case_means']:\n",
    "                print(f\"  {var.upper()}:\")\n",
    "                print(f\"    Global Range: [{stats['min']:.6e}, {stats['max']:.6e}]\")\n",
    "                print(f\"    Range Span: {stats['range']:.6e}\")\n",
    "                print(f\"    Mean across cases: {stats['global_mean_of_means']:.6e} 췀 {stats['std_of_means']:.6e}\")\n",
    "        \n",
    "        self.all_variable_stats = {\n",
    "            'global_statistics': global_stats,\n",
    "            'case_statistics': case_stats_summary\n",
    "        }\n",
    "        \n",
    "        return global_stats, case_stats_summary\n",
    "    \n",
    "    def process_all_cases(self, output_dir=\"processed_karman_new_structure\"):\n",
    "        \"\"\"Process all discovered Reynolds cases from ALL directories.\"\"\"\n",
    "        print(\"BATCH PROCESSING ALL KARMAN VORTEX CASES FROM MULTIPLE DIRECTORIES\")\n",
    "        print(f\"Processing from {len(self.base_data_dirs)} source directories\")\n",
    "        \n",
    "        case_info_list = self.discover_reynolds_cases()\n",
    "        if not case_info_list:\n",
    "            print(\"No cases found to process!\")\n",
    "            return None\n",
    "        \n",
    "        # First, analyze variable ranges across all cases\n",
    "        global_stats, case_stats = self.analyze_global_variable_ranges(case_info_list)\n",
    "        \n",
    "        if global_stats is None:\n",
    "            print(\"Variable range analysis failed!\")\n",
    "            return None\n",
    "        \n",
    "        # Create output directory\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(exist_ok=True)\n",
    "        (output_path / \"individual_cases\").mkdir(exist_ok=True)\n",
    "        \n",
    "        all_data = []\n",
    "        case_sample_counts = {}\n",
    "        reynolds_numbers = []\n",
    "        \n",
    "        print(\"Processing individual Reynolds cases:\")\n",
    "        \n",
    "        # Process each case\n",
    "        for case_dir, case_name, reynolds_number, timesteps, source_dir_idx in case_info_list:\n",
    "            case_data = self.process_single_case(case_dir, case_name, reynolds_number, timesteps, output_path, source_dir_idx)\n",
    "            \n",
    "            if case_data:\n",
    "                all_data.extend(case_data)\n",
    "                case_sample_counts[case_name] = len(case_data)\n",
    "                reynolds_numbers.append(reynolds_number)\n",
    "            else:\n",
    "                print(f\"  No data processed for case: {case_name}\")\n",
    "        \n",
    "        if not all_data:\n",
    "            print(\"No data was successfully processed!\")\n",
    "            return None\n",
    "        \n",
    "        # Calculate Reynolds number statistics\n",
    "        reynolds_stats = {\n",
    "            'min': min(reynolds_numbers),\n",
    "            'max': max(reynolds_numbers),\n",
    "            'unique_values': sorted(list(set(reynolds_numbers)))\n",
    "        }\n",
    "        \n",
    "        print(\"Processing summary:\")\n",
    "        print(f\"  Total Reynolds cases processed: {len(case_sample_counts)}\")\n",
    "        print(f\"  Total samples: {len(all_data)}\")\n",
    "        print(f\"  Reynolds numbers: {reynolds_stats['unique_values']}\")\n",
    "        print(f\"  x.shape: [{all_data[0].x.shape[0]}, {all_data[0].x.shape[1]}] (nodes, features)\")\n",
    "        print(f\"    - First 3 columns: positions (x, y, z)\")\n",
    "        print(f\"    - Next 7 columns: physics (pressure, velocity_x/y/z, vorticity_x/y/z)\")\n",
    "        print(f\"  y.shape: [{all_data[0].y.shape[0]}, {all_data[0].y.shape[1]}] (nodes, physics only)\")\n",
    "        print(f\"  global_params.shape: [{all_data[0].global_params.shape[0]}] (Reynolds number)\")\n",
    "        \n",
    "        # Save comprehensive metadata\n",
    "        metadata = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'data_structure': 'NEW - Double-nested Reynolds directories',\n",
    "            'data_format': 'SHOCK_TUBE_COMPATIBLE - positions + physics in x',\n",
    "            'data_processing': 'RAW_DATA_ONLY - No derived features',\n",
    "            'source_directories': [str(d) for d in self.base_data_dirs],\n",
    "            'num_source_directories': len(self.base_data_dirs),\n",
    "            'total_samples': len(all_data),\n",
    "            'total_cases': len(case_sample_counts),\n",
    "            'include_parameters_as_features': self.include_parameters_as_features,\n",
    "            'parameter_storage_method': 'global_graph_attributes' if self.include_parameters_as_features else None,\n",
    "            'parameter_names': self.parameter_names if self.include_parameters_as_features else [],\n",
    "            'case_info': self.case_info,\n",
    "            'case_sample_counts': case_sample_counts,\n",
    "            'parameter_statistics': self.parameter_stats,\n",
    "            'variable_statistics': self.all_variable_stats,\n",
    "            'reynolds_summary': reynolds_stats,\n",
    "            'feature_layout': self._get_feature_layout_description(),\n",
    "            'target_layout': self.physics_vars,\n",
    "            'physics_model': 'Incompressible Navier-Stokes with Karman vortex shedding',\n",
    "            'domain_description': 'Flow around cylinder',\n",
    "            'mesh_type': 'Unstructured VTK mesh'\n",
    "        }\n",
    "        \n",
    "        with open(output_path / 'karman_dataset_metadata.json', 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        # Save variable statistics separately\n",
    "        with open(output_path / 'variable_statistics.json', 'w') as f:\n",
    "            json.dump(self.all_variable_stats, f, indent=2)\n",
    "        \n",
    "        return all_data, output_path\n",
    "    \n",
    "    def create_train_test_val_split(self, all_data, output_path, \n",
    "                                  train_reynolds=None, val_reynolds=None, test_reynolds=None,\n",
    "                                  train_ratio=0.7, val_ratio=0.15, test_ratio=0.15,\n",
    "                                  random_state=42):\n",
    "        \"\"\"\n",
    "        Create train/test/validation splits with manual Reynolds number specification.\n",
    "        \n",
    "        Args:\n",
    "            all_data: List of PyG Data objects\n",
    "            output_path: Path to save the split datasets\n",
    "            train_reynolds: List of Reynolds numbers for training (e.g., [50, 75, 100])\n",
    "            val_reynolds: List of Reynolds numbers for validation (e.g., [125])\n",
    "            test_reynolds: List of Reynolds numbers for testing (e.g., [150, 175])\n",
    "            train_ratio: Ratio for training split (used if manual split not specified)\n",
    "            val_ratio: Ratio for validation split (used if manual split not specified)\n",
    "            test_ratio: Ratio for testing split (used if manual split not specified)\n",
    "            random_state: Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        print(\"Creating train/test/val split\")\n",
    "        \n",
    "        # Group data by Reynolds number\n",
    "        reynolds_groups = {}\n",
    "        for data in all_data:\n",
    "            re_num = data.reynolds_number\n",
    "            if re_num not in reynolds_groups:\n",
    "                reynolds_groups[re_num] = []\n",
    "            reynolds_groups[re_num].append(data)\n",
    "        \n",
    "        available_reynolds = sorted(reynolds_groups.keys())\n",
    "        print(f\"  Found {len(reynolds_groups)} unique Reynolds numbers\")\n",
    "        print(f\"  Available Reynolds numbers: {available_reynolds}\")\n",
    "        \n",
    "        # Check if manual split is specified\n",
    "        manual_split = (train_reynolds is not None or val_reynolds is not None or test_reynolds is not None)\n",
    "        \n",
    "        if manual_split:\n",
    "            print(\"\\n  Using MANUAL Reynolds number assignment:\")\n",
    "            \n",
    "            # Convert to lists if not already\n",
    "            train_reynolds = train_reynolds if train_reynolds is not None else []\n",
    "            val_reynolds = val_reynolds if val_reynolds is not None else []\n",
    "            test_reynolds = test_reynolds if test_reynolds is not None else []\n",
    "            \n",
    "            # Validate that all specified Reynolds numbers exist\n",
    "            specified_reynolds = set(train_reynolds + val_reynolds + test_reynolds)\n",
    "            available_set = set(available_reynolds)\n",
    "            \n",
    "            missing = specified_reynolds - available_set\n",
    "            if missing:\n",
    "                print(f\"  WARNING: Specified Reynolds numbers not found in data: {sorted(missing)}\")\n",
    "            \n",
    "            unassigned = available_set - specified_reynolds\n",
    "            if unassigned:\n",
    "                print(f\"  WARNING: Reynolds numbers not assigned to any split: {sorted(unassigned)}\")\n",
    "                print(f\"           These will be EXCLUDED from all splits!\")\n",
    "            \n",
    "            print(f\"    Train Reynolds: {train_reynolds}\")\n",
    "            print(f\"    Val Reynolds:   {val_reynolds}\")\n",
    "            print(f\"    Test Reynolds:  {test_reynolds}\")\n",
    "            \n",
    "            # Assign samples based on manual specification\n",
    "            train_data = []\n",
    "            val_data = []\n",
    "            test_data = []\n",
    "            \n",
    "            for re_num in train_reynolds:\n",
    "                if re_num in reynolds_groups:\n",
    "                    train_data.extend(reynolds_groups[re_num])\n",
    "                    \n",
    "            for re_num in val_reynolds:\n",
    "                if re_num in reynolds_groups:\n",
    "                    val_data.extend(reynolds_groups[re_num])\n",
    "                    \n",
    "            for re_num in test_reynolds:\n",
    "                if re_num in reynolds_groups:\n",
    "                    test_data.extend(reynolds_groups[re_num])\n",
    "            \n",
    "            split_method = 'manual_reynolds_specification'\n",
    "            \n",
    "        else:\n",
    "            # Automatic split by ratio\n",
    "            print(f\"\\n  Using AUTOMATIC split by ratios:\")\n",
    "            print(f\"    Train: {train_ratio}, Val: {val_ratio}, Test: {test_ratio}\")\n",
    "            \n",
    "            if abs(train_ratio + val_ratio + test_ratio - 1.0) > 1e-6:\n",
    "                print(\"  ERROR: Split ratios don't sum to 1.0!\")\n",
    "                return None\n",
    "            \n",
    "            # Split Reynolds numbers by ratio\n",
    "            n_reynolds = len(available_reynolds)\n",
    "            train_end = int(n_reynolds * train_ratio)\n",
    "            val_end = train_end + int(n_reynolds * val_ratio)\n",
    "            \n",
    "            train_reynolds = available_reynolds[:train_end]\n",
    "            val_reynolds = available_reynolds[train_end:val_end]\n",
    "            test_reynolds = available_reynolds[val_end:]\n",
    "            \n",
    "            print(f\"    Train Reynolds: {train_reynolds}\")\n",
    "            print(f\"    Val Reynolds:   {val_reynolds}\")\n",
    "            print(f\"    Test Reynolds:  {test_reynolds}\")\n",
    "            \n",
    "            # Assign ALL samples from each Reynolds to designated split\n",
    "            train_data = []\n",
    "            val_data = []\n",
    "            test_data = []\n",
    "            \n",
    "            for re_num in train_reynolds:\n",
    "                train_data.extend(reynolds_groups[re_num])\n",
    "            for re_num in val_reynolds:\n",
    "                val_data.extend(reynolds_groups[re_num])\n",
    "            for re_num in test_reynolds:\n",
    "                test_data.extend(reynolds_groups[re_num])\n",
    "            \n",
    "            split_method = 'automatic_reynolds_by_ratio'\n",
    "        \n",
    "        print(f\"\\nFinal split sizes:\")\n",
    "        print(f\"  Train: {len(train_data)} samples ({len(train_data)/len(all_data)*100:.1f}%)\")\n",
    "        print(f\"  Val:   {len(val_data)} samples ({len(val_data)/len(all_data)*100:.1f}%)\")\n",
    "        print(f\"  Test:  {len(test_data)} samples ({len(test_data)/len(all_data)*100:.1f}%)\")\n",
    "        \n",
    "        # Check for empty splits\n",
    "        if len(train_data) == 0:\n",
    "            print(\"  WARNING: Training set is empty!\")\n",
    "        if len(val_data) == 0:\n",
    "            print(\"  WARNING: Validation set is empty!\")\n",
    "        if len(test_data) == 0:\n",
    "            print(\"  WARNING: Test set is empty!\")\n",
    "        \n",
    "        # Save the datasets\n",
    "        datasets = {\n",
    "            'train': train_data,\n",
    "            'val': val_data,\n",
    "            'test': test_data\n",
    "        }\n",
    "        \n",
    "        saved_files = {}\n",
    "        for split_name, split_data in datasets.items():\n",
    "            if len(split_data) > 0:\n",
    "                filename = f\"karman_vortex_{split_name}_with_pos.pt\"\n",
    "                filepath = output_path / filename\n",
    "                torch.save(split_data, filepath)\n",
    "                saved_files[split_name] = str(filepath)\n",
    "                print(f\"  Saved {split_name} split: {filepath}\")\n",
    "            else:\n",
    "                print(f\"  Skipped {split_name} split (empty)\")\n",
    "                saved_files[split_name] = None\n",
    "        \n",
    "        # Update metadata with split information\n",
    "        split_metadata = {\n",
    "            'split_method': split_method,\n",
    "            'split_ratios': {'train': train_ratio, 'val': val_ratio, 'test': test_ratio} if not manual_split else 'manual',\n",
    "            'split_sizes': {\n",
    "                'train': len(train_data), \n",
    "                'val': len(val_data), \n",
    "                'test': len(test_data)\n",
    "            },\n",
    "            'reynolds_assignment': {\n",
    "                'train_reynolds': train_reynolds,\n",
    "                'val_reynolds': val_reynolds,\n",
    "                'test_reynolds': test_reynolds\n",
    "            },\n",
    "            'random_state': random_state,\n",
    "            'saved_files': saved_files\n",
    "        }\n",
    "        \n",
    "        # Load existing metadata and add split info\n",
    "        metadata_file = output_path / 'karman_dataset_metadata.json'\n",
    "        with open(metadata_file, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        metadata['train_test_val_split'] = split_metadata\n",
    "        \n",
    "        with open(metadata_file, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nUpdated metadata: {metadata_file}\")\n",
    "        \n",
    "        return datasets\n",
    "    \n",
    "    def _get_feature_layout_description(self):\n",
    "        \"\"\"Get description of the feature layout.\"\"\"\n",
    "        if self.include_parameters_as_features:\n",
    "            return {\n",
    "                'x_tensor': '[x_pos, y_pos, z_pos, pressure, velocity_x, velocity_y, velocity_z, vorticity_x, vorticity_y, vorticity_z] (positions + physics)',\n",
    "                'y_tensor': '[pressure, velocity_x, velocity_y, velocity_z, vorticity_x, vorticity_y, vorticity_z] (physics only)',\n",
    "                'global_params': '[reynolds_number]',\n",
    "                'num_static_feats': 3,\n",
    "                'num_dynamic_feats': 7,\n",
    "                'num_global_feats': 1\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'x_tensor': '[x_pos, y_pos, z_pos, pressure, velocity_x, velocity_y, velocity_z, vorticity_x, vorticity_y, vorticity_z] (positions + physics)',\n",
    "                'y_tensor': '[pressure, velocity_x, velocity_y, velocity_z, vorticity_x, vorticity_y, vorticity_z] (physics only)',\n",
    "                'global_params': 'none'\n",
    "            }\n",
    "\n",
    "\n",
    "class KarmanDatasetNormalizerNewStructure:\n",
    "    \"\"\"\n",
    "    Normalizer for Karman vortex street datasets - matches shock tube format.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, source_dir, target_dir, metadata_file=\"karman_dataset_metadata.json\"):\n",
    "        self.source_dir = Path(source_dir)\n",
    "        self.target_dir = Path(target_dir)\n",
    "        self.target_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Variable names - positions + physics (like shock tube)\n",
    "        self.position_vars = ['x_pos', 'y_pos', 'z_pos']\n",
    "        self.physics_vars = ['pressure', 'velocity_x', 'velocity_y', 'velocity_z', \n",
    "                           'vorticity_x', 'vorticity_y', 'vorticity_z']\n",
    "        self.var_names = self.position_vars + self.physics_vars\n",
    "        \n",
    "        # Load metadata\n",
    "        metadata_path = self.source_dir / metadata_file\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            self.metadata = json.load(f)\n",
    "        \n",
    "        # Extract normalization parameters\n",
    "        self.normalization_params = self._compute_normalization_params()\n",
    "        self.global_param_normalization = self._compute_global_param_normalization()\n",
    "        \n",
    "    def _compute_normalization_params(self):\n",
    "        \"\"\"Compute global min/max for each variable from metadata.\"\"\"\n",
    "        print(\"Computing global normalization parameters...\")\n",
    "        \n",
    "        params = {}\n",
    "        global_stats = self.metadata['variable_statistics']['global_statistics']\n",
    "        \n",
    "        for var in self.var_names:\n",
    "            if var in global_stats:\n",
    "                var_min = global_stats[var]['min']\n",
    "                var_max = global_stats[var]['max']\n",
    "                params[var] = {'min': var_min, 'max': var_max}\n",
    "                print(f\"  {var}: [{var_min:.6e}, {var_max:.6e}]\")\n",
    "            else:\n",
    "                print(f\"  Warning: {var} not found in global statistics\")\n",
    "        \n",
    "        return params\n",
    "    \n",
    "    def _compute_global_param_normalization(self):\n",
    "        \"\"\"Compute normalization for global parameters (Reynolds number).\"\"\"\n",
    "        print(\"Computing global parameter normalization...\")\n",
    "        \n",
    "        if 'reynolds_summary' in self.metadata:\n",
    "            reynolds_stats = self.metadata['reynolds_summary']\n",
    "            return {\n",
    "                'reynolds_number': {\n",
    "                    'min': float(reynolds_stats['min']),\n",
    "                    'max': float(reynolds_stats['max'])\n",
    "                }\n",
    "            }\n",
    "        return {}\n",
    "    \n",
    "    def normalize_tensor(self, data, var_name, param_dict=None):\n",
    "        \"\"\"Normalize a tensor using min-max normalization.\"\"\"\n",
    "        if param_dict is None:\n",
    "            param_dict = self.normalization_params\n",
    "        \n",
    "        if var_name not in param_dict:\n",
    "            print(f\"Warning: {var_name} not found in normalization parameters\")\n",
    "            return data.clone()\n",
    "        \n",
    "        params = param_dict[var_name]\n",
    "        var_min, var_max = params['min'], params['max']\n",
    "        \n",
    "        if var_max == var_min:\n",
    "            return torch.zeros_like(data)\n",
    "        \n",
    "        normalized = (data - var_min) / (var_max - var_min)\n",
    "        normalized = torch.clamp(normalized, 0.0, 1.0)\n",
    "        \n",
    "        return normalized\n",
    "    \n",
    "    def normalize_global_params(self, global_params):\n",
    "        \"\"\"Normalize global parameters (Reynolds number).\"\"\"\n",
    "        if global_params is None or len(self.global_param_normalization) == 0:\n",
    "            return global_params.clone() if global_params is not None else None\n",
    "        \n",
    "        normalized_params = global_params.clone()\n",
    "        \n",
    "        if 'reynolds_number' in self.global_param_normalization:\n",
    "            normalized_params[0] = self.normalize_tensor(\n",
    "                global_params[0], 'reynolds_number', self.global_param_normalization\n",
    "            )\n",
    "        \n",
    "        return normalized_params\n",
    "    \n",
    "    def normalize_sample(self, data):\n",
    "        \"\"\"Normalize a single PyG Data object.\"\"\"\n",
    "        # Normalize x (positions + physics)\n",
    "        x_normalized = data.x.clone()\n",
    "        for i, var_name in enumerate(self.var_names):\n",
    "            if i < data.x.shape[1]:\n",
    "                x_normalized[:, i] = self.normalize_tensor(data.x[:, i], var_name)\n",
    "        \n",
    "        # Normalize y (physics only)\n",
    "        y_normalized = data.y.clone()\n",
    "        for i, var_name in enumerate(self.physics_vars):\n",
    "            if i < data.y.shape[1]:\n",
    "                y_normalized[:, i] = self.normalize_tensor(data.y[:, i], var_name)\n",
    "        \n",
    "        # Normalize pos separately (for compatibility)\n",
    "        pos_normalized = data.pos.clone()\n",
    "        for i, var_name in enumerate(self.position_vars):\n",
    "            if i < data.pos.shape[1]:\n",
    "                pos_normalized[:, i] = self.normalize_tensor(data.pos[:, i], var_name)\n",
    "        \n",
    "        # Create normalized data object\n",
    "        normalized_data = data.clone()\n",
    "        normalized_data.x = x_normalized\n",
    "        normalized_data.y = y_normalized\n",
    "        normalized_data.pos = pos_normalized\n",
    "        \n",
    "        # Normalize global parameters\n",
    "        if hasattr(data, 'global_params') and data.global_params is not None:\n",
    "            normalized_data.global_params = self.normalize_global_params(data.global_params)\n",
    "        \n",
    "        return normalized_data\n",
    "    \n",
    "    def normalize_dataset_file(self, input_file, output_file):\n",
    "        \"\"\"Normalize a single dataset file.\"\"\"\n",
    "        print(f\"  Loading: {input_file}\")\n",
    "        dataset = torch.load(input_file, weights_only=False)\n",
    "        \n",
    "        print(f\"  Normalizing {len(dataset)} samples...\")\n",
    "        normalized_dataset = []\n",
    "        \n",
    "        for i, data in enumerate(dataset):\n",
    "            if i % 100 == 0:\n",
    "                print(f\"    Progress: {i}/{len(dataset)}\")\n",
    "            \n",
    "            normalized_data = self.normalize_sample(data)\n",
    "            normalized_dataset.append(normalized_data)\n",
    "        \n",
    "        print(f\"  Saving: {output_file}\")\n",
    "        torch.save(normalized_dataset, output_file)\n",
    "        \n",
    "        return len(normalized_dataset)\n",
    "    \n",
    "    def normalize_all_datasets(self):\n",
    "        \"\"\"Normalize all dataset files.\"\"\"\n",
    "        print(\"NORMALIZING KARMAN VORTEX DATASETS - SHOCK TUBE FORMAT\")\n",
    "        \n",
    "        total_samples = 0\n",
    "        \n",
    "        # Normalize main split files\n",
    "        splits = ['train', 'val', 'test']\n",
    "        for split in splits:\n",
    "            input_file = self.source_dir / f\"karman_vortex_{split}_with_pos.pt\"\n",
    "            output_file = self.target_dir / f\"karman_vortex_{split}_normalized.pt\"\n",
    "            \n",
    "            if input_file.exists():\n",
    "                samples = self.normalize_dataset_file(input_file, output_file)\n",
    "                total_samples += samples\n",
    "                print(f\"  {split}: {samples} samples normalized\")\n",
    "            else:\n",
    "                print(f\"  {split}: File not found\")\n",
    "        \n",
    "        # Normalize individual cases\n",
    "        individual_cases_dir = self.source_dir / \"individual_cases\"\n",
    "        if individual_cases_dir.exists():\n",
    "            normalized_cases_dir = self.target_dir / \"individual_cases_normalized\"\n",
    "            normalized_cases_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            case_files = list(individual_cases_dir.glob(\"*.pt\"))\n",
    "            print(f\"Normalizing {len(case_files)} individual cases...\")\n",
    "            \n",
    "            for case_file in case_files:\n",
    "                output_file = normalized_cases_dir / f\"{case_file.stem}_normalized.pt\"\n",
    "                samples = self.normalize_dataset_file(case_file, output_file)\n",
    "                total_samples += samples\n",
    "        \n",
    "        # Save normalization metadata\n",
    "        norm_metadata = {\n",
    "            'normalization_method': 'min_max_0_to_1',\n",
    "            'data_format': 'shock_tube_compatible',\n",
    "            'normalization_params': self.normalization_params,\n",
    "            'global_param_normalization': self.global_param_normalization,\n",
    "            'original_metadata': self.metadata\n",
    "        }\n",
    "        \n",
    "        with open(self.target_dir / 'normalization_metadata.json', 'w') as f:\n",
    "            json.dump(norm_metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"NORMALIZATION COMPLETE!\")\n",
    "        print(f\"Total samples normalized: {total_samples}\")\n",
    "        \n",
    "        return total_samples\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function - WITH MANUAL REYNOLDS SPLIT OPTION.\"\"\"\n",
    "    \n",
    "    # CHANGE THESE PATHS TO YOUR TWO DIRECTORIES\n",
    "    BASE_DATA_DIRS = [\n",
    "        \"/standard/sds_baek_energetic/von_karman_vortex/Reynolds 200~\",\n",
    "        \"/standard/sds_baek_energetic/von_karman_vortex/Reynolds 1~150\"\n",
    "    ]\n",
    "    \n",
    "    OUTPUT_DIR = \"/standard/sds_baek_energetic/von_karman_vortex/full_data\"\n",
    "    \n",
    "    print(\"KARMAN VORTEX STREET DATASET PROCESSOR - MULTIPLE DIRECTORIES\")\n",
    "    print(\"SHOCK TUBE FORMAT: positions + physics in x tensor\")\n",
    "    print(f\"Sources:\")\n",
    "    for i, dir_path in enumerate(BASE_DATA_DIRS, 1):\n",
    "        print(f\"  {i}. {dir_path}\")\n",
    "    print(f\"Output: {OUTPUT_DIR}\")\n",
    "    \n",
    "    # Initialize processor with multiple directories\n",
    "    processor = KarmanVortexProcessorNewStructure(BASE_DATA_DIRS)\n",
    "    \n",
    "    # Process all cases from all directories\n",
    "    result = processor.process_all_cases(output_dir=OUTPUT_DIR)\n",
    "    \n",
    "    if result is None:\n",
    "        print(\"Processing failed!\")\n",
    "        return\n",
    "    \n",
    "    all_data, output_path = result\n",
    "    \n",
    "    print(\"READY FOR TRAIN/TEST/VAL SPLIT\")\n",
    "    print(\"Variable range analysis complete\")\n",
    "    print(\"Individual simulations saved\")\n",
    "    \n",
    "    # ============================================================\n",
    "    # MANUAL REYNOLDS NUMBER SPLIT - MODIFY THESE LISTS!\n",
    "    # ============================================================\n",
    "    # Option 1: Specify exact Reynolds numbers for each split\n",
    "    MANUAL_SPLIT = True  # Set to False to use automatic ratio-based split\n",
    "    \n",
    "    if MANUAL_SPLIT:\n",
    "        # CUSTOMIZE THESE LISTS FOR YOUR DESIRED SPLIT\n",
    "        TRAIN_REYNOLDS = [40, 50, 100, 150, 250, 400, 450, 500, 550, 700, 750, 850, 950]      # Training Reynolds numbers\n",
    "        VAL_REYNOLDS = [20, 200, 300, 650, 1000]                      # Validation Reynolds numbers  \n",
    "        TEST_REYNOLDS = [1, 350, 600, 800, 900]                # Test Reynolds numbers\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"USING MANUAL REYNOLDS NUMBER SPLIT\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        datasets = processor.create_train_test_val_split(\n",
    "            all_data, \n",
    "            output_path,\n",
    "            train_reynolds=TRAIN_REYNOLDS,\n",
    "            val_reynolds=VAL_REYNOLDS,\n",
    "            test_reynolds=TEST_REYNOLDS\n",
    "        )\n",
    "    else:\n",
    "        # Option 2: Use automatic ratio-based split\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"USING AUTOMATIC RATIO-BASED SPLIT\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        datasets = processor.create_train_test_val_split(\n",
    "            all_data, \n",
    "            output_path,\n",
    "            train_ratio=0.7,\n",
    "            val_ratio=0.15,\n",
    "            test_ratio=0.15\n",
    "        )\n",
    "    # ============================================================\n",
    "    \n",
    "    if datasets:\n",
    "        print(\"\\nNORMALIZING DATASETS\")\n",
    "        normalizer = KarmanDatasetNormalizerNewStructure(output_path, output_path / \"normalized\")\n",
    "        normalizer.normalize_all_datasets()\n",
    "    \n",
    "    print(\"\\nKARMAN VORTEX PROCESSING COMPLETE!\")\n",
    "    print(f\"All outputs saved to: {output_path}\")\n",
    "    print(\"Generated files:\")\n",
    "    print(\"  - karman_vortex_train_with_pos.pt\")\n",
    "    print(\"  - karman_vortex_val_with_pos.pt\")\n",
    "    print(\"  - karman_vortex_test_with_pos.pt\")\n",
    "    print(\"  - normalized/karman_vortex_*_normalized.pt\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd066362-520a-4cd9-9652-22a2aadc27bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SPLIT & NORMALIZE INDIVIDUAL CASE FILES BY REYNOLDS NUMBER\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  Cases directory: /standard/sds_baek_energetic/von_karman_vortex/full_data/individual_cases\n",
      "  Output directory: /standard/sds_baek_energetic/von_karman_vortex/full_data/split_normalized\n",
      "  Metadata file: /standard/sds_baek_energetic/von_karman_vortex/full_data/karman_dataset_metadata.json\n",
      "Reynolds number splits:\n",
      "  Train: [40, 50, 100, 150, 250, 400, 450, 500, 550, 700, 750, 850, 950]\n",
      "  Val:   [20, 200, 300, 650, 1000]\n",
      "  Test:  [1, 350, 600, 800, 900]\n",
      "\n",
      "Loading metadata from: /standard/sds_baek_energetic/von_karman_vortex/full_data/karman_dataset_metadata.json\n",
      "  Computing normalization parameters from dataset statistics...\n",
      "    x_pos          : [-5.000000e-02, 1.500000e-01]\n",
      "    y_pos          : [-5.000000e-02, 5.000000e-02]\n",
      "    z_pos          : [0.000000e+00, 1.000000e-02]\n",
      "    pressure       : [-4.066926e+00, 1.581088e+00]\n",
      "    velocity_x     : [-2.089695e+00, 3.098995e+00]\n",
      "    velocity_y     : [-2.557230e+00, 2.146482e+00]\n",
      "    velocity_z     : [-3.454671e-19, 3.312376e-19]\n",
      "    vorticity_x    : [-2.844113e-14, 5.295167e-14]\n",
      "    vorticity_y    : [-3.342095e-14, 2.517809e-14]\n",
      "    vorticity_z    : [-2.488884e+04, 2.035310e+04]\n",
      "  Computing global parameter normalization...\n",
      "    reynolds_number: [1.0, 1000.0]\n",
      "九 Initialized successfully\n",
      "\n",
      "\n",
      "======================================================================\n",
      "PROCESSING 23 CASE FILES\n",
      "======================================================================\n",
      "\n",
      "[1/23] ============================================================\n",
      "\n",
      "Processing: Reynolds_1000_dir0_raw_data.pt\n",
      "  Reynolds: 1000  Split: VAL\n",
      "  Loading dataset...\n",
      "  Loaded 400 samples\n",
      "  Normalizing...\n",
      "    Progress: 400/400\n",
      "  Saving to: Re_1000_normalized.pt\n",
      "  九 Complete: 400 samples saved\n",
      "\n",
      "[2/23] ============================================================\n",
      "\n",
      "Processing: Reynolds_100_dir1_raw_data.pt\n",
      "  Reynolds: 100  Split: TRAIN\n",
      "  Loading dataset...\n",
      "  Loaded 400 samples\n",
      "  Normalizing...\n",
      "    Progress: 400/400\n",
      "  Saving to: Re_100_normalized.pt\n",
      "  九 Complete: 400 samples saved\n",
      "\n",
      "[3/23] ============================================================\n",
      "\n",
      "Processing: Reynolds_150_dir1_raw_data.pt\n",
      "  Reynolds: 150  Split: TRAIN\n",
      "  Loading dataset...\n",
      "  Loaded 400 samples\n",
      "  Normalizing...\n",
      "    Progress: 400/400\n",
      "  Saving to: Re_150_normalized.pt\n",
      "  九 Complete: 400 samples saved\n",
      "\n",
      "[4/23] ============================================================\n",
      "\n",
      "Processing: Reynolds_1_dir1_raw_data.pt\n",
      "  Reynolds: 1  Split: TEST\n",
      "  Loading dataset...\n",
      "  Loaded 400 samples\n",
      "  Normalizing...\n",
      "    Progress: 400/400\n",
      "  Saving to: Re_1_normalized.pt\n",
      "  九 Complete: 400 samples saved\n",
      "\n",
      "[5/23] ============================================================\n",
      "\n",
      "Processing: Reynolds_200_dir0_raw_data.pt\n",
      "  Reynolds: 200  Split: VAL\n",
      "  Loading dataset...\n",
      "  Loaded 400 samples\n",
      "  Normalizing...\n",
      "    Progress: 400/400\n",
      "  Saving to: Re_200_normalized.pt\n",
      "  九 Complete: 400 samples saved\n",
      "\n",
      "[6/23] ============================================================\n",
      "\n",
      "Processing: Reynolds_20_dir1_raw_data.pt\n",
      "  Reynolds: 20  Split: VAL\n",
      "  Loading dataset...\n",
      "  Loaded 400 samples\n",
      "  Normalizing...\n",
      "    Progress: 400/400\n",
      "  Saving to: Re_20_normalized.pt\n",
      "  九 Complete: 400 samples saved\n",
      "\n",
      "[7/23] ============================================================\n",
      "\n",
      "Processing: Reynolds_250_dir0_raw_data.pt\n",
      "  Reynolds: 250  Split: TRAIN\n",
      "  Loading dataset...\n",
      "  Loaded 400 samples\n",
      "  Normalizing...\n",
      "    Progress: 400/400\n",
      "  Saving to: Re_250_normalized.pt\n",
      "  九 Complete: 400 samples saved\n",
      "\n",
      "[8/23] ============================================================\n",
      "\n",
      "Processing: Reynolds_300_dir0_raw_data.pt\n",
      "  Reynolds: 300  Split: VAL\n",
      "  Loading dataset...\n",
      "  Loaded 400 samples\n",
      "  Normalizing...\n",
      "    Progress: 400/400\n",
      "  Saving to: Re_300_normalized.pt\n",
      "  九 Complete: 400 samples saved\n",
      "\n",
      "[9/23] ============================================================\n",
      "\n",
      "Processing: Reynolds_350_dir0_raw_data.pt\n",
      "  Reynolds: 350  Split: TEST\n",
      "  Loading dataset...\n",
      "  Loaded 400 samples\n",
      "  Normalizing...\n",
      "    Progress: 400/400\n",
      "  Saving to: Re_350_normalized.pt\n",
      "  九 Complete: 400 samples saved\n",
      "\n",
      "[10/23] ============================================================\n",
      "\n",
      "Processing: Reynolds_400_dir0_raw_data.pt\n",
      "  Reynolds: 400  Split: TRAIN\n",
      "  Loading dataset...\n",
      "  Loaded 400 samples\n",
      "  Normalizing...\n",
      "    Progress: 400/400\n",
      "  Saving to: Re_400_normalized.pt\n",
      "  九 Complete: 400 samples saved\n",
      "\n",
      "[11/23] ============================================================\n",
      "\n",
      "Processing: Reynolds_40_dir1_raw_data.pt\n",
      "  Reynolds: 40  Split: TRAIN\n",
      "  Loading dataset...\n",
      "  Loaded 400 samples\n",
      "  Normalizing...\n",
      "    Progress: 400/400\n",
      "  Saving to: Re_40_normalized.pt\n",
      "  九 Complete: 400 samples saved\n",
      "\n",
      "[12/23] ============================================================\n",
      "\n",
      "Processing: Reynolds_450_dir0_raw_data.pt\n",
      "  Reynolds: 450  Split: TRAIN\n",
      "  Loading dataset...\n",
      "  Loaded 400 samples\n",
      "  Normalizing...\n",
      "    Progress: 400/400\n",
      "  Saving to: Re_450_normalized.pt\n",
      "  九 Complete: 400 samples saved\n",
      "\n",
      "[13/23] ============================================================\n",
      "\n",
      "Processing: Reynolds_500_dir0_raw_data.pt\n",
      "  Reynolds: 500  Split: TRAIN\n",
      "  Loading dataset...\n",
      "  Loaded 400 samples\n",
      "  Normalizing...\n",
      "    Progress: 400/400\n",
      "  Saving to: Re_500_normalized.pt\n",
      "  九 Complete: 400 samples saved\n",
      "\n",
      "[14/23] ============================================================\n",
      "\n",
      "Processing: Reynolds_50_dir1_raw_data.pt\n",
      "  Reynolds: 50  Split: TRAIN\n",
      "  Loading dataset...\n",
      "  Loaded 400 samples\n",
      "  Normalizing...\n",
      "    Progress: 400/400\n",
      "  Saving to: Re_50_normalized.pt\n",
      "  九 Complete: 400 samples saved\n",
      "\n",
      "[15/23] ============================================================\n",
      "\n",
      "Processing: Reynolds_550_dir0_raw_data.pt\n",
      "  Reynolds: 550  Split: TRAIN\n",
      "  Loading dataset...\n",
      "  Loaded 400 samples\n",
      "  Normalizing...\n",
      "    Progress: 400/400\n",
      "  Saving to: Re_550_normalized.pt\n",
      "  九 Complete: 400 samples saved\n",
      "\n",
      "[16/23] ============================================================\n",
      "\n",
      "Processing: Reynolds_600_dir0_raw_data.pt\n",
      "  Reynolds: 600  Split: TEST\n",
      "  Loading dataset...\n",
      "  Loaded 400 samples\n",
      "  Normalizing...\n",
      "    Progress: 400/400\n",
      "  Saving to: Re_600_normalized.pt\n",
      "  九 Complete: 400 samples saved\n",
      "\n",
      "[17/23] ============================================================\n",
      "\n",
      "Processing: Reynolds_650_dir0_raw_data.pt\n",
      "  Reynolds: 650  Split: VAL\n",
      "  Loading dataset...\n",
      "  Loaded 400 samples\n",
      "  Normalizing...\n",
      "  Loaded 400 samples0\n",
      "  Normalizing...\n",
      "    Progress: 400/400\n",
      "  Saving to: Re_750_normalized.pt\n",
      "  九 Complete: 400 samples saved\n",
      "\n",
      "[20/23] ============================================================\n",
      "\n",
      "Processing: Reynolds_800_dir0_raw_data.pt\n",
      "  Reynolds: 800  Split: TEST\n",
      "  Loading dataset...\n",
      "  Loaded 400 samples\n",
      "  Normalizing...\n",
      "    Progress: 400/400\n",
      "  Saving to: Re_800_normalized.pt\n",
      "  九 Complete: 400 samples saved\n",
      "\n",
      "[21/23] ============================================================\n",
      "\n",
      "Processing: Reynolds_850_dir0_raw_data.pt\n",
      "  Reynolds: 850  Split: TRAIN\n",
      "  Loading dataset...\n",
      "  Loaded 400 samples\n",
      "  Normalizing...\n",
      "    Progress: 400/400\n",
      "  Saving to: Re_850_normalized.pt\n",
      "  九 Complete: 400 samples saved\n",
      "\n",
      "[22/23] ============================================================\n",
      "\n",
      "Processing: Reynolds_900_dir0_raw_data.pt\n",
      "  Reynolds: 900  Split: TEST\n",
      "  Loading dataset...\n",
      "  Loaded 400 samples\n",
      "  Normalizing...\n",
      "    Progress: 400/400\n",
      "  Saving to: Re_900_normalized.pt\n",
      "  九 Complete: 400 samples saved\n",
      "\n",
      "[23/23] ============================================================\n",
      "\n",
      "Processing: Reynolds_950_dir0_raw_data.pt\n",
      "  Reynolds: 950  Split: TRAIN\n",
      "  Loading dataset...\n",
      "  Loaded 400 samples\n",
      "  Normalizing...\n",
      "    Progress: 400/400\n",
      "  Saving to: Re_950_normalized.pt\n",
      "  九 Complete: 400 samples saved\n",
      "\n",
      "======================================================================\n",
      "PROCESSING COMPLETE - SUMMARY\n",
      "======================================================================\n",
      "Files processed: 23\n",
      "\n",
      "Samples by split:\n",
      "  Train:     5200 samples (Re: [40, 50, 100, 150, 250, 400, 450, 500, 550, 700, 750, 850, 950])\n",
      "  Val:       2000 samples (Re: [20, 200, 300, 650, 1000])\n",
      "  Test:      2000 samples (Re: [1, 350, 600, 800, 900])\n",
      "\n",
      "Summary saved to: /standard/sds_baek_energetic/von_karman_vortex/full_data/split_normalized/split_summary.json\n",
      "Output directory: /standard/sds_baek_energetic/von_karman_vortex/full_data/split_normalized\n",
      "======================================================================\n",
      "\n",
      "九 All processing complete!\n",
      "\n",
      "Output structure:\n",
      "  /standard/sds_baek_energetic/von_karman_vortex/full_data/split_normalized/\n",
      "    train/\n",
      "      Re_40_normalized.pt\n",
      "      Re_50_normalized.pt\n",
      "      ...\n",
      "    val/\n",
      "      Re_20_normalized.pt\n",
      "      ...\n",
      "    test/\n",
      "      Re_1_normalized.pt\n",
      "      ...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import gc\n",
    "from datetime import datetime\n",
    "\n",
    "class SimpleReynoldsSplitter:\n",
    "    \"\"\"\n",
    "    Simple processor for splitting and normalizing individual Karman case files.\n",
    "    Splits based on Reynolds numbers for physics-informed learning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cases_dir, output_dir, metadata_file, \n",
    "                 train_reynolds, val_reynolds, test_reynolds):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cases_dir: Directory containing individual case files\n",
    "            output_dir: Directory to save split and normalized data\n",
    "            metadata_file: Path to karman_dataset_metadata.json\n",
    "            train_reynolds: List of Reynolds numbers for training\n",
    "            val_reynolds: List of Reynolds numbers for validation\n",
    "            test_reynolds: List of Reynolds numbers for testing\n",
    "        \"\"\"\n",
    "        self.cases_dir = Path(cases_dir)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        # Reynolds number splits\n",
    "        self.train_reynolds = set(train_reynolds)\n",
    "        self.val_reynolds = set(val_reynolds)\n",
    "        self.test_reynolds = set(test_reynolds)\n",
    "        \n",
    "        print(f\"Reynolds number splits:\")\n",
    "        print(f\"  Train: {sorted(train_reynolds)}\")\n",
    "        print(f\"  Val:   {sorted(val_reynolds)}\")\n",
    "        print(f\"  Test:  {sorted(test_reynolds)}\")\n",
    "        \n",
    "        # Verify no overlap\n",
    "        overlap = (self.train_reynolds & self.val_reynolds) | \\\n",
    "                  (self.train_reynolds & self.test_reynolds) | \\\n",
    "                  (self.val_reynolds & self.test_reynolds)\n",
    "        if overlap:\n",
    "            raise ValueError(f\"Reynolds numbers overlap between splits: {overlap}\")\n",
    "        \n",
    "        # Variable names\n",
    "        self.position_vars = ['x_pos', 'y_pos', 'z_pos']\n",
    "        self.physics_vars = ['pressure', 'velocity_x', 'velocity_y', 'velocity_z', \n",
    "                           'vorticity_x', 'vorticity_y', 'vorticity_z']\n",
    "        self.var_names = self.position_vars + self.physics_vars\n",
    "        \n",
    "        # Load metadata and extract normalization parameters\n",
    "        print(f\"\\nLoading metadata from: {metadata_file}\")\n",
    "        with open(metadata_file, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        self.normalization_params = self._compute_normalization_params(metadata)\n",
    "        self.global_param_normalization = self._compute_global_param_normalization(metadata)\n",
    "        \n",
    "        print(\"九 Initialized successfully\\n\")\n",
    "    \n",
    "    def _compute_normalization_params(self, metadata):\n",
    "        \"\"\"Compute normalization parameters from dataset metadata.\"\"\"\n",
    "        print(\"  Computing normalization parameters from dataset statistics...\")\n",
    "        \n",
    "        params = {}\n",
    "        global_stats = metadata['variable_statistics']['global_statistics']\n",
    "        \n",
    "        for var in self.var_names:\n",
    "            if var in global_stats:\n",
    "                var_min = global_stats[var]['min']\n",
    "                var_max = global_stats[var]['max']\n",
    "                params[var] = {'min': var_min, 'max': var_max}\n",
    "                print(f\"    {var:15s}: [{var_min:12.6e}, {var_max:12.6e}]\")\n",
    "            else:\n",
    "                print(f\"    WARNING: {var} not found in global statistics\")\n",
    "        \n",
    "        return params\n",
    "    \n",
    "    def _compute_global_param_normalization(self, metadata):\n",
    "        \"\"\"Compute global parameter normalization from metadata.\"\"\"\n",
    "        print(\"  Computing global parameter normalization...\")\n",
    "        \n",
    "        if 'reynolds_summary' in metadata:\n",
    "            reynolds_stats = metadata['reynolds_summary']\n",
    "            params = {\n",
    "                'reynolds_number': {\n",
    "                    'min': float(reynolds_stats['min']),\n",
    "                    'max': float(reynolds_stats['max'])\n",
    "                }\n",
    "            }\n",
    "            print(f\"    reynolds_number: [{params['reynolds_number']['min']}, {params['reynolds_number']['max']}]\")\n",
    "            return params\n",
    "        \n",
    "        print(\"    No Reynolds number statistics found\")\n",
    "        return {}\n",
    "    \n",
    "    def normalize_tensor(self, data, var_name, param_dict=None):\n",
    "        \"\"\"Normalize a tensor using min-max normalization to [0, 1].\"\"\"\n",
    "        if param_dict is None:\n",
    "            param_dict = self.normalization_params\n",
    "        \n",
    "        if var_name not in param_dict:\n",
    "            print(f\"WARNING: {var_name} not found in normalization parameters\")\n",
    "            return data.clone()\n",
    "        \n",
    "        params = param_dict[var_name]\n",
    "        var_min, var_max = params['min'], params['max']\n",
    "        \n",
    "        if var_max == var_min:\n",
    "            return torch.zeros_like(data)\n",
    "        \n",
    "        normalized = (data - var_min) / (var_max - var_min)\n",
    "        normalized = torch.clamp(normalized, 0.0, 1.0)\n",
    "        \n",
    "        return normalized\n",
    "    \n",
    "    def normalize_global_params(self, global_params):\n",
    "        \"\"\"Normalize global parameters (Reynolds number).\"\"\"\n",
    "        if global_params is None or len(self.global_param_normalization) == 0:\n",
    "            return global_params.clone() if global_params is not None else None\n",
    "        \n",
    "        normalized_params = global_params.clone()\n",
    "        \n",
    "        if 'reynolds_number' in self.global_param_normalization:\n",
    "            normalized_params[0] = self.normalize_tensor(\n",
    "                global_params[0], 'reynolds_number', self.global_param_normalization\n",
    "            )\n",
    "        \n",
    "        return normalized_params\n",
    "    \n",
    "    def normalize_sample(self, data):\n",
    "        \"\"\"Normalize a single PyG Data object.\"\"\"\n",
    "        # Normalize x (positions + physics)\n",
    "        x_normalized = data.x.clone()\n",
    "        for i, var_name in enumerate(self.var_names):\n",
    "            if i < data.x.shape[1]:\n",
    "                x_normalized[:, i] = self.normalize_tensor(data.x[:, i], var_name)\n",
    "        \n",
    "        # Normalize y (physics only)\n",
    "        y_normalized = data.y.clone()\n",
    "        for i, var_name in enumerate(self.physics_vars):\n",
    "            if i < data.y.shape[1]:\n",
    "                y_normalized[:, i] = self.normalize_tensor(data.y[:, i], var_name)\n",
    "        \n",
    "        # Normalize pos separately\n",
    "        pos_normalized = data.pos.clone()\n",
    "        for i, var_name in enumerate(self.position_vars):\n",
    "            if i < data.pos.shape[1]:\n",
    "                pos_normalized[:, i] = self.normalize_tensor(data.pos[:, i], var_name)\n",
    "        \n",
    "        # Create normalized data object\n",
    "        normalized_data = data.clone()\n",
    "        normalized_data.x = x_normalized\n",
    "        normalized_data.y = y_normalized\n",
    "        normalized_data.pos = pos_normalized\n",
    "        \n",
    "        # Normalize global parameters\n",
    "        if hasattr(data, 'global_params') and data.global_params is not None:\n",
    "            normalized_data.global_params = self.normalize_global_params(data.global_params)\n",
    "        \n",
    "        return normalized_data\n",
    "    \n",
    "    def extract_reynolds_from_filename(self, filename):\n",
    "        \"\"\"Extract Reynolds number from filename.\"\"\"\n",
    "        import re\n",
    "        \n",
    "        # Pattern 1: Re_{number}\n",
    "        match = re.search(r'Re_(\\d+)', str(filename))\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "        \n",
    "        # Pattern 2: reynolds_{number}\n",
    "        match = re.search(r'reynolds_(\\d+)', str(filename), re.IGNORECASE)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "        \n",
    "        # Pattern 3: re{number}\n",
    "        match = re.search(r're(\\d+)', str(filename), re.IGNORECASE)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def determine_split(self, reynolds_number):\n",
    "        \"\"\"Determine which split (train/val/test) a Reynolds number belongs to.\"\"\"\n",
    "        if reynolds_number in self.train_reynolds:\n",
    "            return 'train'\n",
    "        elif reynolds_number in self.val_reynolds:\n",
    "            return 'val'\n",
    "        elif reynolds_number in self.test_reynolds:\n",
    "            return 'test'\n",
    "        else:\n",
    "            return 'unknown'\n",
    "    \n",
    "    def process_single_file(self, case_file):\n",
    "        \"\"\"Process a single case file: determine split, normalize, and save.\"\"\"\n",
    "        print(f\"\\nProcessing: {case_file.name}\")\n",
    "        \n",
    "        # Extract Reynolds number and determine split\n",
    "        reynolds_number = self.extract_reynolds_from_filename(case_file.name)\n",
    "        if reynolds_number is None:\n",
    "            print(f\"  九 Could not extract Reynolds number from filename\")\n",
    "            return {'split': 'unknown', 'count': 0, 'reynolds': None}\n",
    "            \n",
    "        split_name = self.determine_split(reynolds_number)\n",
    "        print(f\"  Reynolds: {reynolds_number}  Split: {split_name.upper()}\")\n",
    "        \n",
    "        if split_name == 'unknown':\n",
    "            print(f\"  九 Reynolds number {reynolds_number} not in any split definition\")\n",
    "            return {'split': 'unknown', 'count': 0, 'reynolds': reynolds_number}\n",
    "        \n",
    "        # Create output directory\n",
    "        output_split_dir = self.output_dir / split_name\n",
    "        output_split_dir.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        # Load dataset\n",
    "        print(f\"  Loading dataset...\")\n",
    "        try:\n",
    "            dataset = torch.load(case_file, weights_only=False)\n",
    "        except TypeError:\n",
    "            dataset = torch.load(case_file)\n",
    "        \n",
    "        total_samples = len(dataset)\n",
    "        print(f\"  Loaded {total_samples} samples\")\n",
    "        \n",
    "        if total_samples == 0:\n",
    "            print(f\"  九 Empty dataset\")\n",
    "            del dataset\n",
    "            gc.collect()\n",
    "            return {'split': split_name, 'count': 0, 'reynolds': reynolds_number}\n",
    "        \n",
    "        # Normalize all samples\n",
    "        print(f\"  Normalizing...\")\n",
    "        normalized_dataset = []\n",
    "        for idx, sample in enumerate(dataset):\n",
    "            normalized_sample = self.normalize_sample(sample)\n",
    "            normalized_dataset.append(normalized_sample)\n",
    "            \n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print(f\"    Progress: {idx + 1}/{total_samples}\", end='\\r')\n",
    "        \n",
    "        print(f\"    Progress: {total_samples}/{total_samples}\")\n",
    "        \n",
    "        # Save normalized dataset\n",
    "        output_file = output_split_dir / f\"Re_{reynolds_number}_normalized.pt\"\n",
    "        print(f\"  Saving to: {output_file.name}\")\n",
    "        torch.save(normalized_dataset, output_file)\n",
    "        \n",
    "        # Cleanup\n",
    "        del dataset, normalized_dataset\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"  九 Complete: {total_samples} samples saved\")\n",
    "        \n",
    "        return {\n",
    "            'split': split_name,\n",
    "            'count': total_samples,\n",
    "            'reynolds': reynolds_number,\n",
    "            'output_file': str(output_file)\n",
    "        }\n",
    "    \n",
    "    def process_all_cases(self):\n",
    "        \"\"\"Process all individual case files.\"\"\"\n",
    "        # Find all case files\n",
    "        case_files = sorted(list(self.cases_dir.glob(\"*_raw_data.pt\")))\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"PROCESSING {len(case_files)} CASE FILES\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        all_results = {}\n",
    "        split_counts = {'train': 0, 'val': 0, 'test': 0, 'unknown': 0}\n",
    "        reynolds_by_split = {'train': [], 'val': [], 'test': [], 'unknown': []}\n",
    "        \n",
    "        for i, case_file in enumerate(case_files, 1):\n",
    "            print(f\"\\n[{i}/{len(case_files)}] {'='*60}\")\n",
    "            \n",
    "            try:\n",
    "                result = self.process_single_file(case_file)\n",
    "                \n",
    "                all_results[case_file.name] = result\n",
    "                split_name = result['split']\n",
    "                split_counts[split_name] += result['count']\n",
    "                \n",
    "                if result['reynolds'] is not None:\n",
    "                    reynolds_by_split[split_name].append(result['reynolds'])\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  九 ERROR: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                all_results[case_file.name] = {'error': str(e), 'split': 'error'}\n",
    "            \n",
    "            # Cleanup between files\n",
    "            gc.collect()\n",
    "        \n",
    "        # Save summary\n",
    "        summary = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'total_files_processed': len(case_files),\n",
    "            'split_method': 'reynolds_number',\n",
    "            'reynolds_splits': {\n",
    "                'train': sorted(list(self.train_reynolds)),\n",
    "                'val': sorted(list(self.val_reynolds)),\n",
    "                'test': sorted(list(self.test_reynolds))\n",
    "            },\n",
    "            'sample_counts': split_counts,\n",
    "            'reynolds_processed': {\n",
    "                'train': sorted(reynolds_by_split['train']),\n",
    "                'val': sorted(reynolds_by_split['val']),\n",
    "                'test': sorted(reynolds_by_split['test']),\n",
    "                'unknown': sorted(reynolds_by_split['unknown'])\n",
    "            },\n",
    "            'file_results': all_results\n",
    "        }\n",
    "        \n",
    "        summary_file = self.output_dir / 'split_summary.json'\n",
    "        with open(summary_file, 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"PROCESSING COMPLETE - SUMMARY\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Files processed: {len(case_files)}\")\n",
    "        print(f\"\\nSamples by split:\")\n",
    "        print(f\"  Train:   {split_counts['train']:6d} samples (Re: {sorted(reynolds_by_split['train'])})\")\n",
    "        print(f\"  Val:     {split_counts['val']:6d} samples (Re: {sorted(reynolds_by_split['val'])})\")\n",
    "        print(f\"  Test:    {split_counts['test']:6d} samples (Re: {sorted(reynolds_by_split['test'])})\")\n",
    "        if split_counts['unknown'] > 0:\n",
    "            print(f\"  Unknown: {split_counts['unknown']:6d} samples (Re: {sorted(reynolds_by_split['unknown'])})\")\n",
    "        print(f\"\\nSummary saved to: {summary_file}\")\n",
    "        print(f\"Output directory: {self.output_dir}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        return summary\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SPLIT & NORMALIZE INDIVIDUAL CASE FILES BY REYNOLDS NUMBER\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Configuration\n",
    "    CASES_DIR = \"/standard/sds_baek_energetic/von_karman_vortex/full_data/individual_cases\"\n",
    "    OUTPUT_DIR = \"/standard/sds_baek_energetic/von_karman_vortex/full_data/split_normalized\"\n",
    "    METADATA_FILE = \"/standard/sds_baek_energetic/von_karman_vortex/full_data/karman_dataset_metadata.json\"\n",
    "    \n",
    "    # Reynolds number splits - PHYSICS-INFORMED SPLITTING\n",
    "    TRAIN_REYNOLDS = [40, 50, 100, 150, 250, 400, 450, 500, 550, 700, 750, 850, 950]\n",
    "    VAL_REYNOLDS = [20, 200, 300, 650, 1000]\n",
    "    TEST_REYNOLDS = [1, 350, 600, 800, 900]\n",
    "    \n",
    "    print(f\"\\nConfiguration:\")\n",
    "    print(f\"  Cases directory: {CASES_DIR}\")\n",
    "    print(f\"  Output directory: {OUTPUT_DIR}\")\n",
    "    print(f\"  Metadata file: {METADATA_FILE}\")\n",
    "    \n",
    "    # Verify paths exist\n",
    "    if not Path(CASES_DIR).exists():\n",
    "        print(f\"\\n九 ERROR: Cases directory not found: {CASES_DIR}\")\n",
    "        return\n",
    "    \n",
    "    if not Path(METADATA_FILE).exists():\n",
    "        print(f\"\\n九 ERROR: Metadata file not found: {METADATA_FILE}\")\n",
    "        return\n",
    "    \n",
    "    # Initialize processor\n",
    "    try:\n",
    "        processor = SimpleReynoldsSplitter(\n",
    "            CASES_DIR, \n",
    "            OUTPUT_DIR, \n",
    "            METADATA_FILE,\n",
    "            TRAIN_REYNOLDS,\n",
    "            VAL_REYNOLDS,\n",
    "            TEST_REYNOLDS\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"\\n九 ERROR: Failed to initialize processor\")\n",
    "        print(f\"  {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return\n",
    "    \n",
    "    # Process all files\n",
    "    summary = processor.process_all_cases()\n",
    "    \n",
    "    print(\"\\n九 All processing complete!\")\n",
    "    print(\"\\nOutput structure:\")\n",
    "    print(f\"  {OUTPUT_DIR}/\")\n",
    "    print(f\"    train/\")\n",
    "    print(f\"      Re_40_normalized.pt\")\n",
    "    print(f\"      Re_50_normalized.pt\")\n",
    "    print(f\"      ...\")\n",
    "    print(f\"    val/\")\n",
    "    print(f\"      Re_20_normalized.pt\")\n",
    "    print(f\"      ...\")\n",
    "    print(f\"    test/\")\n",
    "    print(f\"      Re_1_normalized.pt\")\n",
    "    print(f\"      ...\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5967ac6-9a57-4699-8d42-1dc5c968f175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "File: Re_100_normalized.pt\n",
      "======================================================================\n",
      "\n",
      "Data type: <class 'list'>\n",
      "\n",
      "List with 400 elements\n",
      "\n",
      "First element type: <class 'torch_geometric.data.data.Data'>\n",
      "\n",
      "======================================================================\n",
      "ATTEMPTING TO DETERMINE NUMBER OF NODES:\n",
      "======================================================================\n",
      "九 Could not determine number of nodes automatically\n",
      "\n",
      "\n",
      "Quick check of all train files:\n",
      "----------------------------------------------------------------------\n",
      "  Re =  100: Re_100_normalized.pt\n",
      "  Re =  150: Re_150_normalized.pt\n",
      "  Re =  250: Re_250_normalized.pt\n",
      "  Re =  400: Re_400_normalized.pt\n",
      "  Re =   40: Re_40_normalized.pt\n",
      "  Re =  450: Re_450_normalized.pt\n",
      "  Re =  500: Re_500_normalized.pt\n",
      "  Re =   50: Re_50_normalized.pt\n",
      "  Re =  550: Re_550_normalized.pt\n",
      "  Re =  700: Re_700_normalized.pt\n",
      "  Re =  750: Re_750_normalized.pt\n",
      "  Re =  850: Re_850_normalized.pt\n",
      "  Re =  950: Re_950_normalized.pt\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Quick diagnostic script to inspect .pt file structure\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "BASE_DIR = Path(\"/standard/sds_baek_energetic/von_karman_vortex/full_data/split_normalized\")\n",
    "\n",
    "def inspect_file(file_path):\n",
    "    \"\"\"Inspect the structure of a .pt file\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"File: {file_path.name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    data = torch.load(file_path, weights_only=False)\n",
    "    \n",
    "    print(f\"\\nData type: {type(data)}\")\n",
    "    \n",
    "    if isinstance(data, dict):\n",
    "        print(f\"\\nDictionary with keys: {list(data.keys())}\")\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                print(f\"  {key}: shape = {value.shape}, dtype = {value.dtype}\")\n",
    "            else:\n",
    "                print(f\"  {key}: type = {type(value)}\")\n",
    "    \n",
    "    elif isinstance(data, list):\n",
    "        print(f\"\\nList with {len(data)} elements\")\n",
    "        if len(data) > 0:\n",
    "            print(f\"\\nFirst element type: {type(data[0])}\")\n",
    "            if isinstance(data[0], dict):\n",
    "                print(f\"Keys in first element: {list(data[0].keys())}\")\n",
    "                for key, value in data[0].items():\n",
    "                    if isinstance(value, torch.Tensor):\n",
    "                        print(f\"  {key}: shape = {value.shape}, dtype = {value.dtype}\")\n",
    "                    else:\n",
    "                        print(f\"  {key}: type = {type(value)}\")\n",
    "    \n",
    "    elif isinstance(data, torch.Tensor):\n",
    "        print(f\"\\nTensor shape: {data.shape}, dtype: {data.dtype}\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"\\nUnknown data structure\")\n",
    "    \n",
    "    # Try to find number of nodes\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"ATTEMPTING TO DETERMINE NUMBER OF NODES:\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    num_nodes = None\n",
    "    \n",
    "    if isinstance(data, dict):\n",
    "        # Common field names for positions\n",
    "        pos_fields = ['pos', 'x_pos', 'positions', 'coordinates', 'x', 'y', 'z']\n",
    "        for field in pos_fields:\n",
    "            if field in data:\n",
    "                if isinstance(data[field], torch.Tensor):\n",
    "                    num_nodes = data[field].shape[0]\n",
    "                    print(f\"九 Found {num_nodes:,} nodes from field '{field}'\")\n",
    "                    break\n",
    "                elif isinstance(data[field], (list, tuple)):\n",
    "                    num_nodes = len(data[field])\n",
    "                    print(f\"九 Found {num_nodes:,} nodes from field '{field}'\")\n",
    "                    break\n",
    "        \n",
    "        if num_nodes is None:\n",
    "            # Just take the first tensor dimension\n",
    "            for key, value in data.items():\n",
    "                if isinstance(value, torch.Tensor) and len(value.shape) > 0:\n",
    "                    num_nodes = value.shape[0]\n",
    "                    print(f\"九 Inferred {num_nodes:,} nodes from field '{key}' (shape: {value.shape})\")\n",
    "                    break\n",
    "    \n",
    "    elif isinstance(data, list) and len(data) > 0:\n",
    "        sample = data[0]\n",
    "        if isinstance(sample, dict):\n",
    "            pos_fields = ['pos', 'x_pos', 'positions', 'coordinates']\n",
    "            for field in pos_fields:\n",
    "                if field in sample:\n",
    "                    if isinstance(sample[field], torch.Tensor):\n",
    "                        num_nodes = sample[field].shape[0]\n",
    "                        print(f\"九 Found {num_nodes:,} nodes from field '{field}' in sample 0\")\n",
    "                        break\n",
    "    \n",
    "    if num_nodes is None:\n",
    "        print(\"九 Could not determine number of nodes automatically\")\n",
    "    \n",
    "    return num_nodes\n",
    "\n",
    "def main():\n",
    "    \"\"\"Quick inspection of one file from each split\"\"\"\n",
    "    \n",
    "    for split in ['train', 'val', 'test']:\n",
    "        split_dir = BASE_DIR / split\n",
    "        if split_dir.exists():\n",
    "            pt_files = sorted(split_dir.glob(\"*.pt\"))\n",
    "            if pt_files:\n",
    "                # Inspect first file\n",
    "                inspect_file(pt_files[0])\n",
    "                \n",
    "                # If structure is clear, check a few more\n",
    "                print(f\"\\n\\nQuick check of all {split} files:\")\n",
    "                print(\"-\" * 70)\n",
    "                for pt_file in pt_files:\n",
    "                    try:\n",
    "                        data = torch.load(pt_file, weights_only=False)\n",
    "                        re_num = int(pt_file.stem.split('_')[1])\n",
    "                        \n",
    "                        # Quick node count\n",
    "                        if isinstance(data, dict):\n",
    "                            for field in ['pos', 'x_pos', 'positions']:\n",
    "                                if field in data:\n",
    "                                    num_nodes = data[field].shape[0] if isinstance(data[field], torch.Tensor) else len(data[field])\n",
    "                                    break\n",
    "                        elif isinstance(data, list):\n",
    "                            num_nodes = \"N/A (list structure)\"\n",
    "                        \n",
    "                        print(f\"  Re = {re_num:4d}: {pt_file.name}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"  九 Error: {pt_file.name} - {e}\")\n",
    "                \n",
    "                break  # Only inspect one split in detail\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b088a086-443f-4696-9dfc-82270cc5da4f",
   "metadata": {},
   "source": [
    "# Old Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7e28137-6a1a-4d14-bf8d-3fd3ad98e1db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Karman Vortex Street Dataset Visualizer\n",
      "Analyzing dataset at: /standard/sds_baek_energetic/von_karman_vortex/full_data/split_normalized\n",
      "Creating complete Karman vortex dataset analysis...\n",
      "\n",
      "1. Dataset Overview\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Re_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 528\u001b[39m\n\u001b[32m    525\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mKarman Vortex Street Dataset Visualizer\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    526\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAnalyzing dataset at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m528\u001b[39m visualizer = \u001b[43manalyze_karman_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 518\u001b[39m, in \u001b[36manalyze_karman_dataset\u001b[39m\u001b[34m(dataset_dir)\u001b[39m\n\u001b[32m    509\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    510\u001b[39m \u001b[33;03mAnalyze a Karman vortex dataset.\u001b[39;00m\n\u001b[32m    511\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    515\u001b[39m \u001b[33;03m    Path to the dataset directory containing metadata and normalized data\u001b[39;00m\n\u001b[32m    516\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    517\u001b[39m visualizer = KarmanDatasetVisualizer(dataset_dir)\n\u001b[32m--> \u001b[39m\u001b[32m518\u001b[39m \u001b[43mvisualizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_complete_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m visualizer\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 494\u001b[39m, in \u001b[36mKarmanDatasetVisualizer.create_complete_analysis\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    491\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCreating complete Karman vortex dataset analysis...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    493\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m1. Dataset Overview\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvisualize_dataset_overview\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m2. Normalized Feature Analysis\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    497\u001b[39m \u001b[38;5;28mself\u001b[39m.analyze_normalized_features()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mKarmanDatasetVisualizer.visualize_dataset_overview\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     55\u001b[39m ax = axes[\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m]\n\u001b[32m     56\u001b[39m reynolds_nums = \u001b[38;5;28mself\u001b[39m.metadata[\u001b[33m'\u001b[39m\u001b[33mreynolds_summary\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33munique_values\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m case_counts = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcase_sample_counts\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mRe_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mre\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m re \u001b[38;5;129;01min\u001b[39;00m reynolds_nums]\n\u001b[32m     59\u001b[39m bars = ax.bar(reynolds_nums, case_counts, alpha=\u001b[32m0.7\u001b[39m, color=\u001b[33m'\u001b[39m\u001b[33msteelblue\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     60\u001b[39m ax.set_xlabel(\u001b[33m'\u001b[39m\u001b[33mReynolds Number\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: 'Re_1'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x1200 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "\n",
    "class KarmanDatasetVisualizer:\n",
    "    \"\"\"\n",
    "    Visualizer for analyzing the normalized Karman vortex street datasets.\n",
    "    Updated for shock tube format: positions + physics in x tensor.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_dir):\n",
    "        self.dataset_dir = Path(dataset_dir)\n",
    "        self.normalized_dir = self.dataset_dir / \"normalized\"\n",
    "        \n",
    "        # Load metadata\n",
    "        self.metadata = self._load_metadata()\n",
    "        self.norm_metadata = self._load_normalization_metadata()\n",
    "        \n",
    "        # Variable names (shock tube format)\n",
    "        self.position_vars = ['x_pos', 'y_pos', 'z_pos']\n",
    "        self.physics_vars = ['pressure', 'velocity_x', 'velocity_y', 'velocity_z', \n",
    "                           'vorticity_x', 'vorticity_y', 'vorticity_z']\n",
    "        self.var_names = self.position_vars + self.physics_vars\n",
    "        \n",
    "        # Set up plotting style\n",
    "        plt.style.use('default')\n",
    "        sns.set_palette(\"husl\")\n",
    "        \n",
    "    def _load_metadata(self):\n",
    "        \"\"\"Load the main dataset metadata.\"\"\"\n",
    "        metadata_file = self.dataset_dir / \"karman_dataset_metadata.json\"\n",
    "        if metadata_file.exists():\n",
    "            with open(metadata_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return None\n",
    "    \n",
    "    def _load_normalization_metadata(self):\n",
    "        \"\"\"Load the normalization metadata.\"\"\"\n",
    "        norm_metadata_file = self.normalized_dir / \"normalization_metadata.json\"\n",
    "        if norm_metadata_file.exists():\n",
    "            with open(norm_metadata_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return None\n",
    "    \n",
    "    def visualize_dataset_overview(self):\n",
    "        \"\"\"Create an overview visualization of the dataset splits and parameters.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        fig.suptitle('Karman Vortex Dataset Overview (Shock Tube Format)', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # 1. Reynolds number distribution\n",
    "        ax = axes[0, 0]\n",
    "        reynolds_nums = self.metadata['reynolds_summary']['unique_values']\n",
    "        case_counts = [self.metadata['case_sample_counts'][f'Re_{re}'] for re in reynolds_nums]\n",
    "        \n",
    "        bars = ax.bar(reynolds_nums, case_counts, alpha=0.7, color='steelblue')\n",
    "        ax.set_xlabel('Reynolds Number')\n",
    "        ax.set_ylabel('Number of Timestep Pairs')\n",
    "        ax.set_title('Dataset Size by Reynolds Number')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        for bar, count in zip(bars, case_counts):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "                   f'{count}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # 2. Train/Test/Val split visualization\n",
    "        ax = axes[0, 1]\n",
    "        split_info = self.metadata['train_test_val_split']\n",
    "        split_sizes = split_info['split_sizes']\n",
    "        \n",
    "        labels = ['Train', 'Val', 'Test']\n",
    "        sizes = [split_sizes['train'], split_sizes['val'], split_sizes['test']]\n",
    "        colors = ['#2ca02c', '#d62728', '#ff7f0e']\n",
    "        \n",
    "        non_zero_labels = []\n",
    "        non_zero_sizes = []\n",
    "        non_zero_colors = []\n",
    "        for label, size, color in zip(labels, sizes, colors):\n",
    "            if size > 0:\n",
    "                non_zero_labels.append(f'{label}\\n({size} samples)')\n",
    "                non_zero_sizes.append(size)\n",
    "                non_zero_colors.append(color)\n",
    "        \n",
    "        wedges, texts, autotexts = ax.pie(non_zero_sizes, labels=non_zero_labels, \n",
    "                                         colors=non_zero_colors, autopct='%1.1f%%',\n",
    "                                         startangle=90)\n",
    "        ax.set_title('Train/Test/Val Split Distribution')\n",
    "        \n",
    "        # 3. Reynolds assignment by split\n",
    "        ax = axes[0, 2]\n",
    "        reynolds_assignment = split_info['reynolds_assignment']\n",
    "        \n",
    "        split_reynolds = {\n",
    "            'Train': reynolds_assignment['train_reynolds'],\n",
    "            'Val': reynolds_assignment['val_reynolds'], \n",
    "            'Test': reynolds_assignment['test_reynolds']\n",
    "        }\n",
    "        \n",
    "        y_pos = np.arange(len(split_reynolds))\n",
    "        reynolds_text = []\n",
    "        colors_list = []\n",
    "        \n",
    "        for split, reynolds_list in split_reynolds.items():\n",
    "            if isinstance(reynolds_list, list) and len(reynolds_list) > 0:\n",
    "                reynolds_text.append(f\"Re = {reynolds_list}\")\n",
    "                if split == 'Train':\n",
    "                    colors_list.append('#2ca02c')\n",
    "                elif split == 'Val':\n",
    "                    colors_list.append('#d62728')\n",
    "                else:\n",
    "                    colors_list.append('#ff7f0e')\n",
    "            else:\n",
    "                reynolds_text.append(\"None\")\n",
    "                colors_list.append('#cccccc')\n",
    "        \n",
    "        bars = ax.barh(y_pos, [1, 1, 1], color=colors_list, alpha=0.7)\n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels(list(split_reynolds.keys()))\n",
    "        ax.set_xlabel('Split Assignment')\n",
    "        ax.set_title('Reynolds Number Assignment by Split')\n",
    "        \n",
    "        for i, (bar, text) in enumerate(zip(bars, reynolds_text)):\n",
    "            ax.text(0.5, bar.get_y() + bar.get_height()/2, text,\n",
    "                   ha='center', va='center', fontweight='bold', color='white', fontsize=9)\n",
    "        \n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_xticks([])\n",
    "        \n",
    "        # 4. Feature dimensions breakdown\n",
    "        ax = axes[1, 0]\n",
    "        \n",
    "        # Get feature info\n",
    "        sample_case = list(self.metadata['case_info'].values())[0]\n",
    "        total_features = sample_case['feature_dimensions']['node_features_total']\n",
    "        pos_features = sample_case['feature_dimensions']['position_features']\n",
    "        physics_features = sample_case['feature_dimensions']['physics_features']\n",
    "        global_features = sample_case['feature_dimensions']['global_features']\n",
    "        \n",
    "        dimensions = {\n",
    "            'Position\\nFeatures': pos_features,\n",
    "            'Physics\\nFeatures': physics_features, \n",
    "            'Global\\nFeatures': global_features\n",
    "        }\n",
    "        \n",
    "        dim_names = list(dimensions.keys())\n",
    "        dim_values = list(dimensions.values())\n",
    "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "        \n",
    "        bars = ax.bar(dim_names, dim_values, color=colors, alpha=0.7)\n",
    "        ax.set_ylabel('Number of Features')\n",
    "        ax.set_title(f'Feature Dimensions (Total in x: {total_features})')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        for bar, value in zip(bars, dim_values):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                   f'{value}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # 5. Variable ranges\n",
    "        ax = axes[1, 1]\n",
    "        \n",
    "        key_vars = ['pressure', 'velocity_x', 'vorticity_x']\n",
    "        var_ranges = []\n",
    "        var_names = []\n",
    "        \n",
    "        for var in key_vars:\n",
    "            if var in self.norm_metadata['normalization_params']:\n",
    "                orig_range = self.norm_metadata['normalization_params'][var]\n",
    "                var_ranges.append(orig_range['max'] - orig_range['min'])\n",
    "                var_names.append(var.replace('_', ' ').title())\n",
    "        \n",
    "        bars = ax.bar(var_names, var_ranges, color=['#d62728', '#2ca02c', '#ff7f0e'], alpha=0.7)\n",
    "        ax.set_ylabel('Original Range Span')\n",
    "        ax.set_title('Original Variable Ranges')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        for bar, value in zip(bars, var_ranges):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height * 1.05,\n",
    "                   f'{value:.2e}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "        \n",
    "        # 6. Data structure info\n",
    "        ax = axes[1, 2]\n",
    "        ax.axis('off')\n",
    "        \n",
    "        total_samples = self.metadata['total_samples']\n",
    "        mesh_info = {\n",
    "            'Total Samples': f\"{total_samples:,}\",\n",
    "            'Reynolds Cases': f\"{self.metadata['total_cases']}\",\n",
    "            'Features in x': f\"{total_features} (3 pos + 7 phys)\",\n",
    "            'Features in y': f\"7 (physics only)\",\n",
    "            'Global params': f\"1 (Reynolds)\",\n",
    "            'Format': 'Shock tube compatible'\n",
    "        }\n",
    "        \n",
    "        y_positions = np.linspace(0.9, 0.1, len(mesh_info))\n",
    "        \n",
    "        for i, (key, value) in enumerate(mesh_info.items()):\n",
    "            ax.text(0.1, y_positions[i], f'{key}:', fontsize=11, fontweight='bold')\n",
    "            ax.text(0.55, y_positions[i], value, fontsize=11)\n",
    "        \n",
    "        ax.set_title('Dataset Information')\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def analyze_normalized_features(self):\n",
    "        \"\"\"Analyze the distribution of normalized features (physics only).\"\"\"\n",
    "        samples = {}\n",
    "        \n",
    "        # Load train data\n",
    "        train_file = self.normalized_dir / \"karman_vortex_train_normalized.pt\"\n",
    "        if train_file.exists():\n",
    "            train_data = torch.load(train_file, weights_only=False)\n",
    "            samples['train'] = train_data[0] if len(train_data) > 0 else None\n",
    "        \n",
    "        # Load test data  \n",
    "        test_file = self.normalized_dir / \"karman_vortex_test_normalized.pt\"\n",
    "        if test_file.exists():\n",
    "            test_data = torch.load(test_file, weights_only=False)\n",
    "            samples['test'] = test_data[0] if len(test_data) > 0 else None\n",
    "        \n",
    "        if not samples:\n",
    "            print(\"No normalized data files found!\")\n",
    "            return\n",
    "        \n",
    "        sample_data = list(samples.values())[0]\n",
    "        \n",
    "        # Get physics feature names (skip first 3 position columns)\n",
    "        if hasattr(sample_data, 'feature_names'):\n",
    "            feature_names = sample_data.feature_names\n",
    "        else:\n",
    "            feature_names = self.physics_vars\n",
    "        \n",
    "        n_features = len(feature_names)\n",
    "        \n",
    "        # Create subplot grid\n",
    "        n_cols = 3\n",
    "        n_rows = (n_features + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
    "        fig.suptitle('Normalized Physics Feature Distributions', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        if n_rows == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        axes_flat = axes.flatten()\n",
    "        \n",
    "        # Plot each physics feature (skip first 3 position columns in x)\n",
    "        for i, feature_name in enumerate(feature_names):\n",
    "            ax = axes_flat[i]\n",
    "            \n",
    "            for split_name, data in samples.items():\n",
    "                if data is not None:\n",
    "                    # Get physics features from x (columns 3-9)\n",
    "                    feature_values = data.x[:, 3 + i].numpy()\n",
    "                    \n",
    "                    if len(feature_values) > 5000:\n",
    "                        feature_values = np.random.choice(feature_values, 5000, replace=False)\n",
    "                    \n",
    "                    ax.hist(feature_values, bins=50, alpha=0.6, \n",
    "                           label=f'{split_name.title()} (Re={data.reynolds_number})', \n",
    "                           density=True)\n",
    "            \n",
    "            ax.set_xlabel('Normalized Value')\n",
    "            ax.set_ylabel('Density')\n",
    "            ax.set_title(f'{feature_name.replace(\"_\", \" \").title()}')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.set_xlim(0, 1)\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for i in range(n_features, len(axes_flat)):\n",
    "            axes_flat[i].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def visualize_parameter_normalization(self):\n",
    "        \"\"\"Visualize the parameter normalization (Reynolds numbers).\"\"\"\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        fig.suptitle('Reynolds Number Normalization', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        reynolds_nums = self.metadata['reynolds_summary']['unique_values']\n",
    "        case_counts = [self.metadata['case_sample_counts'][f'Re_{re}'] for re in reynolds_nums]\n",
    "        \n",
    "        # Original Reynolds numbers\n",
    "        ax = axes[0]\n",
    "        bars = ax.bar(reynolds_nums, case_counts, alpha=0.7, color='steelblue')\n",
    "        ax.set_xlabel('Original Reynolds Number')\n",
    "        ax.set_ylabel('Number of Samples')\n",
    "        ax.set_title('Original Reynolds Distribution')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        for bar, count in zip(bars, case_counts):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "                   f'{count}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Normalized Reynolds numbers\n",
    "        ax = axes[1]\n",
    "        if 'reynolds_number' in self.norm_metadata['global_param_normalization']:\n",
    "            re_norm_params = self.norm_metadata['global_param_normalization']['reynolds_number']\n",
    "            re_min = re_norm_params['min']\n",
    "            re_max = re_norm_params['max']\n",
    "            \n",
    "            normalized_reynolds = [(re - re_min) / (re_max - re_min) for re in reynolds_nums]\n",
    "            \n",
    "            bars = ax.bar(normalized_reynolds, case_counts, alpha=0.7, \n",
    "                         color='steelblue', width=0.05)\n",
    "            ax.set_xlabel('Normalized Reynolds Number')\n",
    "            ax.set_ylabel('Number of Samples')\n",
    "            ax.set_title('Normalized Reynolds Distribution')\n",
    "            ax.set_xlim(-0.1, 1.1)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            for bar, orig_re, count in zip(bars, reynolds_nums, case_counts):\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "                       f'Re={orig_re}\\n({count})', ha='center', va='bottom', \n",
    "                       fontweight='bold', fontsize=9)\n",
    "        \n",
    "        # Normalization mapping\n",
    "        ax = axes[2]\n",
    "        if 'reynolds_number' in self.norm_metadata['global_param_normalization']:\n",
    "            re_norm_params = self.norm_metadata['global_param_normalization']['reynolds_number']\n",
    "            re_min = re_norm_params['min']\n",
    "            re_max = re_norm_params['max']\n",
    "            \n",
    "            original_range = np.linspace(re_min, re_max, 100)\n",
    "            normalized_range = (original_range - re_min) / (re_max - re_min)\n",
    "            \n",
    "            ax.plot(original_range, normalized_range, 'b-', linewidth=2, \n",
    "                   label='Normalization Function')\n",
    "            \n",
    "            for re_num in reynolds_nums:\n",
    "                norm_val = (re_num - re_min) / (re_max - re_min)\n",
    "                ax.plot(re_num, norm_val, 'ro', markersize=10)\n",
    "                ax.annotate(f'Re={re_num}', (re_num, norm_val), \n",
    "                           xytext=(10, 10), textcoords='offset points', fontweight='bold')\n",
    "            \n",
    "            ax.set_xlabel('Original Reynolds Number')\n",
    "            ax.set_ylabel('Normalized Value [0, 1]')\n",
    "            ax.set_title('Reynolds Number Normalization Mapping')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def analyze_individual_cases(self):\n",
    "        \"\"\"Analyze the individual normalized case files.\"\"\"\n",
    "        individual_cases_dir = self.normalized_dir / \"individual_cases_normalized\"\n",
    "        \n",
    "        if not individual_cases_dir.exists():\n",
    "            print(\"Individual normalized cases directory not found!\")\n",
    "            return\n",
    "        \n",
    "        case_files = list(individual_cases_dir.glob(\"*_normalized.pt\"))\n",
    "        \n",
    "        if not case_files:\n",
    "            print(\"No individual normalized case files found!\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        fig.suptitle('Individual Normalized Case Analysis', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        case_info = {}\n",
    "        \n",
    "        for case_file in case_files:\n",
    "            re_match = re.search(r'Reynolds_(\\d+)', case_file.name)\n",
    "            reynolds_num = int(re_match.group(1)) if re_match else 0\n",
    "            \n",
    "            case_data = torch.load(case_file, weights_only=False)\n",
    "            \n",
    "            case_info[reynolds_num] = {\n",
    "                'n_samples': len(case_data),\n",
    "                'filename': case_file.name,\n",
    "                'sample_data': case_data[0] if len(case_data) > 0 else None\n",
    "            }\n",
    "        \n",
    "        # 1. Sample counts per case\n",
    "        ax = axes[0, 0]\n",
    "        reynolds_list = sorted(case_info.keys())\n",
    "        sample_counts = [case_info[re]['n_samples'] for re in reynolds_list]\n",
    "        \n",
    "        bars = ax.bar(reynolds_list, sample_counts, alpha=0.7, color='steelblue')\n",
    "        ax.set_xlabel('Reynolds Number')\n",
    "        ax.set_ylabel('Number of Timestep Pairs')\n",
    "        ax.set_title('Samples per Individual Case File')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        for bar, count in zip(bars, sample_counts):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 2,\n",
    "                   f'{count}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # 2. Global parameter values (normalized Reynolds)\n",
    "        ax = axes[0, 1]\n",
    "        global_params = []\n",
    "        \n",
    "        for reynolds_num in reynolds_list:\n",
    "            sample = case_info[reynolds_num]['sample_data']\n",
    "            if sample and hasattr(sample, 'global_params') and sample.global_params is not None:\n",
    "                global_params.append(sample.global_params[0].item())\n",
    "            else:\n",
    "                global_params.append(0)\n",
    "        \n",
    "        bars = ax.bar(reynolds_list, global_params, alpha=0.7, color='steelblue')\n",
    "        ax.set_xlabel('Original Reynolds Number')\n",
    "        ax.set_ylabel('Normalized Global Parameter Value')\n",
    "        ax.set_title('Global Parameter Values (Normalized Reynolds)')\n",
    "        ax.set_ylim(0, 1.1)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        for bar, param_val in zip(bars, global_params):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                   f'{param_val:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # 3. Feature range verification\n",
    "        ax = axes[1, 0]\n",
    "        \n",
    "        feature_ranges = {}\n",
    "        \n",
    "        for reynolds_num in reynolds_list:\n",
    "            sample = case_info[reynolds_num]['sample_data']\n",
    "            if sample:\n",
    "                x_min = sample.x.min().item()\n",
    "                x_max = sample.x.max().item()\n",
    "                feature_ranges[reynolds_num] = {'min': x_min, 'max': x_max}\n",
    "        \n",
    "        reynolds_nums = list(feature_ranges.keys())\n",
    "        mins = [feature_ranges[re]['min'] for re in reynolds_nums]\n",
    "        maxs = [feature_ranges[re]['max'] for re in reynolds_nums]\n",
    "        \n",
    "        x_pos = np.arange(len(reynolds_nums))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars1 = ax.bar(x_pos - width/2, mins, width, label='Minimum Values', \n",
    "                      alpha=0.7, color='red')\n",
    "        bars2 = ax.bar(x_pos + width/2, maxs, width, label='Maximum Values', \n",
    "                      alpha=0.7, color='blue')\n",
    "        \n",
    "        ax.set_xlabel('Reynolds Number')\n",
    "        ax.set_ylabel('Feature Value Range')\n",
    "        ax.set_title('Normalized Feature Range Verification')\n",
    "        ax.set_xticks(x_pos)\n",
    "        ax.set_xticklabels(reynolds_nums)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.axhline(y=0, color='green', linestyle='--', alpha=0.7)\n",
    "        ax.axhline(y=1, color='green', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # 4. Data structure summary\n",
    "        ax = axes[1, 1]\n",
    "        ax.axis('off')\n",
    "        \n",
    "        sample = case_info[reynolds_list[0]]['sample_data']\n",
    "        if sample:\n",
    "            summary_info = {\n",
    "                'Nodes per Graph': f\"{sample.pos.shape[0]:,}\",\n",
    "                'x features (total)': f\"{sample.x.shape[1]} (3 pos + 7 phys)\",\n",
    "                'y features (phys)': f\"{sample.y.shape[1]}\",\n",
    "                'Edges': f\"{sample.edge_index.shape[1]:,}\",\n",
    "                'Global params': f\"{sample.global_params.shape[0] if sample.global_params is not None else 0}\",\n",
    "                'Format': 'Shock tube compatible'\n",
    "            }\n",
    "            \n",
    "            y_positions = np.linspace(0.9, 0.1, len(summary_info))\n",
    "            \n",
    "            for i, (key, value) in enumerate(summary_info.items()):\n",
    "                ax.text(0.1, y_positions[i], f'{key}:', fontsize=11, fontweight='bold')\n",
    "                ax.text(0.55, y_positions[i], value, fontsize=11)\n",
    "        \n",
    "        ax.set_title('Normalized Data Structure')\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def create_complete_analysis(self):\n",
    "        \"\"\"Create a complete analysis of the dataset.\"\"\"\n",
    "        print(\"Creating complete Karman vortex dataset analysis...\")\n",
    "        \n",
    "        print(\"\\n1. Dataset Overview\")\n",
    "        self.visualize_dataset_overview()\n",
    "        \n",
    "        print(\"\\n2. Normalized Feature Analysis\")\n",
    "        self.analyze_normalized_features()\n",
    "        \n",
    "        print(\"\\n3. Parameter Normalization Analysis\")\n",
    "        self.visualize_parameter_normalization()\n",
    "        \n",
    "        print(\"\\n4. Individual Case Analysis\")\n",
    "        self.analyze_individual_cases()\n",
    "        \n",
    "        print(\"\\nAnalysis complete!\")\n",
    "\n",
    "\n",
    "def analyze_karman_dataset(dataset_dir):\n",
    "    \"\"\"\n",
    "    Analyze a Karman vortex dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset_dir : str\n",
    "        Path to the dataset directory containing metadata and normalized data\n",
    "    \"\"\"\n",
    "    visualizer = KarmanDatasetVisualizer(dataset_dir)\n",
    "    visualizer.create_complete_analysis()\n",
    "    return visualizer\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset_path = \"/standard/sds_baek_energetic/von_karman_vortex/full_data/split_normalized\"\n",
    "    \n",
    "    print(\"Karman Vortex Street Dataset Visualizer\")\n",
    "    print(f\"Analyzing dataset at: {dataset_path}\")\n",
    "    \n",
    "    visualizer = analyze_karman_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1f20cc-98b7-4c97-b741-32fd9182d84f",
   "metadata": {},
   "source": [
    "# New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87ed6f2-4f69-4657-a731-2b463862bd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Karman Vortex Street Dataset Visualizer\n",
      "Analyzing dataset at: /standard/sds_baek_energetic/von_karman_vortex/full_data/split_normalized\n",
      "------------------------------------------------------------\n",
      "Found metadata: /standard/sds_baek_energetic/von_karman_vortex/full_data/split_normalized/karman_dataset_metadata.json\n",
      "Warning: Normalization metadata file not found. Some visualizations will be limited.\n",
      "Creating complete Karman vortex dataset analysis...\n",
      "Files expected: Re_XXX_normalized.pt format\n",
      "\n",
      "1. Dataset Overview\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "\n",
    "class KarmanDatasetVisualizer:\n",
    "    \"\"\"\n",
    "    Visualizer for analyzing the normalized Karman vortex street datasets.\n",
    "    Updated for train/test/val folder structure with shock tube format.\n",
    "    File naming convention: Re_XXX_normalized.pt\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_dir):\n",
    "        self.dataset_dir = Path(dataset_dir)\n",
    "        \n",
    "        # Updated folder structure with separate train/test/val folders\n",
    "        self.train_dir = self.dataset_dir / \"train\"\n",
    "        self.test_dir = self.dataset_dir / \"test\"\n",
    "        self.val_dir = self.dataset_dir / \"val\"\n",
    "        \n",
    "        # Load metadata\n",
    "        self.metadata = self._load_metadata()\n",
    "        self.norm_metadata = self._load_normalization_metadata()\n",
    "        \n",
    "        # Variable names (shock tube format)\n",
    "        self.position_vars = ['x_pos', 'y_pos', 'z_pos']\n",
    "        self.physics_vars = ['pressure', 'velocity_x', 'velocity_y', 'velocity_z', \n",
    "                           'vorticity_x', 'vorticity_y', 'vorticity_z']\n",
    "        self.var_names = self.position_vars + self.physics_vars\n",
    "        \n",
    "        # Set up plotting style\n",
    "        plt.style.use('default')\n",
    "        sns.set_palette(\"husl\")\n",
    "        \n",
    "    def _load_metadata(self):\n",
    "        \"\"\"Load the main dataset metadata from various possible locations.\"\"\"\n",
    "        # Try multiple possible locations\n",
    "        possible_locations = [\n",
    "            self.dataset_dir / \"karman_dataset_metadata.json\",\n",
    "            self.train_dir / \"karman_dataset_metadata.json\",\n",
    "            self.dataset_dir / \"metadata\" / \"karman_dataset_metadata.json\",\n",
    "            self.dataset_dir.parent / \"karman_dataset_metadata.json\"\n",
    "        ]\n",
    "        \n",
    "        for metadata_file in possible_locations:\n",
    "            if metadata_file.exists():\n",
    "                print(f\"Found metadata: {metadata_file}\")\n",
    "                with open(metadata_file, 'r') as f:\n",
    "                    return json.load(f)\n",
    "        \n",
    "        print(\"Warning: Main metadata file not found. Analyzing from actual files.\")\n",
    "        return None\n",
    "    \n",
    "    def _load_normalization_metadata(self):\n",
    "        \"\"\"Load the normalization metadata from various possible locations.\"\"\"\n",
    "        # Try multiple possible locations\n",
    "        possible_locations = [\n",
    "            self.train_dir / \"normalization_metadata.json\",\n",
    "            self.dataset_dir / \"normalization_metadata.json\",\n",
    "            self.dataset_dir / \"metadata\" / \"normalization_metadata.json\",\n",
    "            self.dataset_dir.parent / \"normalization_metadata.json\"\n",
    "        ]\n",
    "        \n",
    "        for norm_metadata_file in possible_locations:\n",
    "            if norm_metadata_file.exists():\n",
    "                print(f\"Found normalization metadata: {norm_metadata_file}\")\n",
    "                with open(norm_metadata_file, 'r') as f:\n",
    "                    return json.load(f)\n",
    "        \n",
    "        print(\"Warning: Normalization metadata file not found. Some visualizations will be limited.\")\n",
    "        return None\n",
    "    \n",
    "    def _extract_reynolds_from_filename(self, filename):\n",
    "        \"\"\"Extract Reynolds number from filename like Re_150_normalized.pt\"\"\"\n",
    "        re_match = re.search(r'Re_(\\d+)', filename)\n",
    "        return int(re_match.group(1)) if re_match else None\n",
    "    \n",
    "    def _load_split_data(self, split_name):\n",
    "        \"\"\"Load data from a specific split folder.\"\"\"\n",
    "        split_dir = getattr(self, f\"{split_name}_dir\")\n",
    "        \n",
    "        if not split_dir.exists():\n",
    "            return None\n",
    "        \n",
    "        # Look for .pt files in the split directory\n",
    "        pt_files = list(split_dir.glob(\"Re_*_normalized.pt\"))\n",
    "        \n",
    "        if not pt_files:\n",
    "            print(f\"No Re_*_normalized.pt files found in {split_dir}\")\n",
    "            return None\n",
    "        \n",
    "        # Load the first file as a sample\n",
    "        print(f\"Loading sample from: {pt_files[0].name}\")\n",
    "        data = torch.load(pt_files[0], weights_only=False)\n",
    "        return data[0] if len(data) > 0 else None\n",
    "    \n",
    "    def _count_samples_in_split(self, split_name):\n",
    "        \"\"\"Count total samples across all files in a split.\"\"\"\n",
    "        split_dir = getattr(self, f\"{split_name}_dir\")\n",
    "        \n",
    "        if not split_dir.exists():\n",
    "            return 0\n",
    "        \n",
    "        pt_files = list(split_dir.glob(\"Re_*_normalized.pt\"))\n",
    "        total_samples = 0\n",
    "        \n",
    "        for pt_file in pt_files:\n",
    "            data = torch.load(pt_file, weights_only=False)\n",
    "            total_samples += len(data)\n",
    "        \n",
    "        return total_samples\n",
    "    \n",
    "    def visualize_dataset_overview(self):\n",
    "        \"\"\"Create an overview visualization of the dataset splits and parameters.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        fig.suptitle('Karman Vortex Dataset Overview (Train/Test/Val Structure)', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # 1. Reynolds number distribution (count from actual files)\n",
    "        ax = axes[0, 0]\n",
    "        \n",
    "        # Always count samples from actual files for accuracy\n",
    "        train_files = list(self.train_dir.glob(\"Re_*_normalized.pt\")) if self.train_dir.exists() else []\n",
    "        test_files = list(self.test_dir.glob(\"Re_*_normalized.pt\")) if self.test_dir.exists() else []\n",
    "        val_files = list(self.val_dir.glob(\"Re_*_normalized.pt\")) if self.val_dir.exists() else []\n",
    "        \n",
    "        reynolds_counts = {}\n",
    "        for file_list in [train_files, test_files, val_files]:\n",
    "            for file in file_list:\n",
    "                re_num = self._extract_reynolds_from_filename(file.name)\n",
    "                if re_num:\n",
    "                    data = torch.load(file, weights_only=False)\n",
    "                    reynolds_counts[re_num] = reynolds_counts.get(re_num, 0) + len(data)\n",
    "        \n",
    "        if reynolds_counts:\n",
    "            reynolds_nums = sorted(reynolds_counts.keys())\n",
    "            case_counts = [reynolds_counts[re] for re in reynolds_nums]\n",
    "            \n",
    "            bars = ax.bar(reynolds_nums, case_counts, alpha=0.7, color='steelblue')\n",
    "            ax.set_xlabel('Reynolds Number')\n",
    "            ax.set_ylabel('Number of Timestep Pairs')\n",
    "            ax.set_title('Dataset Size by Reynolds Number')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            for bar, count in zip(bars, case_counts):\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "                       f'{count}', ha='center', va='bottom', fontweight='bold')\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'No data files found', ha='center', va='center', transform=ax.transAxes)\n",
    "            ax.set_title('Dataset Size by Reynolds Number')\n",
    "        \n",
    "        # 2. Train/Test/Val split visualization\n",
    "        ax = axes[0, 1]\n",
    "        \n",
    "        # Count samples in each split\n",
    "        train_count = self._count_samples_in_split('train')\n",
    "        test_count = self._count_samples_in_split('test')\n",
    "        val_count = self._count_samples_in_split('val')\n",
    "        \n",
    "        labels = []\n",
    "        sizes = []\n",
    "        colors = []\n",
    "        \n",
    "        if train_count > 0:\n",
    "            labels.append(f'Train\\n({train_count} samples)')\n",
    "            sizes.append(train_count)\n",
    "            colors.append('#2ca02c')\n",
    "        \n",
    "        if val_count > 0:\n",
    "            labels.append(f'Val\\n({val_count} samples)')\n",
    "            sizes.append(val_count)\n",
    "            colors.append('#d62728')\n",
    "        \n",
    "        if test_count > 0:\n",
    "            labels.append(f'Test\\n({test_count} samples)')\n",
    "            sizes.append(test_count)\n",
    "            colors.append('#ff7f0e')\n",
    "        \n",
    "        if sizes:\n",
    "            wedges, texts, autotexts = ax.pie(sizes, labels=labels, \n",
    "                                             colors=colors, autopct='%1.1f%%',\n",
    "                                             startangle=90)\n",
    "            ax.set_title('Train/Test/Val Split Distribution')\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'No data found', ha='center', va='center', transform=ax.transAxes)\n",
    "            ax.set_title('Train/Test/Val Split Distribution')\n",
    "        \n",
    "        # 3. Reynolds assignment by split\n",
    "        ax = axes[0, 2]\n",
    "        \n",
    "        # Determine which Reynolds numbers are in each split\n",
    "        split_reynolds = {'Train': [], 'Val': [], 'Test': []}\n",
    "        \n",
    "        for split_name, split_dir in [('Train', self.train_dir), \n",
    "                                       ('Val', self.val_dir), \n",
    "                                       ('Test', self.test_dir)]:\n",
    "            if split_dir.exists():\n",
    "                pt_files = list(split_dir.glob(\"Re_*_normalized.pt\"))\n",
    "                reynolds_set = set()\n",
    "                for file in pt_files:\n",
    "                    re_num = self._extract_reynolds_from_filename(file.name)\n",
    "                    if re_num:\n",
    "                        reynolds_set.add(re_num)\n",
    "                split_reynolds[split_name] = sorted(list(reynolds_set))\n",
    "        \n",
    "        y_pos = np.arange(len(split_reynolds))\n",
    "        reynolds_text = []\n",
    "        colors_list = []\n",
    "        \n",
    "        for split, reynolds_list in split_reynolds.items():\n",
    "            if reynolds_list:\n",
    "                reynolds_text.append(f\"Re = {reynolds_list}\")\n",
    "                if split == 'Train':\n",
    "                    colors_list.append('#2ca02c')\n",
    "                elif split == 'Val':\n",
    "                    colors_list.append('#d62728')\n",
    "                else:\n",
    "                    colors_list.append('#ff7f0e')\n",
    "            else:\n",
    "                reynolds_text.append(\"None\")\n",
    "                colors_list.append('#cccccc')\n",
    "        \n",
    "        bars = ax.barh(y_pos, [1, 1, 1], color=colors_list, alpha=0.7)\n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels(list(split_reynolds.keys()))\n",
    "        ax.set_xlabel('Split Assignment')\n",
    "        ax.set_title('Reynolds Number Assignment by Split')\n",
    "        \n",
    "        for i, (bar, text) in enumerate(zip(bars, reynolds_text)):\n",
    "            ax.text(0.5, bar.get_y() + bar.get_height()/2, text,\n",
    "                   ha='center', va='center', fontweight='bold', color='white', fontsize=9)\n",
    "        \n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_xticks([])\n",
    "        \n",
    "        # 4. Feature dimensions breakdown\n",
    "        ax = axes[1, 0]\n",
    "        \n",
    "        # Get feature info from a sample\n",
    "        sample = self._load_split_data('train')\n",
    "        if sample is None:\n",
    "            sample = self._load_split_data('test')\n",
    "        if sample is None:\n",
    "            sample = self._load_split_data('val')\n",
    "        \n",
    "        if sample:\n",
    "            total_features = sample.x.shape[1]\n",
    "            pos_features = 3\n",
    "            physics_features = total_features - pos_features\n",
    "            global_features = sample.global_params.shape[0] if hasattr(sample, 'global_params') and sample.global_params is not None else 0\n",
    "            \n",
    "            dimensions = {\n",
    "                'Position\\nFeatures': pos_features,\n",
    "                'Physics\\nFeatures': physics_features, \n",
    "                'Global\\nFeatures': global_features\n",
    "            }\n",
    "            \n",
    "            dim_names = list(dimensions.keys())\n",
    "            dim_values = list(dimensions.values())\n",
    "            colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "            \n",
    "            bars = ax.bar(dim_names, dim_values, color=colors, alpha=0.7)\n",
    "            ax.set_ylabel('Number of Features')\n",
    "            ax.set_title(f'Feature Dimensions (Total in x: {total_features})')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            for bar, value in zip(bars, dim_values):\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                       f'{value}', ha='center', va='bottom', fontweight='bold')\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'No sample data found', ha='center', va='center', transform=ax.transAxes)\n",
    "            ax.set_title('Feature Dimensions')\n",
    "        \n",
    "        # 5. Variable ranges (only if normalization metadata exists)\n",
    "        ax = axes[1, 1]\n",
    "        \n",
    "        if self.norm_metadata and 'normalization_params' in self.norm_metadata:\n",
    "            key_vars = ['pressure', 'velocity_x', 'vorticity_x']\n",
    "            var_ranges = []\n",
    "            var_names = []\n",
    "            \n",
    "            for var in key_vars:\n",
    "                if var in self.norm_metadata['normalization_params']:\n",
    "                    orig_range = self.norm_metadata['normalization_params'][var]\n",
    "                    var_ranges.append(orig_range['max'] - orig_range['min'])\n",
    "                    var_names.append(var.replace('_', ' ').title())\n",
    "            \n",
    "            if var_ranges:\n",
    "                bars = ax.bar(var_names, var_ranges, color=['#d62728', '#2ca02c', '#ff7f0e'], alpha=0.7)\n",
    "                ax.set_ylabel('Original Range Span')\n",
    "                ax.set_title('Original Variable Ranges')\n",
    "                ax.tick_params(axis='x', rotation=45)\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                \n",
    "                for bar, value in zip(bars, var_ranges):\n",
    "                    height = bar.get_height()\n",
    "                    ax.text(bar.get_x() + bar.get_width()/2., height * 1.05,\n",
    "                           f'{value:.2e}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, 'Variable range data not available', ha='center', va='center', transform=ax.transAxes)\n",
    "        else:\n",
    "            # Show actual ranges from loaded data instead\n",
    "            if sample:\n",
    "                ax.text(0.5, 0.5, 'Normalization metadata not available\\n(showing data structure instead)', \n",
    "                       ha='center', va='center', transform=ax.transAxes, fontsize=10)\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, 'Normalization metadata not available', \n",
    "                       ha='center', va='center', transform=ax.transAxes)\n",
    "        \n",
    "        ax.set_title('Original Variable Ranges')\n",
    "        \n",
    "        # 6. Data structure info\n",
    "        ax = axes[1, 2]\n",
    "        ax.axis('off')\n",
    "        \n",
    "        total_samples = train_count + test_count + val_count\n",
    "        \n",
    "        if sample:\n",
    "            mesh_info = {\n",
    "                'Total Samples': f\"{total_samples:,}\",\n",
    "                'Train/Val/Test': f\"{train_count}/{val_count}/{test_count}\",\n",
    "                'Features in x': f\"{sample.x.shape[1]} (3 pos + {sample.x.shape[1]-3} phys)\",\n",
    "                'Features in y': f\"{sample.y.shape[1]}\",\n",
    "                'Nodes per Graph': f\"{sample.pos.shape[0]:,}\",\n",
    "                'Global params': f\"{sample.global_params.shape[0] if hasattr(sample, 'global_params') and sample.global_params is not None else 0}\",\n",
    "                'Format': 'Shock tube compatible'\n",
    "            }\n",
    "        else:\n",
    "            mesh_info = {\n",
    "                'Total Samples': f\"{total_samples:,}\",\n",
    "                'Train Samples': f\"{train_count:,}\",\n",
    "                'Val Samples': f\"{val_count:,}\",\n",
    "                'Test Samples': f\"{test_count:,}\",\n",
    "                'Format': 'Train/Test/Val folders'\n",
    "            }\n",
    "        \n",
    "        y_positions = np.linspace(0.9, 0.1, len(mesh_info))\n",
    "        \n",
    "        for i, (key, value) in enumerate(mesh_info.items()):\n",
    "            ax.text(0.1, y_positions[i], f'{key}:', fontsize=11, fontweight='bold')\n",
    "            ax.text(0.55, y_positions[i], value, fontsize=11)\n",
    "        \n",
    "        ax.set_title('Dataset Information')\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def analyze_normalized_features(self):\n",
    "        \"\"\"Analyze the distribution of normalized features (physics only).\"\"\"\n",
    "        samples = {}\n",
    "        \n",
    "        # Load one sample from each split\n",
    "        for split_name in ['train', 'test', 'val']:\n",
    "            sample = self._load_split_data(split_name)\n",
    "            if sample is not None:\n",
    "                samples[split_name] = sample\n",
    "        \n",
    "        if not samples:\n",
    "            print(\"No normalized data files found!\")\n",
    "            return\n",
    "        \n",
    "        sample_data = list(samples.values())[0]\n",
    "        \n",
    "        # Get physics feature names (skip first 3 position columns)\n",
    "        if hasattr(sample_data, 'feature_names'):\n",
    "            feature_names = sample_data.feature_names\n",
    "        else:\n",
    "            feature_names = self.physics_vars\n",
    "        \n",
    "        n_features = len(feature_names)\n",
    "        \n",
    "        # Create subplot grid\n",
    "        n_cols = 3\n",
    "        n_rows = (n_features + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
    "        fig.suptitle('Normalized Physics Feature Distributions', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        if n_rows == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        axes_flat = axes.flatten()\n",
    "        \n",
    "        # Plot each physics feature (skip first 3 position columns in x)\n",
    "        for i, feature_name in enumerate(feature_names):\n",
    "            ax = axes_flat[i]\n",
    "            \n",
    "            for split_name, data in samples.items():\n",
    "                if data is not None:\n",
    "                    # Get physics features from x (columns 3 onwards)\n",
    "                    feature_values = data.x[:, 3 + i].numpy()\n",
    "                    \n",
    "                    if len(feature_values) > 5000:\n",
    "                        feature_values = np.random.choice(feature_values, 5000, replace=False)\n",
    "                    \n",
    "                    re_num = data.reynolds_number if hasattr(data, 'reynolds_number') else 'Unknown'\n",
    "                    ax.hist(feature_values, bins=50, alpha=0.6, \n",
    "                           label=f'{split_name.title()} (Re={re_num})', \n",
    "                           density=True)\n",
    "            \n",
    "            ax.set_xlabel('Normalized Value')\n",
    "            ax.set_ylabel('Density')\n",
    "            ax.set_title(f'{feature_name.replace(\"_\", \" \").title()}')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.set_xlim(0, 1)\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for i in range(n_features, len(axes_flat)):\n",
    "            axes_flat[i].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def visualize_parameter_normalization(self):\n",
    "        \"\"\"Visualize the parameter normalization (Reynolds numbers).\"\"\"\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        fig.suptitle('Reynolds Number Normalization', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Collect Reynolds numbers from all splits\n",
    "        reynolds_counts = {}\n",
    "        \n",
    "        for split_dir in [self.train_dir, self.test_dir, self.val_dir]:\n",
    "            if split_dir.exists():\n",
    "                pt_files = list(split_dir.glob(\"Re_*_normalized.pt\"))\n",
    "                for file in pt_files:\n",
    "                    re_num = self._extract_reynolds_from_filename(file.name)\n",
    "                    if re_num:\n",
    "                        data = torch.load(file, weights_only=False)\n",
    "                        reynolds_counts[re_num] = reynolds_counts.get(re_num, 0) + len(data)\n",
    "        \n",
    "        if not reynolds_counts:\n",
    "            print(\"No Reynolds number data found!\")\n",
    "            for ax in axes:\n",
    "                ax.text(0.5, 0.5, 'No data found', ha='center', va='center', transform=ax.transAxes)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            return\n",
    "        \n",
    "        reynolds_nums = sorted(reynolds_counts.keys())\n",
    "        case_counts = [reynolds_counts[re] for re in reynolds_nums]\n",
    "        \n",
    "        # Original Reynolds numbers\n",
    "        ax = axes[0]\n",
    "        bars = ax.bar(reynolds_nums, case_counts, alpha=0.7, color='steelblue')\n",
    "        ax.set_xlabel('Original Reynolds Number')\n",
    "        ax.set_ylabel('Number of Samples')\n",
    "        ax.set_title('Original Reynolds Distribution')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        for bar, count in zip(bars, case_counts):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "                   f'{count}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Normalized Reynolds numbers (if metadata available)\n",
    "        ax = axes[1]\n",
    "        if self.norm_metadata and 'global_param_normalization' in self.norm_metadata:\n",
    "            if 'reynolds_number' in self.norm_metadata['global_param_normalization']:\n",
    "                re_norm_params = self.norm_metadata['global_param_normalization']['reynolds_number']\n",
    "                re_min = re_norm_params['min']\n",
    "                re_max = re_norm_params['max']\n",
    "                \n",
    "                normalized_reynolds = [(re - re_min) / (re_max - re_min) for re in reynolds_nums]\n",
    "                \n",
    "                bars = ax.bar(normalized_reynolds, case_counts, alpha=0.7, \n",
    "                             color='steelblue', width=0.05)\n",
    "                ax.set_xlabel('Normalized Reynolds Number')\n",
    "                ax.set_ylabel('Number of Samples')\n",
    "                ax.set_title('Normalized Reynolds Distribution')\n",
    "                ax.set_xlim(-0.1, 1.1)\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                \n",
    "                for bar, orig_re, count in zip(bars, reynolds_nums, case_counts):\n",
    "                    height = bar.get_height()\n",
    "                    ax.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "                           f'Re={orig_re}\\n({count})', ha='center', va='bottom', \n",
    "                           fontweight='bold', fontsize=9)\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, 'Reynolds normalization\\ndata not in metadata', \n",
    "                       ha='center', va='center', transform=ax.transAxes)\n",
    "        else:\n",
    "            # Try to infer from actual data\n",
    "            sample_file = None\n",
    "            for split_dir in [self.train_dir, self.test_dir, self.val_dir]:\n",
    "                if split_dir.exists():\n",
    "                    files = list(split_dir.glob(\"Re_*_normalized.pt\"))\n",
    "                    if files:\n",
    "                        sample_file = files[0]\n",
    "                        break\n",
    "            \n",
    "            if sample_file:\n",
    "                data = torch.load(sample_file, weights_only=False)\n",
    "                if len(data) > 0 and hasattr(data[0], 'global_params') and data[0].global_params is not None:\n",
    "                    ax.text(0.5, 0.5, f'Normalization metadata not available\\n(Sample global_params: {data[0].global_params[0].item():.3f})', \n",
    "                           ha='center', va='center', transform=ax.transAxes, fontsize=10)\n",
    "                else:\n",
    "                    ax.text(0.5, 0.5, 'Normalization metadata not available', \n",
    "                           ha='center', va='center', transform=ax.transAxes)\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, 'Normalization metadata not available', \n",
    "                       ha='center', va='center', transform=ax.transAxes)\n",
    "        \n",
    "        # Normalization mapping\n",
    "        ax = axes[2]\n",
    "        if self.norm_metadata and 'global_param_normalization' in self.norm_metadata:\n",
    "            if 'reynolds_number' in self.norm_metadata['global_param_normalization']:\n",
    "                re_norm_params = self.norm_metadata['global_param_normalization']['reynolds_number']\n",
    "                re_min = re_norm_params['min']\n",
    "                re_max = re_norm_params['max']\n",
    "                \n",
    "                original_range = np.linspace(re_min, re_max, 100)\n",
    "                normalized_range = (original_range - re_min) / (re_max - re_min)\n",
    "                \n",
    "                ax.plot(original_range, normalized_range, 'b-', linewidth=2, \n",
    "                       label='Normalization Function')\n",
    "                \n",
    "                for re_num in reynolds_nums:\n",
    "                    norm_val = (re_num - re_min) / (re_max - re_min)\n",
    "                    ax.plot(re_num, norm_val, 'ro', markersize=10)\n",
    "                    ax.annotate(f'Re={re_num}', (re_num, norm_val), \n",
    "                               xytext=(10, 10), textcoords='offset points', fontweight='bold')\n",
    "                \n",
    "                ax.set_xlabel('Original Reynolds Number')\n",
    "                ax.set_ylabel('Normalized Value [0, 1]')\n",
    "                ax.set_title('Reynolds Number Normalization Mapping')\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                ax.legend()\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, 'Reynolds normalization\\ndata not in metadata', \n",
    "                       ha='center', va='center', transform=ax.transAxes)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'Normalization metadata not available', \n",
    "                   ha='center', va='center', transform=ax.transAxes)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def analyze_individual_cases(self):\n",
    "        \"\"\"Analyze the individual normalized case files across all splits.\"\"\"\n",
    "        case_info = {}\n",
    "        \n",
    "        # Collect all case files from all splits\n",
    "        for split_name, split_dir in [('train', self.train_dir), \n",
    "                                       ('val', self.val_dir), \n",
    "                                       ('test', self.test_dir)]:\n",
    "            if split_dir.exists():\n",
    "                case_files = list(split_dir.glob(\"Re_*_normalized.pt\"))\n",
    "                \n",
    "                for case_file in case_files:\n",
    "                    re_num = self._extract_reynolds_from_filename(case_file.name)\n",
    "                    if re_num is None:\n",
    "                        continue\n",
    "                    \n",
    "                    case_data = torch.load(case_file, weights_only=False)\n",
    "                    \n",
    "                    key = f\"{re_num}_{split_name}\"\n",
    "                    case_info[key] = {\n",
    "                        'reynolds': re_num,\n",
    "                        'split': split_name,\n",
    "                        'n_samples': len(case_data),\n",
    "                        'filename': case_file.name,\n",
    "                        'sample_data': case_data[0] if len(case_data) > 0 else None\n",
    "                    }\n",
    "        \n",
    "        if not case_info:\n",
    "            print(\"No individual case files found!\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        fig.suptitle('Individual Case Analysis (All Splits)', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # 1. Sample counts per case with split coloring\n",
    "        ax = axes[0, 0]\n",
    "        \n",
    "        sorted_cases = sorted(case_info.items(), key=lambda x: (x[1]['reynolds'], x[1]['split']))\n",
    "        labels = [f\"Re={info['reynolds']}\\n({info['split']})\" for _, info in sorted_cases]\n",
    "        sample_counts = [info['n_samples'] for _, info in sorted_cases]\n",
    "        colors = ['#2ca02c' if info['split'] == 'train' else \n",
    "                 '#d62728' if info['split'] == 'val' else '#ff7f0e' \n",
    "                 for _, info in sorted_cases]\n",
    "        \n",
    "        x_pos = np.arange(len(labels))\n",
    "        bars = ax.bar(x_pos, sample_counts, alpha=0.7, color=colors)\n",
    "        ax.set_xticks(x_pos)\n",
    "        ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "        ax.set_ylabel('Number of Timestep Pairs')\n",
    "        ax.set_title('Samples per Case File (Colored by Split)')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        for bar, count in zip(bars, sample_counts):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 2,\n",
    "                   f'{count}', ha='center', va='bottom', fontweight='bold', fontsize=8)\n",
    "        \n",
    "        # 2. Global parameter values by split\n",
    "        ax = axes[0, 1]\n",
    "        \n",
    "        for split_name, split_color in [('train', '#2ca02c'), ('val', '#d62728'), ('test', '#ff7f0e')]:\n",
    "            split_cases = [(info['reynolds'], info['sample_data']) \n",
    "                          for info in case_info.values() if info['split'] == split_name]\n",
    "            \n",
    "            if split_cases:\n",
    "                reynolds_list = [re for re, _ in split_cases]\n",
    "                global_params = []\n",
    "                \n",
    "                for _, sample in split_cases:\n",
    "                    if sample and hasattr(sample, 'global_params') and sample.global_params is not None:\n",
    "                        global_params.append(sample.global_params[0].item())\n",
    "                    else:\n",
    "                        global_params.append(0)\n",
    "                \n",
    "                ax.scatter(reynolds_list, global_params, alpha=0.7, s=100, \n",
    "                          label=split_name.title(), color=split_color)\n",
    "        \n",
    "        ax.set_xlabel('Original Reynolds Number')\n",
    "        ax.set_ylabel('Normalized Global Parameter Value')\n",
    "        ax.set_title('Global Parameter Values by Split')\n",
    "        ax.set_ylim(-0.1, 1.1)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend()\n",
    "        \n",
    "        # 3. Feature range verification\n",
    "        ax = axes[1, 0]\n",
    "        \n",
    "        feature_ranges = {}\n",
    "        \n",
    "        for key, info in case_info.items():\n",
    "            sample = info['sample_data']\n",
    "            if sample:\n",
    "                x_min = sample.x.min().item()\n",
    "                x_max = sample.x.max().item()\n",
    "                feature_ranges[key] = {\n",
    "                    'min': x_min, \n",
    "                    'max': x_max,\n",
    "                    'reynolds': info['reynolds'],\n",
    "                    'split': info['split']\n",
    "                }\n",
    "        \n",
    "        sorted_ranges = sorted(feature_ranges.items(), key=lambda x: (x[1]['reynolds'], x[1]['split']))\n",
    "        \n",
    "        labels = [f\"Re={info['reynolds']}\\n({info['split']})\" for _, info in sorted_ranges]\n",
    "        mins = [info['min'] for _, info in sorted_ranges]\n",
    "        maxs = [info['max'] for _, info in sorted_ranges]\n",
    "        colors = ['#2ca02c' if info['split'] == 'train' else \n",
    "                 '#d62728' if info['split'] == 'val' else '#ff7f0e' \n",
    "                 for _, info in sorted_ranges]\n",
    "        \n",
    "        x_pos = np.arange(len(labels))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars1 = ax.bar(x_pos - width/2, mins, width, label='Min', alpha=0.7, color='red')\n",
    "        bars2 = ax.bar(x_pos + width/2, maxs, width, label='Max', alpha=0.7, color='blue')\n",
    "        \n",
    "        ax.set_xticks(x_pos)\n",
    "        ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "        ax.set_ylabel('Feature Value Range')\n",
    "        ax.set_title('Normalized Feature Range Verification')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.axhline(y=0, color='green', linestyle='--', alpha=0.7, label='Expected min (0)')\n",
    "        ax.axhline(y=1, color='green', linestyle='--', alpha=0.7, label='Expected max (1)')\n",
    "        \n",
    "        # 4. Data structure summary\n",
    "        ax = axes[1, 1]\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Use first available sample\n",
    "        sample = None\n",
    "        for info in case_info.values():\n",
    "            if info['sample_data'] is not None:\n",
    "                sample = info['sample_data']\n",
    "                break\n",
    "        \n",
    "        if sample:\n",
    "            total_samples = sum(info['n_samples'] for info in case_info.values())\n",
    "            n_train = sum(info['n_samples'] for info in case_info.values() if info['split'] == 'train')\n",
    "            n_val = sum(info['n_samples'] for info in case_info.values() if info['split'] == 'val')\n",
    "            n_test = sum(info['n_samples'] for info in case_info.values() if info['split'] == 'test')\n",
    "            \n",
    "            summary_info = {\n",
    "                'Total Samples': f\"{total_samples:,}\",\n",
    "                'Train/Val/Test': f\"{n_train}/{n_val}/{n_test}\",\n",
    "                'Nodes per Graph': f\"{sample.pos.shape[0]:,}\",\n",
    "                'x features': f\"{sample.x.shape[1]} (3 pos + {sample.x.shape[1]-3} phys)\",\n",
    "                'y features': f\"{sample.y.shape[1]}\",\n",
    "                'Edges': f\"{sample.edge_index.shape[1]:,}\",\n",
    "                'Global params': f\"{sample.global_params.shape[0] if sample.global_params is not None else 0}\",\n",
    "                'Format': 'Shock tube compatible'\n",
    "            }\n",
    "            \n",
    "            y_positions = np.linspace(0.9, 0.1, len(summary_info))\n",
    "            \n",
    "            for i, (key, value) in enumerate(summary_info.items()):\n",
    "                ax.text(0.1, y_positions[i], f'{key}:', fontsize=11, fontweight='bold')\n",
    "                ax.text(0.6, y_positions[i], value, fontsize=11)\n",
    "        \n",
    "        ax.set_title('Data Structure Summary')\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def create_complete_analysis(self):\n",
    "        \"\"\"Create a complete analysis of the dataset.\"\"\"\n",
    "        print(\"Creating complete Karman vortex dataset analysis...\")\n",
    "        print(f\"Files expected: Re_XXX_normalized.pt format\")\n",
    "        \n",
    "        print(\"\\n1. Dataset Overview\")\n",
    "        self.visualize_dataset_overview()\n",
    "        \n",
    "        print(\"\\n2. Normalized Feature Analysis\")\n",
    "        self.analyze_normalized_features()\n",
    "        \n",
    "        print(\"\\n3. Parameter Normalization Analysis\")\n",
    "        self.visualize_parameter_normalization()\n",
    "        \n",
    "        print(\"\\n4. Individual Case Analysis\")\n",
    "        self.analyze_individual_cases()\n",
    "        \n",
    "        print(\"\\nAnalysis complete!\")\n",
    "\n",
    "def analyze_karman_dataset(dataset_dir):\n",
    "    \"\"\"\n",
    "    Analyze a Karman vortex dataset with train/test/val folder structure.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset_dir : str\n",
    "        Path to the dataset directory containing train, test, and val folders\n",
    "        Files should be named: Re_XXX_normalized.pt\n",
    "    \"\"\"\n",
    "    visualizer = KarmanDatasetVisualizer(dataset_dir)\n",
    "    visualizer.create_complete_analysis()\n",
    "    return visualizer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset_path = \"/standard/sds_baek_energetic/von_karman_vortex/full_data/split_normalized\"\n",
    "    \n",
    "    print(\"Karman Vortex Street Dataset Visualizer\")\n",
    "    print(f\"Analyzing dataset at: {dataset_path}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    visualizer = analyze_karman_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c5f486d-0561-4c8b-8a79-81a2e9daaf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 training files\n",
      "Files: ['Reynolds_20_raw_data_normalized.pt', 'Reynolds_50_raw_data_normalized.pt', 'Reynolds_150_raw_data_normalized.pt', 'Reynolds_100_raw_data_normalized.pt', 'Reynolds_40_raw_data_normalized.pt']\n",
      "\n",
      "============================================================\n",
      "Inspecting: Reynolds_20_raw_data_normalized.pt\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtb3sud/.local/lib/python3.11/site-packages/torch_geometric/typing.py:68: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n",
      "/home/jtb3sud/.local/lib/python3.11/site-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /sfs/gpfs/tardis/home/jtb3sud/.local/lib/python3.11/site-packages/torch_scatter/_version_cpu.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "/home/jtb3sud/.local/lib/python3.11/site-packages/torch_geometric/typing.py:97: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: /sfs/gpfs/tardis/home/jtb3sud/.local/lib/python3.11/site-packages/torch_cluster/_version_cpu.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n",
      "/home/jtb3sud/.local/lib/python3.11/site-packages/torch_geometric/typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: /sfs/gpfs/tardis/home/jtb3sud/.local/lib/python3.11/site-packages/torch_spline_conv/_version_cpu.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
      "  warnings.warn(\n",
      "/home/jtb3sud/.local/lib/python3.11/site-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /sfs/gpfs/tardis/home/jtb3sud/.local/lib/python3.11/site-packages/torch_sparse/_version_cpu.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total timesteps in file: 400\n",
      "\n",
      "First timestep structure:\n",
      "  Type: <class 'torch_geometric.data.data.Data'>\n",
      "  Attributes: ['__abstractmethods__', '__annotations__', '__call__', '__cat_dim__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__inc__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_edge_attr_cls', '_edge_to_layout', '_edges_to_layout', '_get_edge_index', '_get_tensor', '_get_tensor_size', '_multi_get_tensor', '_put_edge_index', '_put_tensor', '_remove_edge_index', '_remove_tensor', '_store', '_tensor_attr_cls', '_to_type', 'apply', 'apply_', 'batch', 'clone', 'coalesce', 'concat', 'contains_isolated_nodes', 'contains_self_loops', 'contiguous', 'coo', 'cpu', 'csc', 'csr', 'cuda', 'debug', 'detach', 'detach_', 'edge_attr', 'edge_attrs', 'edge_index', 'edge_stores', 'edge_subgraph', 'edge_weight', 'face', 'from_dict', 'generate_ids', 'get_all_edge_attrs', 'get_all_tensor_attrs', 'get_edge_index', 'get_tensor', 'get_tensor_size', 'has_isolated_nodes', 'has_self_loops', 'is_coalesced', 'is_cuda', 'is_directed', 'is_edge_attr', 'is_node_attr', 'is_sorted', 'is_sorted_by_time', 'is_undirected', 'keys', 'multi_get_tensor', 'node_attrs', 'node_offsets', 'node_stores', 'num_edge_features', 'num_edge_types', 'num_edges', 'num_faces', 'num_features', 'num_node_features', 'num_node_types', 'num_nodes', 'pin_memory', 'pos', 'put_edge_index', 'put_tensor', 'record_stream', 'remove_edge_index', 'remove_tensor', 'requires_grad_', 'share_memory_', 'size', 'snapshot', 'sort', 'sort_by_time', 'stores', 'stores_as', 'subgraph', 'time', 'to', 'to_dict', 'to_heterogeneous', 'to_namedtuple', 'up_to', 'update', 'update_tensor', 'validate', 'view', 'x', 'y']\n",
      "\n",
      "Tensor shapes:\n",
      "  x.shape: torch.Size([60746, 10])\n",
      "  y.shape: torch.Size([60746, 7])\n",
      "  pos.shape: torch.Size([60746, 3])\n",
      "  edge_index.shape: torch.Size([2, 318178])\n",
      "  global_params.shape: torch.Size([1])\n",
      "  global_params value: tensor([0.1275])\n",
      "  reynolds_number: 20\n",
      "\n",
      "============================================================\n",
      "X TENSOR STATISTICS (Input features)\n",
      "============================================================\n",
      "Shape: torch.Size([60746, 10]) (nodes, features)\n",
      "\n",
      "Per-feature statistics (first 10 features):\n",
      "\n",
      "  [0] x_pos:\n",
      "    min=0.000000, max=1.000000\n",
      "    mean=0.542294, std=0.270234\n",
      "    NaN: False, Inf: False\n",
      "\n",
      "  [1] y_pos:\n",
      "    min=0.000000, max=1.000000\n",
      "    mean=0.499992, std=0.222780\n",
      "    NaN: False, Inf: False\n",
      "\n",
      "  [2] z_pos:\n",
      "    min=0.000000, max=1.000000\n",
      "    mean=0.500000, std=0.500004\n",
      "    NaN: False, Inf: False\n",
      "\n",
      "  [3] pressure:\n",
      "    min=0.656789, max=0.656789\n",
      "    mean=0.656789, std=0.000000\n",
      "    NaN: False, Inf: False\n",
      "\n",
      "  [4] vel_x:\n",
      "    min=0.299571, max=0.357008\n",
      "    mean=0.314047, std=0.006861\n",
      "    NaN: False, Inf: False\n",
      "\n",
      "  [5] vel_y:\n",
      "    min=0.453305, max=0.453305\n",
      "    mean=0.453305, std=0.000000\n",
      "    NaN: False, Inf: False\n",
      "\n",
      "  [6] vel_z:\n",
      "    min=0.628807, max=0.628807\n",
      "    mean=0.628807, std=0.000000\n",
      "    NaN: False, Inf: False\n",
      "\n",
      "  [7] vort_x:\n",
      "    min=0.596225, max=0.596225\n",
      "    mean=0.596225, std=0.000000\n",
      "    NaN: False, Inf: False\n",
      "\n",
      "  [8] vort_y:\n",
      "    min=0.593315, max=0.678677\n",
      "    mean=0.629491, std=0.000844\n",
      "    NaN: False, Inf: False\n",
      "\n",
      "  [9] vort_z:\n",
      "    min=0.476932, max=0.610266\n",
      "    mean=0.550234, std=0.001927\n",
      "    NaN: False, Inf: False\n",
      "\n",
      "============================================================\n",
      "Y TENSOR STATISTICS (Target/labels)\n",
      "============================================================\n",
      "Shape: torch.Size([60746, 7]) (nodes, features)\n",
      "\n",
      "Per-feature statistics:\n",
      "\n",
      "  [0] pressure:\n",
      "    min=0.642544, max=0.668512\n",
      "    mean=0.655940, std=0.001707\n",
      "    NaN: False, Inf: False\n",
      "\n",
      "  [1] vel_x:\n",
      "    min=0.299571, max=0.390282\n",
      "    mean=0.355485, std=0.008145\n",
      "    NaN: False, Inf: False\n",
      "\n",
      "  [2] vel_y:\n",
      "    min=0.398559, max=0.517110\n",
      "    mean=0.454996, std=0.006262\n",
      "    NaN: False, Inf: False\n",
      "\n",
      "  [3] vel_z:\n",
      "    min=0.617642, max=0.638353\n",
      "    mean=0.628804, std=0.000264\n",
      "    NaN: False, Inf: False\n",
      "\n",
      "  [4] vort_x:\n",
      "    min=0.575096, max=0.609260\n",
      "    mean=0.596223, std=0.000426\n",
      "    NaN: False, Inf: False\n",
      "\n",
      "  [5] vort_y:\n",
      "    min=0.602307, max=0.652721\n",
      "    mean=0.629480, std=0.000633\n",
      "    NaN: False, Inf: False\n",
      "\n",
      "  [6] vort_z:\n",
      "    min=0.524074, max=0.575774\n",
      "    mean=0.550247, std=0.001463\n",
      "    NaN: False, Inf: False\n",
      "\n",
      "============================================================\n",
      "CHECKING MULTIPLE TIMESTEPS\n",
      "============================================================\n",
      "\n",
      "九 No obvious issues in first 10 timesteps\n",
      "\n",
      "============================================================\n",
      "SCANNING ALL FILES\n",
      "============================================================\n",
      "九 Reynolds_20_raw_data_normalized.pt: 400 timesteps, x=torch.Size([60746, 10]), y=torch.Size([60746, 7])\n",
      "九 Reynolds_50_raw_data_normalized.pt: 400 timesteps, x=torch.Size([60746, 10]), y=torch.Size([60746, 7])\n",
      "九 Reynolds_150_raw_data_normalized.pt: 400 timesteps, x=torch.Size([60746, 10]), y=torch.Size([60746, 7])\n",
      "九 Reynolds_100_raw_data_normalized.pt: 400 timesteps, x=torch.Size([60746, 10]), y=torch.Size([60746, 7])\n",
      "九 Reynolds_40_raw_data_normalized.pt: 400 timesteps, x=torch.Size([60746, 10]), y=torch.Size([60746, 7])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def inspect_karman_training_data(train_dir=\"/standard/sds_baek_energetic/von_karman_vortex/Reynolds 1~150/processed_parc/normalized/train/\"):\n",
    "    \"\"\"Inspect normalized Karman vortex training data for issues.\"\"\"\n",
    "    \n",
    "    train_path = Path(train_dir)\n",
    "    \n",
    "    if not train_path.exists():\n",
    "        print(f\"Directory not found: {train_path}\")\n",
    "        return\n",
    "    \n",
    "    # Find all .pt files\n",
    "    data_files = list(train_path.glob(\"*.pt\"))\n",
    "    print(f\"Found {len(data_files)} training files\")\n",
    "    print(f\"Files: {[f.name for f in data_files[:5]]}\")\n",
    "    \n",
    "    if not data_files:\n",
    "        print(\"No .pt files found!\")\n",
    "        return\n",
    "    \n",
    "    # Load and inspect first file\n",
    "    first_file = data_files[0]\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Inspecting: {first_file.name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    data = torch.load(first_file, weights_only=False)\n",
    "    \n",
    "    if not isinstance(data, list):\n",
    "        print(f\"Unexpected format: {type(data)}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Total timesteps in file: {len(data)}\")\n",
    "    \n",
    "    # Inspect first timestep\n",
    "    sample = data[0]\n",
    "    print(f\"\\nFirst timestep structure:\")\n",
    "    print(f\"  Type: {type(sample)}\")\n",
    "    print(f\"  Attributes: {dir(sample)}\")\n",
    "    \n",
    "    # Check shapes\n",
    "    print(f\"\\nTensor shapes:\")\n",
    "    print(f\"  x.shape: {sample.x.shape}\")\n",
    "    print(f\"  y.shape: {sample.y.shape}\")\n",
    "    if hasattr(sample, 'pos'):\n",
    "        print(f\"  pos.shape: {sample.pos.shape}\")\n",
    "    if hasattr(sample, 'edge_index'):\n",
    "        print(f\"  edge_index.shape: {sample.edge_index.shape}\")\n",
    "    if hasattr(sample, 'global_params'):\n",
    "        print(f\"  global_params.shape: {sample.global_params.shape}\")\n",
    "        print(f\"  global_params value: {sample.global_params}\")\n",
    "    \n",
    "    # Check for metadata\n",
    "    if hasattr(sample, 'reynolds_number'):\n",
    "        print(f\"  reynolds_number: {sample.reynolds_number}\")\n",
    "    \n",
    "    # Detailed statistics on x tensor\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"X TENSOR STATISTICS (Input features)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    x = sample.x\n",
    "    print(f\"Shape: {x.shape} (nodes, features)\")\n",
    "    print(f\"\\nPer-feature statistics (first 10 features):\")\n",
    "    \n",
    "    feature_names = ['x_pos', 'y_pos', 'z_pos', 'pressure', 'vel_x', 'vel_y', 'vel_z', 'vort_x', 'vort_y', 'vort_z']\n",
    "    \n",
    "    for i in range(min(x.shape[1], 10)):\n",
    "        feat = x[:, i]\n",
    "        name = feature_names[i] if i < len(feature_names) else f\"feat_{i}\"\n",
    "        \n",
    "        has_nan = torch.isnan(feat).any().item()\n",
    "        has_inf = torch.isinf(feat).any().item()\n",
    "        \n",
    "        print(f\"\\n  [{i}] {name}:\")\n",
    "        print(f\"    min={feat.min().item():.6f}, max={feat.max().item():.6f}\")\n",
    "        print(f\"    mean={feat.mean().item():.6f}, std={feat.std().item():.6f}\")\n",
    "        print(f\"    NaN: {has_nan}, Inf: {has_inf}\")\n",
    "        \n",
    "        if has_nan or has_inf:\n",
    "            print(f\"    丘멆잺 WARNING: Contains NaN or Inf!\")\n",
    "    \n",
    "    # Check y tensor\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Y TENSOR STATISTICS (Target/labels)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    y = sample.y\n",
    "    print(f\"Shape: {y.shape} (nodes, features)\")\n",
    "    print(f\"\\nPer-feature statistics:\")\n",
    "    \n",
    "    target_names = ['pressure', 'vel_x', 'vel_y', 'vel_z', 'vort_x', 'vort_y', 'vort_z']\n",
    "    \n",
    "    for i in range(y.shape[1]):\n",
    "        feat = y[:, i]\n",
    "        name = target_names[i] if i < len(target_names) else f\"target_{i}\"\n",
    "        \n",
    "        has_nan = torch.isnan(feat).any().item()\n",
    "        has_inf = torch.isinf(feat).any().item()\n",
    "        \n",
    "        print(f\"\\n  [{i}] {name}:\")\n",
    "        print(f\"    min={feat.min().item():.6f}, max={feat.max().item():.6f}\")\n",
    "        print(f\"    mean={feat.mean().item():.6f}, std={feat.std().item():.6f}\")\n",
    "        print(f\"    NaN: {has_nan}, Inf: {has_inf}\")\n",
    "        \n",
    "        if has_nan or has_inf:\n",
    "            print(f\"    丘멆잺 WARNING: Contains NaN or Inf!\")\n",
    "    \n",
    "    # Check multiple timesteps for consistency\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CHECKING MULTIPLE TIMESTEPS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    issues_found = []\n",
    "    \n",
    "    for t in range(min(10, len(data))):\n",
    "        sample_t = data[t]\n",
    "        \n",
    "        if torch.isnan(sample_t.x).any():\n",
    "            issues_found.append(f\"Timestep {t}: NaN in x\")\n",
    "        if torch.isinf(sample_t.x).any():\n",
    "            issues_found.append(f\"Timestep {t}: Inf in x\")\n",
    "        if torch.isnan(sample_t.y).any():\n",
    "            issues_found.append(f\"Timestep {t}: NaN in y\")\n",
    "        if torch.isinf(sample_t.y).any():\n",
    "            issues_found.append(f\"Timestep {t}: Inf in y\")\n",
    "        \n",
    "        # Check if values are reasonable (should be in [0, 1] if normalized)\n",
    "        if sample_t.x.min() < -0.1 or sample_t.x.max() > 1.1:\n",
    "            issues_found.append(f\"Timestep {t}: x values outside [0,1] range: [{sample_t.x.min():.3f}, {sample_t.x.max():.3f}]\")\n",
    "        if sample_t.y.min() < -0.1 or sample_t.y.max() > 1.1:\n",
    "            issues_found.append(f\"Timestep {t}: y values outside [0,1] range: [{sample_t.y.min():.3f}, {sample_t.y.max():.3f}]\")\n",
    "    \n",
    "    if issues_found:\n",
    "        print(\"\\n丘멆잺 ISSUES FOUND:\")\n",
    "        for issue in issues_found:\n",
    "            print(f\"  - {issue}\")\n",
    "    else:\n",
    "        print(\"\\n九 No obvious issues in first 10 timesteps\")\n",
    "    \n",
    "    # Check all files briefly\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SCANNING ALL FILES\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for file_path in data_files[:5]:  # Check first 5 files\n",
    "        try:\n",
    "            file_data = torch.load(file_path, weights_only=False)\n",
    "            sample = file_data[0]\n",
    "            \n",
    "            has_issues = (\n",
    "                torch.isnan(sample.x).any() or \n",
    "                torch.isinf(sample.x).any() or\n",
    "                torch.isnan(sample.y).any() or \n",
    "                torch.isinf(sample.y).any()\n",
    "            )\n",
    "            \n",
    "            status = \"丘멆잺 HAS ISSUES\" if has_issues else \"九늎"\n",
    "            print(f\"{status} {file_path.name}: {len(file_data)} timesteps, x={sample.x.shape}, y={sample.y.shape}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"九 {file_path.name}: Error loading - {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    inspect_karman_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d134900-5e48-40ef-9cb5-e8ea61c672c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded simulation with 400 timesteps\n",
      "First timestep has 60746 nodes\n",
      "Feature shape: torch.Size([60746, 10])\n",
      "\n",
      "Timesteps: 400\n",
      "Nodes: 60746\n",
      "Reynolds: 0.12751677632331848\n",
      "\n",
      "Visualizing 40 timesteps\n",
      "Using 10000 nodes\n",
      "\n",
      "Creating GIF: ./cylinder_gifs/Re0_pressure.gif\n",
      "  Variable: pressure\n",
      "  Frames: 40\n",
      "  九 Saved: ./cylinder_gifs/Re0_pressure.gif\n",
      "\n",
      "Creating multi-variable GIF: ./cylinder_gifs/Re0_multi_pressure_x_velocity_y_velocity.gif\n",
      "  Variables: ['pressure', 'x_velocity', 'y_velocity']\n",
      "  Frames: 40\n",
      "  九 Saved: ./cylinder_gifs/Re0_multi_pressure_x_velocity_y_velocity.gif\n",
      "\n",
      "Creating GIFs for all 7 variables...\n",
      "Output directory: ./cylinder_gifs\n",
      "\n",
      "Creating GIF: ./cylinder_gifs/Re0_pressure.gif\n",
      "  Variable: pressure\n",
      "  Frames: 40\n",
      "  九 Saved: ./cylinder_gifs/Re0_pressure.gif\n",
      "\n",
      "Creating GIF: ./cylinder_gifs/Re0_x_velocity.gif\n",
      "  Variable: x_velocity\n",
      "  Frames: 40\n",
      "  九 Saved: ./cylinder_gifs/Re0_x_velocity.gif\n",
      "\n",
      "Creating GIF: ./cylinder_gifs/Re0_y_velocity.gif\n",
      "  Variable: y_velocity\n",
      "  Frames: 40\n",
      "  九 Saved: ./cylinder_gifs/Re0_y_velocity.gif\n",
      "\n",
      "Creating GIF: ./cylinder_gifs/Re0_z_velocity.gif\n",
      "  Variable: z_velocity\n",
      "  Frames: 40\n",
      "  九 Saved: ./cylinder_gifs/Re0_z_velocity.gif\n",
      "\n",
      "Creating GIF: ./cylinder_gifs/Re0_x_vorticity.gif\n",
      "  Variable: x_vorticity\n",
      "  Frames: 40\n",
      "  九 Saved: ./cylinder_gifs/Re0_x_vorticity.gif\n",
      "\n",
      "Creating GIF: ./cylinder_gifs/Re0_y_vorticity.gif\n",
      "  Variable: y_vorticity\n",
      "  Frames: 40\n",
      "  九 Saved: ./cylinder_gifs/Re0_y_vorticity.gif\n",
      "\n",
      "Creating GIF: ./cylinder_gifs/Re0_z_vorticity.gif\n",
      "  Variable: z_vorticity\n",
      "  Frames: 40\n",
      "  九 Saved: ./cylinder_gifs/Re0_z_vorticity.gif\n",
      "\n",
      "============================================================\n",
      "Summary: Created 7/7 GIFs\n",
      "============================================================\n",
      "  ./cylinder_gifs/Re0_pressure.gif\n",
      "  ./cylinder_gifs/Re0_x_velocity.gif\n",
      "  ./cylinder_gifs/Re0_y_velocity.gif\n",
      "  ./cylinder_gifs/Re0_z_velocity.gif\n",
      "  ./cylinder_gifs/Re0_x_vorticity.gif\n",
      "  ./cylinder_gifs/Re0_y_vorticity.gif\n",
      "  ./cylinder_gifs/Re0_z_vorticity.gif\n",
      "\n",
      "======================================================================\n",
      "VARIABLE STATISTICS\n",
      "======================================================================\n",
      "\n",
      "Pressure:\n",
      "  Range: [6.4642e-01, 6.6418e-01]\n",
      "  Mean:  6.5641e-01 췀 1.0190e-03\n",
      "\n",
      "X Velocity:\n",
      "  Range: [2.9650e-01, 3.7815e-01]\n",
      "  Mean:  3.4909e-01 췀 1.2859e-02\n",
      "\n",
      "Y Velocity:\n",
      "  Range: [4.0549e-01, 5.1854e-01]\n",
      "  Mean:  4.5973e-01 췀 9.8341e-03\n",
      "\n",
      "Z Velocity:\n",
      "  Range: [6.2135e-01, 6.3519e-01]\n",
      "  Mean:  6.2881e-01 췀 1.2606e-04\n",
      "\n",
      "X Vorticity:\n",
      "  Range: [5.8430e-01, 6.0406e-01]\n",
      "  Mean:  5.9622e-01 췀 1.8160e-04\n",
      "\n",
      "Y Vorticity:\n",
      "  Range: [5.9332e-01, 6.7868e-01]\n",
      "  Mean:  6.2948e-01 췀 3.4567e-04\n",
      "\n",
      "Z Vorticity:\n",
      "  Range: [4.7693e-01, 6.1027e-01]\n",
      "  Mean:  5.5031e-01 췀 7.7634e-04\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Jupyter Notebook for Visualizing Cylinder Flow - GIF Export Version\n",
    "====================================================================\n",
    "Creates actual GIF files saved to disk.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 1: Load Data\n",
    "# ============================================================================\n",
    "\n",
    "simulation_file = \"/standard/sds_baek_energetic/von_karman_vortex/Reynolds 1~150/processed_parc/normalized/train/Reynolds_20_raw_data_normalized.pt\"\n",
    "simulation = torch.load(simulation_file, weights_only=False)\n",
    "\n",
    "print(f\"Loaded simulation with {len(simulation)} timesteps\")\n",
    "print(f\"First timestep has {simulation[0].num_nodes} nodes\")\n",
    "print(f\"Feature shape: {simulation[0].x.shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 2: Extract Information\n",
    "# ============================================================================\n",
    "\n",
    "n_timesteps = len(simulation)\n",
    "n_nodes = simulation[0].num_nodes\n",
    "n_features = simulation[0].x.shape[1]\n",
    "positions = simulation[0].x[:, :3].numpy()\n",
    "\n",
    "reynolds = None\n",
    "if hasattr(simulation[0], 'global_params'):\n",
    "    reynolds = float(simulation[0].global_params[0])\n",
    "elif hasattr(simulation[0], 'reynolds'):\n",
    "    reynolds = float(simulation[0].reynolds)\n",
    "\n",
    "print(f\"\\nTimesteps: {n_timesteps}\")\n",
    "print(f\"Nodes: {n_nodes}\")\n",
    "print(f\"Reynolds: {reynolds if reynolds else 'Not found'}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 3: Configuration\n",
    "# ============================================================================\n",
    "\n",
    "num_static_feats = 3\n",
    "num_dynamic_feats = 7\n",
    "\n",
    "var_names = [\n",
    "    'pressure',\n",
    "    'x_velocity', \n",
    "    'y_velocity',\n",
    "    'z_velocity',\n",
    "    'x_vorticity', \n",
    "    'y_vorticity',\n",
    "    'z_vorticity'\n",
    "]\n",
    "\n",
    "# Subsampling settings\n",
    "max_viz_nodes = 10000\n",
    "timestep_skip = 10\n",
    "\n",
    "if max_viz_nodes and n_nodes > max_viz_nodes:\n",
    "    node_indices = np.random.choice(n_nodes, max_viz_nodes, replace=False)\n",
    "    node_indices = np.sort(node_indices)\n",
    "    positions_viz = positions[node_indices]\n",
    "else:\n",
    "    node_indices = np.arange(n_nodes)\n",
    "    positions_viz = positions\n",
    "\n",
    "timesteps_to_viz = list(range(0, n_timesteps, timestep_skip))\n",
    "positions_2d = positions_viz[:, :2]\n",
    "\n",
    "print(f\"\\nVisualizing {len(timesteps_to_viz)} timesteps\")\n",
    "print(f\"Using {len(node_indices)} nodes\")\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 4: Create Single Variable GIF\n",
    "# ============================================================================\n",
    "\n",
    "def create_single_var_gif(var_idx, output_dir='./gifs', fps=5, dpi=100):\n",
    "    \"\"\"Create and save GIF for a single variable.\"\"\"\n",
    "    \n",
    "    Path(output_dir).mkdir(exist_ok=True)\n",
    "    \n",
    "    var_name = var_names[var_idx]\n",
    "    filename = f\"{output_dir}/Re{int(reynolds) if reynolds else 'unknown'}_{var_name}.gif\"\n",
    "    \n",
    "    print(f\"\\nCreating GIF: {filename}\")\n",
    "    print(f\"  Variable: {var_name}\")\n",
    "    print(f\"  Frames: {len(timesteps_to_viz)}\")\n",
    "    \n",
    "    # Get data range\n",
    "    sample_indices = timesteps_to_viz[::max(1, len(timesteps_to_viz)//10)]\n",
    "    all_values = []\n",
    "    for i in sample_indices:\n",
    "        data = simulation[i].x[node_indices, num_static_feats + var_idx].numpy()\n",
    "        all_values.extend(data)\n",
    "    vmin, vmax = np.percentile(all_values, [2, 98])\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(10, 8), dpi=dpi)\n",
    "    \n",
    "    initial_data = simulation[timesteps_to_viz[0]].x[node_indices, num_static_feats + var_idx].numpy()\n",
    "    scatter = ax.scatter(positions_2d[:, 0], positions_2d[:, 1], \n",
    "                        c=initial_data, cmap='viridis', s=1.0,\n",
    "                        vmin=vmin, vmax=vmax, alpha=0.8, rasterized=True)\n",
    "    \n",
    "    ax.set_xlabel('X', fontsize=12)\n",
    "    ax.set_ylabel('Y', fontsize=12)\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    cbar = plt.colorbar(scatter, ax=ax)\n",
    "    cbar.set_label(var_name.replace('_', ' ').title(), fontsize=12)\n",
    "    \n",
    "    title = ax.set_title(\n",
    "        f'{var_name.replace(\"_\", \" \").title()} - Timestep {timesteps_to_viz[0]}' +\n",
    "        (f' (Re={reynolds:.0f})' if reynolds else ''),\n",
    "        fontsize=14\n",
    "    )\n",
    "    \n",
    "    def animate(frame_num):\n",
    "        actual_idx = timesteps_to_viz[frame_num]\n",
    "        data = simulation[actual_idx].x[node_indices, num_static_feats + var_idx].numpy()\n",
    "        scatter.set_array(data)\n",
    "        title.set_text(\n",
    "            f'{var_name.replace(\"_\", \" \").title()} - Timestep {actual_idx}' +\n",
    "            (f' (Re={reynolds:.0f})' if reynolds else '')\n",
    "        )\n",
    "        return scatter, title\n",
    "    \n",
    "    anim = FuncAnimation(fig, animate, frames=len(timesteps_to_viz),\n",
    "                        interval=1000//fps, blit=False)\n",
    "    \n",
    "    writer = PillowWriter(fps=fps)\n",
    "    anim.save(filename, writer=writer, dpi=dpi)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"  九 Saved: {filename}\")\n",
    "    return filename\n",
    "\n",
    "# Create GIF for pressure\n",
    "create_single_var_gif(var_idx=0, output_dir='./cylinder_gifs', fps=5, dpi=100)\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 5: Create Multi-Variable GIF\n",
    "# ============================================================================\n",
    "\n",
    "def create_multi_var_gif(var_indices=[0, 1, 2], output_dir='./gifs', fps=5, dpi=100):\n",
    "    \"\"\"Create and save side-by-side GIF for multiple variables.\"\"\"\n",
    "    \n",
    "    Path(output_dir).mkdir(exist_ok=True)\n",
    "    \n",
    "    selected_vars = [var_names[i] for i in var_indices]\n",
    "    var_string = \"_\".join([var_names[i] for i in var_indices])\n",
    "    filename = f\"{output_dir}/Re{int(reynolds) if reynolds else 'unknown'}_multi_{var_string}.gif\"\n",
    "    \n",
    "    print(f\"\\nCreating multi-variable GIF: {filename}\")\n",
    "    print(f\"  Variables: {selected_vars}\")\n",
    "    print(f\"  Frames: {len(timesteps_to_viz)}\")\n",
    "    \n",
    "    # Get data ranges\n",
    "    vranges = []\n",
    "    for var_idx in var_indices:\n",
    "        sample_indices = timesteps_to_viz[::max(1, len(timesteps_to_viz)//10)]\n",
    "        all_values = []\n",
    "        for i in sample_indices:\n",
    "            data = simulation[i].x[node_indices, num_static_feats + var_idx].numpy()\n",
    "            all_values.extend(data)\n",
    "        vmin, vmax = np.percentile(all_values, [2, 98])\n",
    "        vranges.append((vmin, vmax))\n",
    "    \n",
    "    # Create figure\n",
    "    n_vars = len(var_indices)\n",
    "    fig, axes = plt.subplots(1, n_vars, figsize=(6*n_vars, 6), dpi=dpi)\n",
    "    if n_vars == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    scatters = []\n",
    "    \n",
    "    for i, (ax, var_idx) in enumerate(zip(axes, var_indices)):\n",
    "        initial_data = simulation[timesteps_to_viz[0]].x[node_indices, num_static_feats + var_idx].numpy()\n",
    "        \n",
    "        scatter = ax.scatter(positions_2d[:, 0], positions_2d[:, 1], \n",
    "                           c=initial_data, cmap='viridis', s=1.0,\n",
    "                           vmin=vranges[i][0], vmax=vranges[i][1], \n",
    "                           alpha=0.8, rasterized=True)\n",
    "        \n",
    "        ax.set_xlabel('X', fontsize=10)\n",
    "        ax.set_ylabel('Y', fontsize=10)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.set_title(var_names[var_idx].replace('_', ' ').title(), fontsize=12)\n",
    "        \n",
    "        plt.colorbar(scatter, ax=ax)\n",
    "        scatters.append(scatter)\n",
    "    \n",
    "    main_title = fig.suptitle(\n",
    "        f'Timestep {timesteps_to_viz[0]}' + (f' (Re={reynolds:.0f})' if reynolds else ''),\n",
    "        fontsize=14\n",
    "    )\n",
    "    \n",
    "    def animate(frame_num):\n",
    "        actual_idx = timesteps_to_viz[frame_num]\n",
    "        \n",
    "        for i, var_idx in enumerate(var_indices):\n",
    "            data = simulation[actual_idx].x[node_indices, num_static_feats + var_idx].numpy()\n",
    "            scatters[i].set_array(data)\n",
    "        \n",
    "        main_title.set_text(\n",
    "            f'Timestep {actual_idx}' + (f' (Re={reynolds:.0f})' if reynolds else '')\n",
    "        )\n",
    "        return scatters + [main_title]\n",
    "    \n",
    "    anim = FuncAnimation(fig, animate, frames=len(timesteps_to_viz),\n",
    "                        interval=1000//fps, blit=False)\n",
    "    \n",
    "    writer = PillowWriter(fps=fps)\n",
    "    anim.save(filename, writer=writer, dpi=dpi)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"  九 Saved: {filename}\")\n",
    "    return filename\n",
    "\n",
    "# Create multi-variable GIF\n",
    "create_multi_var_gif(var_indices=[0, 1, 2], output_dir='./cylinder_gifs', fps=5, dpi=100)\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 6: Batch Create All Variables\n",
    "# ============================================================================\n",
    "\n",
    "def create_all_gifs(output_dir='./cylinder_gifs', fps=5, dpi=100):\n",
    "    \"\"\"Create GIFs for all 7 variables.\"\"\"\n",
    "    \n",
    "    print(f\"\\nCreating GIFs for all {len(var_names)} variables...\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    \n",
    "    created_files = []\n",
    "    \n",
    "    for i, var_name in enumerate(var_names):\n",
    "        try:\n",
    "            filename = create_single_var_gif(i, output_dir=output_dir, fps=fps, dpi=dpi)\n",
    "            created_files.append(filename)\n",
    "        except Exception as e:\n",
    "            print(f\"  九 Error creating GIF for {var_name}: {e}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Summary: Created {len(created_files)}/{len(var_names)} GIFs\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for f in created_files:\n",
    "        print(f\"  {f}\")\n",
    "    \n",
    "    return created_files\n",
    "\n",
    "# Create all GIFs\n",
    "all_gif_files = create_all_gifs(output_dir='./cylinder_gifs', fps=5, dpi=100)\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 7: Statistics Summary\n",
    "# ============================================================================\n",
    "\n",
    "def compute_statistics():\n",
    "    \"\"\"Compute statistics across sampled timesteps.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"VARIABLE STATISTICS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    stats = {var: {'min': [], 'max': [], 'mean': [], 'std': []} \n",
    "            for var in var_names}\n",
    "    \n",
    "    for t in timesteps_to_viz:\n",
    "        dynamic_data = simulation[t].x[:, num_static_feats:num_static_feats + num_dynamic_feats]\n",
    "        \n",
    "        for i, var in enumerate(var_names):\n",
    "            data = dynamic_data[:, i].numpy()\n",
    "            stats[var]['min'].append(data.min())\n",
    "            stats[var]['max'].append(data.max())\n",
    "            stats[var]['mean'].append(data.mean())\n",
    "            stats[var]['std'].append(data.std())\n",
    "    \n",
    "    for var in var_names:\n",
    "        print(f\"\\n{var.replace('_', ' ').title()}:\")\n",
    "        print(f\"  Range: [{np.min(stats[var]['min']):.4e}, {np.max(stats[var]['max']):.4e}]\")\n",
    "        print(f\"  Mean:  {np.mean(stats[var]['mean']):.4e} 췀 {np.mean(stats[var]['std']):.4e}\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "\n",
    "compute_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e3f37af-bdec-4d88-9fc8-46e61882bcfa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 training files\n",
      "\n",
      "================================================================================\n",
      "FEATURE IMPORTANCE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Analyzing 5 files for feature statistics...\n",
      "\n",
      "Processing Reynolds_20_raw_data_normalized.pt...\n",
      "Processing Reynolds_50_raw_data_normalized.pt...\n",
      "Processing Reynolds_150_raw_data_normalized.pt...\n",
      "Processing Reynolds_100_raw_data_normalized.pt...\n",
      "Processing Reynolds_40_raw_data_normalized.pt...\n",
      "\n",
      "================================================================================\n",
      "DYNAMIC FEATURE ANALYSIS RESULTS\n",
      "================================================================================\n",
      "\n",
      "[0] pressure:\n",
      "    Mean:         0.667732\n",
      "    Std Dev:      0.019630\n",
      "    Range:        0.225250\n",
      "    CV:           0.029398\n",
      "    Min/Max:  [  0.655600,   0.722699]\n",
      "    九  VARIES significantly - Keep this feature\n",
      "\n",
      "[1] x_velocity:\n",
      "    Mean:         0.492320\n",
      "    Std Dev:      0.044652\n",
      "    Range:        0.317105\n",
      "    CV:           0.090698\n",
      "    Min/Max:  [  0.299711,   0.732392]\n",
      "    九  VARIES significantly - Keep this feature\n",
      "\n",
      "[2] y_velocity:\n",
      "    Mean:         0.453431\n",
      "    Std Dev:      0.058035\n",
      "    Range:        0.385885\n",
      "    CV:           0.127991\n",
      "    Min/Max:  [  0.440896,   0.465981]\n",
      "    九  VARIES significantly - Keep this feature\n",
      "\n",
      "[3] z_velocity:\n",
      "    Mean:         0.628807\n",
      "    Std Dev:      0.002217\n",
      "    Range:        0.179946\n",
      "    CV:           0.003525\n",
      "    Min/Max:  [  0.627980,   0.629630]\n",
      "    丘멆잺  Very low variability (CV < 0.01)\n",
      "\n",
      "[4] x_vorticity:\n",
      "    Mean:         0.596225\n",
      "    Std Dev:      0.001923\n",
      "    Range:        0.204298\n",
      "    CV:           0.003225\n",
      "    Min/Max:  [  0.596187,   0.596285]\n",
      "    丘멆잺  Very low variability (CV < 0.01)\n",
      "\n",
      "[5] y_vorticity:\n",
      "    Mean:         0.629485\n",
      "    Std Dev:      0.002680\n",
      "    Range:        0.241150\n",
      "    CV:           0.004257\n",
      "    Min/Max:  [  0.625180,   0.633799]\n",
      "    丘멆잺  Very low variability (CV < 0.01)\n",
      "\n",
      "[6] z_vorticity:\n",
      "    Mean:         0.550286\n",
      "    Std Dev:      0.005080\n",
      "    Range:        0.237675\n",
      "    CV:           0.009231\n",
      "    Min/Max:  [  0.549805,   0.550460]\n",
      "    丘멆잺  Very low variability (CV < 0.01)\n",
      "\n",
      "================================================================================\n",
      "FEATURE IMPORTANCE RANKING (highest to lowest)\n",
      "================================================================================\n",
      "1. [2] y_velocity           - Importance:   0.022395 - KEEP\n",
      "2. [1] x_velocity           - Importance:   0.014159 - KEEP\n",
      "3. [0] pressure             - Importance:   0.004422 - KEEP\n",
      "4. [6] z_vorticity          - Importance:   0.001207 - KEEP\n",
      "5. [5] y_vorticity          - Importance:   0.000646 - KEEP\n",
      "6. [3] z_velocity           - Importance:   0.000399 - KEEP\n",
      "7. [4] x_vorticity          - Importance:   0.000393 - KEEP\n",
      "\n",
      "================================================================================\n",
      "RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "All features show significant variation - keep all 7 features\n",
      "\n",
      "Features to KEEP (7 features):\n",
      "  - Index 2: y_velocity\n",
      "  - Index 1: x_velocity\n",
      "  - Index 0: pressure\n",
      "  - Index 6: z_vorticity\n",
      "  - Index 5: y_vorticity\n",
      "  - Index 3: z_velocity\n",
      "  - Index 4: x_vorticity\n",
      "\n",
      "Final feature configuration:\n",
      "  --num_static_feats 3\n",
      "  --num_dynamic_feats 7\n",
      "\n",
      "================================================================================\n",
      "ADDITIONAL CHECKS\n",
      "================================================================================\n",
      "\n",
      "Checking component correlations...\n",
      "  X-velocity vs Y-velocity correlation: 0.0244\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def analyze_feature_importance(train_dir=\"/standard/sds_baek_energetic/von_karman_vortex/Reynolds 1~150/processed_parc/normalized/train/\"):\n",
    "    \"\"\"\n",
    "    Analyze which dynamic features are actually varying and important.\n",
    "    Identifies constant or near-constant features that can be skipped.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_path = Path(train_dir)\n",
    "    \n",
    "    if not train_path.exists():\n",
    "        print(f\"Directory not found: {train_path}\")\n",
    "        return\n",
    "    \n",
    "    data_files = list(train_path.glob(\"*.pt\"))\n",
    "    print(f\"Found {len(data_files)} training files\\n\")\n",
    "    \n",
    "    if not data_files:\n",
    "        print(\"No .pt files found!\")\n",
    "        return\n",
    "    \n",
    "    # Feature names\n",
    "    static_feature_names = ['x_pos', 'y_pos', 'z_pos']\n",
    "    dynamic_feature_names = [\n",
    "        'pressure',\n",
    "        'x_velocity', \n",
    "        'y_velocity',\n",
    "        'z_velocity',\n",
    "        'x_vorticity', \n",
    "        'y_vorticity',\n",
    "        'z_vorticity'\n",
    "    ]\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Analyze multiple files for robustness\n",
    "    files_to_analyze = min(5, len(data_files))\n",
    "    print(f\"\\nAnalyzing {files_to_analyze} files for feature statistics...\\n\")\n",
    "    \n",
    "    # Collect statistics across files\n",
    "    dynamic_stats = {name: {'values': [], 'ranges': [], 'stds': [], 'variances': []} \n",
    "                    for name in dynamic_feature_names}\n",
    "    \n",
    "    for file_idx, file_path in enumerate(data_files[:files_to_analyze]):\n",
    "        print(f\"Processing {file_path.name}...\")\n",
    "        \n",
    "        data = torch.load(file_path, weights_only=False)\n",
    "        \n",
    "        # Sample timesteps throughout the simulation\n",
    "        timestep_samples = np.linspace(0, len(data)-1, min(10, len(data)), dtype=int)\n",
    "        \n",
    "        for t in timestep_samples:\n",
    "            sample = data[t]\n",
    "            \n",
    "            # Assuming structure: [static_features, dynamic_features]\n",
    "            num_static = 3\n",
    "            dynamic_data = sample.x[:, num_static:num_static+7]\n",
    "            \n",
    "            for i, name in enumerate(dynamic_feature_names):\n",
    "                feat = dynamic_data[:, i].numpy()\n",
    "                \n",
    "                dynamic_stats[name]['values'].extend(feat.flatten().tolist()[:1000])  # Sample 1000 nodes\n",
    "                dynamic_stats[name]['ranges'].append(feat.max() - feat.min())\n",
    "                dynamic_stats[name]['stds'].append(feat.std())\n",
    "                dynamic_stats[name]['variances'].append(feat.var())\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DYNAMIC FEATURE ANALYSIS RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Analyze each feature\n",
    "    feature_importance = []\n",
    "    \n",
    "    for i, name in enumerate(dynamic_feature_names):\n",
    "        stats = dynamic_stats[name]\n",
    "        \n",
    "        all_values = np.array(stats['values'])\n",
    "        avg_range = np.mean(stats['ranges'])\n",
    "        avg_std = np.mean(stats['stds'])\n",
    "        avg_var = np.mean(stats['variances'])\n",
    "        \n",
    "        # Check if feature is constant or nearly constant\n",
    "        is_constant = avg_std < 1e-6\n",
    "        is_near_constant = avg_std < 1e-3\n",
    "        is_zero = np.abs(all_values).max() < 1e-6\n",
    "        \n",
    "        # Calculate coefficient of variation (normalized variability)\n",
    "        mean_val = np.mean(np.abs(all_values))\n",
    "        cv = avg_std / mean_val if mean_val > 1e-10 else 0\n",
    "        \n",
    "        # Importance score (higher = more important)\n",
    "        importance_score = avg_std * avg_range\n",
    "        \n",
    "        feature_importance.append({\n",
    "            'index': i,\n",
    "            'name': name,\n",
    "            'mean': np.mean(all_values),\n",
    "            'std': avg_std,\n",
    "            'range': avg_range,\n",
    "            'variance': avg_var,\n",
    "            'cv': cv,\n",
    "            'importance': importance_score,\n",
    "            'is_constant': is_constant,\n",
    "            'is_near_constant': is_near_constant,\n",
    "            'is_zero': is_zero\n",
    "        })\n",
    "        \n",
    "        # Print detailed info\n",
    "        print(f\"\\n[{i}] {name}:\")\n",
    "        print(f\"    Mean:     {np.mean(all_values):>12.6f}\")\n",
    "        print(f\"    Std Dev:  {avg_std:>12.6f}\")\n",
    "        print(f\"    Range:    {avg_range:>12.6f}\")\n",
    "        print(f\"    CV:       {cv:>12.6f}\")\n",
    "        print(f\"    Min/Max:  [{all_values.min():>10.6f}, {all_values.max():>10.6f}]\")\n",
    "        \n",
    "        if is_zero:\n",
    "            print(f\"    丘멆잺  ALWAYS ZERO - Consider skipping\")\n",
    "        elif is_constant:\n",
    "            print(f\"    丘멆잺  CONSTANT - Consider skipping\")\n",
    "        elif is_near_constant:\n",
    "            print(f\"    丘멆잺  NEAR-CONSTANT (std < 1e-3) - May not be important\")\n",
    "        elif cv < 0.01:\n",
    "            print(f\"    丘멆잺  Very low variability (CV < 0.01)\")\n",
    "        else:\n",
    "            print(f\"    九  VARIES significantly - Keep this feature\")\n",
    "    \n",
    "    # Sort by importance\n",
    "    feature_importance.sort(key=lambda x: x['importance'], reverse=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FEATURE IMPORTANCE RANKING (highest to lowest)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for rank, feat in enumerate(feature_importance, 1):\n",
    "        status = \"SKIP\" if feat['is_constant'] or feat['is_zero'] else \"KEEP\"\n",
    "        print(f\"{rank}. [{feat['index']}] {feat['name']:20s} - Importance: {feat['importance']:>10.6f} - {status}\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RECOMMENDATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    skip_indices = [f['index'] for f in feature_importance \n",
    "                   if f['is_constant'] or f['is_zero'] or f['is_near_constant']]\n",
    "    keep_indices = [f['index'] for f in feature_importance \n",
    "                   if not (f['is_constant'] or f['is_zero'] or f['is_near_constant'])]\n",
    "    \n",
    "    if skip_indices:\n",
    "        print(f\"\\nFeatures to SKIP (constant or near-constant):\")\n",
    "        for idx in skip_indices:\n",
    "            print(f\"  - Index {idx}: {dynamic_feature_names[idx]}\")\n",
    "        \n",
    "        print(f\"\\nIn your training script, use:\")\n",
    "        print(f\"  --skip_dynamic_indices {' '.join(map(str, skip_indices))}\")\n",
    "    else:\n",
    "        print(\"\\nAll features show significant variation - keep all 7 features\")\n",
    "    \n",
    "    print(f\"\\nFeatures to KEEP ({len(keep_indices)} features):\")\n",
    "    for idx in keep_indices:\n",
    "        print(f\"  - Index {idx}: {dynamic_feature_names[idx]}\")\n",
    "    \n",
    "    print(f\"\\nFinal feature configuration:\")\n",
    "    print(f\"  --num_static_feats 3\")\n",
    "    print(f\"  --num_dynamic_feats {len(keep_indices)}\")\n",
    "    if skip_indices:\n",
    "        print(f\"  --skip_dynamic_indices {' '.join(map(str, skip_indices))}\")\n",
    "    \n",
    "    # Additional checks\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ADDITIONAL CHECKS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Check for Z-velocity and Z-vorticity specifically (often zero in 2D flows)\n",
    "    z_vel_idx = dynamic_feature_names.index('z_velocity')\n",
    "    z_vort_idx = dynamic_feature_names.index('z_vorticity')\n",
    "    \n",
    "    z_vel_stats = feature_importance[z_vel_idx]\n",
    "    z_vort_stats = next(f for f in feature_importance if f['name'] == 'z_vorticity')\n",
    "    \n",
    "    if z_vel_stats['is_zero'] or z_vel_stats['is_near_constant']:\n",
    "        print(\"\\nThis appears to be a 2D flow (Z-velocity is zero/constant)\")\n",
    "        print(\"Z-velocity can be safely skipped\")\n",
    "    \n",
    "    if z_vort_stats['is_zero'] or z_vort_stats['is_near_constant']:\n",
    "        print(\"Z-vorticity is also zero/constant - this is expected for 2D flows\")\n",
    "    \n",
    "    # Check correlation between x and y components\n",
    "    print(\"\\nChecking component correlations...\")\n",
    "    x_vel = np.array(dynamic_stats['x_velocity']['values'])\n",
    "    y_vel = np.array(dynamic_stats['y_velocity']['values'])\n",
    "    \n",
    "    if len(x_vel) > 0 and len(y_vel) > 0:\n",
    "        corr = np.corrcoef(x_vel, y_vel)[0, 1]\n",
    "        print(f\"  X-velocity vs Y-velocity correlation: {corr:.4f}\")\n",
    "        if abs(corr) > 0.9:\n",
    "            print(f\"    丘멆잺  High correlation - components might not be independent\")\n",
    "\n",
    "    return feature_importance, skip_indices, keep_indices\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = analyze_feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8974a2b4-8659-4e0f-a39a-5d639dbcd5f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 training files\n",
      "\n",
      "================================================================================\n",
      "FEATURE IMPORTANCE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Analyzing 5 files for feature statistics...\n",
      "\n",
      "Processing Reynolds_20_raw_data.pt...\n",
      "Processing Reynolds_1_raw_data.pt...\n",
      "Processing Reynolds_50_raw_data.pt...\n",
      "Processing Reynolds_40_raw_data.pt...\n",
      "Processing Reynolds_150_raw_data.pt...\n",
      "\n",
      "================================================================================\n",
      "DYNAMIC FEATURE ANALYSIS RESULTS\n",
      "================================================================================\n",
      "\n",
      "[0] pressure:\n",
      "    Mean:         0.000752\n",
      "    Std Dev:      0.001314\n",
      "    Range:        0.015002\n",
      "    CV:           1.738390\n",
      "    Min/Max:  [ -0.000107,   0.005917]\n",
      "    九  VARIES significantly - Keep this feature\n",
      "\n",
      "[1] x_velocity:\n",
      "    Mean:         0.073474\n",
      "    Std Dev:      0.017264\n",
      "    Range:        0.120537\n",
      "    CV:           0.234966\n",
      "    Min/Max:  [  0.000004,   0.227575]\n",
      "    九  VARIES significantly - Keep this feature\n",
      "\n",
      "[2] y_velocity:\n",
      "    Mean:         0.000067\n",
      "    Std Dev:      0.013665\n",
      "    Range:        0.097057\n",
      "    CV:          29.561843\n",
      "    Min/Max:  [ -0.004154,   0.004243]\n",
      "    九  VARIES significantly - Keep this feature\n",
      "\n",
      "[3] z_velocity:\n",
      "    Mean:        -0.000000\n",
      "    Std Dev:      0.000000\n",
      "    Range:        0.000000\n",
      "    CV:           0.000000\n",
      "    Min/Max:  [ -0.000000,   0.000000]\n",
      "    丘멆잺  ALWAYS ZERO - Consider skipping\n",
      "\n",
      "[4] x_vorticity:\n",
      "    Mean:        -0.000000\n",
      "    Std Dev:      0.000000\n",
      "    Range:        0.000000\n",
      "    CV:           0.000000\n",
      "    Min/Max:  [ -0.000000,   0.000000]\n",
      "    丘멆잺  ALWAYS ZERO - Consider skipping\n",
      "\n",
      "[5] y_vorticity:\n",
      "    Mean:         0.000000\n",
      "    Std Dev:      0.000000\n",
      "    Range:        0.000000\n",
      "    CV:           0.000000\n",
      "    Min/Max:  [ -0.000000,   0.000000]\n",
      "    丘멆잺  ALWAYS ZERO - Consider skipping\n",
      "\n",
      "[6] z_vorticity:\n",
      "    Mean:        -0.027455\n",
      "    Std Dev:      8.260652\n",
      "    Range:      386.721313\n",
      "    CV:          72.959793\n",
      "    Min/Max:  [ -1.132901,   0.346351]\n",
      "    九  VARIES significantly - Keep this feature\n",
      "\n",
      "================================================================================\n",
      "FEATURE IMPORTANCE RANKING (highest to lowest)\n",
      "================================================================================\n",
      "1. [6] z_vorticity          - Importance: 3194.570068 - KEEP\n",
      "2. [1] x_velocity           - Importance:   0.002081 - KEEP\n",
      "3. [2] y_velocity           - Importance:   0.001326 - KEEP\n",
      "4. [0] pressure             - Importance:   0.000020 - KEEP\n",
      "5. [5] y_vorticity          - Importance:   0.000000 - SKIP\n",
      "6. [4] x_vorticity          - Importance:   0.000000 - SKIP\n",
      "7. [3] z_velocity           - Importance:   0.000000 - SKIP\n",
      "\n",
      "================================================================================\n",
      "RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "Features to SKIP (constant or near-constant):\n",
      "  - Index 5: y_vorticity\n",
      "  - Index 4: x_vorticity\n",
      "  - Index 3: z_velocity\n",
      "\n",
      "In your training script, use:\n",
      "  --skip_dynamic_indices 5 4 3\n",
      "\n",
      "Features to KEEP (4 features):\n",
      "  - Index 6: z_vorticity\n",
      "  - Index 1: x_velocity\n",
      "  - Index 2: y_velocity\n",
      "  - Index 0: pressure\n",
      "\n",
      "Final feature configuration:\n",
      "  --num_static_feats 3\n",
      "  --num_dynamic_feats 4\n",
      "  --skip_dynamic_indices 5 4 3\n",
      "\n",
      "================================================================================\n",
      "ADDITIONAL CHECKS\n",
      "================================================================================\n",
      "\n",
      "Checking component correlations...\n",
      "  X-velocity vs Y-velocity correlation: -0.0156\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def analyze_feature_importance(train_dir=\"/standard/sds_baek_energetic/von_karman_vortex/Reynolds 1~150/processed_parc/individual_cases\"):\n",
    "    \"\"\"\n",
    "    Analyze which dynamic features are actually varying and important.\n",
    "    Identifies constant or near-constant features that can be skipped.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_path = Path(train_dir)\n",
    "    \n",
    "    if not train_path.exists():\n",
    "        print(f\"Directory not found: {train_path}\")\n",
    "        return\n",
    "    \n",
    "    data_files = list(train_path.glob(\"*.pt\"))\n",
    "    print(f\"Found {len(data_files)} training files\\n\")\n",
    "    \n",
    "    if not data_files:\n",
    "        print(\"No .pt files found!\")\n",
    "        return\n",
    "    \n",
    "    # Feature names\n",
    "    static_feature_names = ['x_pos', 'y_pos', 'z_pos']\n",
    "    dynamic_feature_names = [\n",
    "        'pressure',\n",
    "        'x_velocity', \n",
    "        'y_velocity',\n",
    "        'z_velocity',\n",
    "        'x_vorticity', \n",
    "        'y_vorticity',\n",
    "        'z_vorticity'\n",
    "    ]\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Analyze multiple files for robustness\n",
    "    files_to_analyze = min(5, len(data_files))\n",
    "    print(f\"\\nAnalyzing {files_to_analyze} files for feature statistics...\\n\")\n",
    "    \n",
    "    # Collect statistics across files\n",
    "    dynamic_stats = {name: {'values': [], 'ranges': [], 'stds': [], 'variances': []} \n",
    "                    for name in dynamic_feature_names}\n",
    "    \n",
    "    for file_idx, file_path in enumerate(data_files[:files_to_analyze]):\n",
    "        print(f\"Processing {file_path.name}...\")\n",
    "        \n",
    "        data = torch.load(file_path, weights_only=False)\n",
    "        \n",
    "        # Sample timesteps throughout the simulation\n",
    "        timestep_samples = np.linspace(0, len(data)-1, min(10, len(data)), dtype=int)\n",
    "        \n",
    "        for t in timestep_samples:\n",
    "            sample = data[t]\n",
    "            \n",
    "            # Assuming structure: [static_features, dynamic_features]\n",
    "            num_static = 3\n",
    "            dynamic_data = sample.x[:, num_static:num_static+7]\n",
    "            \n",
    "            for i, name in enumerate(dynamic_feature_names):\n",
    "                feat = dynamic_data[:, i].numpy()\n",
    "                \n",
    "                dynamic_stats[name]['values'].extend(feat.flatten().tolist()[:1000])  # Sample 1000 nodes\n",
    "                dynamic_stats[name]['ranges'].append(feat.max() - feat.min())\n",
    "                dynamic_stats[name]['stds'].append(feat.std())\n",
    "                dynamic_stats[name]['variances'].append(feat.var())\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DYNAMIC FEATURE ANALYSIS RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Analyze each feature\n",
    "    feature_importance = []\n",
    "    \n",
    "    for i, name in enumerate(dynamic_feature_names):\n",
    "        stats = dynamic_stats[name]\n",
    "        \n",
    "        all_values = np.array(stats['values'])\n",
    "        avg_range = np.mean(stats['ranges'])\n",
    "        avg_std = np.mean(stats['stds'])\n",
    "        avg_var = np.mean(stats['variances'])\n",
    "        \n",
    "        # Check if feature is constant or nearly constant\n",
    "        is_constant = avg_std < 1e-6\n",
    "        is_near_constant = avg_std < 1e-3\n",
    "        is_zero = np.abs(all_values).max() < 1e-6\n",
    "        \n",
    "        # Calculate coefficient of variation (normalized variability)\n",
    "        mean_val = np.mean(np.abs(all_values))\n",
    "        cv = avg_std / mean_val if mean_val > 1e-10 else 0\n",
    "        \n",
    "        # Importance score (higher = more important)\n",
    "        importance_score = avg_std * avg_range\n",
    "        \n",
    "        feature_importance.append({\n",
    "            'index': i,\n",
    "            'name': name,\n",
    "            'mean': np.mean(all_values),\n",
    "            'std': avg_std,\n",
    "            'range': avg_range,\n",
    "            'variance': avg_var,\n",
    "            'cv': cv,\n",
    "            'importance': importance_score,\n",
    "            'is_constant': is_constant,\n",
    "            'is_near_constant': is_near_constant,\n",
    "            'is_zero': is_zero\n",
    "        })\n",
    "        \n",
    "        # Print detailed info\n",
    "        print(f\"\\n[{i}] {name}:\")\n",
    "        print(f\"    Mean:     {np.mean(all_values):>12.6f}\")\n",
    "        print(f\"    Std Dev:  {avg_std:>12.6f}\")\n",
    "        print(f\"    Range:    {avg_range:>12.6f}\")\n",
    "        print(f\"    CV:       {cv:>12.6f}\")\n",
    "        print(f\"    Min/Max:  [{all_values.min():>10.6f}, {all_values.max():>10.6f}]\")\n",
    "        \n",
    "        if is_zero:\n",
    "            print(f\"    丘멆잺  ALWAYS ZERO - Consider skipping\")\n",
    "        elif is_constant:\n",
    "            print(f\"    丘멆잺  CONSTANT - Consider skipping\")\n",
    "        elif is_near_constant:\n",
    "            print(f\"    丘멆잺  NEAR-CONSTANT (std < 1e-3) - May not be important\")\n",
    "        elif cv < 0.01:\n",
    "            print(f\"    丘멆잺  Very low variability (CV < 0.01)\")\n",
    "        else:\n",
    "            print(f\"    九  VARIES significantly - Keep this feature\")\n",
    "    \n",
    "    # Sort by importance\n",
    "    feature_importance.sort(key=lambda x: x['importance'], reverse=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FEATURE IMPORTANCE RANKING (highest to lowest)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for rank, feat in enumerate(feature_importance, 1):\n",
    "        status = \"SKIP\" if feat['is_constant'] or feat['is_zero'] else \"KEEP\"\n",
    "        print(f\"{rank}. [{feat['index']}] {feat['name']:20s} - Importance: {feat['importance']:>10.6f} - {status}\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RECOMMENDATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    skip_indices = [f['index'] for f in feature_importance \n",
    "                   if f['is_constant'] or f['is_zero'] or f['is_near_constant']]\n",
    "    keep_indices = [f['index'] for f in feature_importance \n",
    "                   if not (f['is_constant'] or f['is_zero'] or f['is_near_constant'])]\n",
    "    \n",
    "    if skip_indices:\n",
    "        print(f\"\\nFeatures to SKIP (constant or near-constant):\")\n",
    "        for idx in skip_indices:\n",
    "            print(f\"  - Index {idx}: {dynamic_feature_names[idx]}\")\n",
    "        \n",
    "        print(f\"\\nIn your training script, use:\")\n",
    "        print(f\"  --skip_dynamic_indices {' '.join(map(str, skip_indices))}\")\n",
    "    else:\n",
    "        print(\"\\nAll features show significant variation - keep all 7 features\")\n",
    "    \n",
    "    print(f\"\\nFeatures to KEEP ({len(keep_indices)} features):\")\n",
    "    for idx in keep_indices:\n",
    "        print(f\"  - Index {idx}: {dynamic_feature_names[idx]}\")\n",
    "    \n",
    "    print(f\"\\nFinal feature configuration:\")\n",
    "    print(f\"  --num_static_feats 3\")\n",
    "    print(f\"  --num_dynamic_feats {len(keep_indices)}\")\n",
    "    if skip_indices:\n",
    "        print(f\"  --skip_dynamic_indices {' '.join(map(str, skip_indices))}\")\n",
    "    \n",
    "    # Additional checks\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ADDITIONAL CHECKS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Check for Z-velocity and Z-vorticity specifically (often zero in 2D flows)\n",
    "    z_vel_idx = dynamic_feature_names.index('z_velocity')\n",
    "    z_vort_idx = dynamic_feature_names.index('z_vorticity')\n",
    "    \n",
    "    z_vel_stats = feature_importance[z_vel_idx]\n",
    "    z_vort_stats = next(f for f in feature_importance if f['name'] == 'z_vorticity')\n",
    "    \n",
    "    if z_vel_stats['is_zero'] or z_vel_stats['is_near_constant']:\n",
    "        print(\"\\nThis appears to be a 2D flow (Z-velocity is zero/constant)\")\n",
    "        print(\"Z-velocity can be safely skipped\")\n",
    "    \n",
    "    if z_vort_stats['is_zero'] or z_vort_stats['is_near_constant']:\n",
    "        print(\"Z-vorticity is also zero/constant - this is expected for 2D flows\")\n",
    "    \n",
    "    # Check correlation between x and y components\n",
    "    print(\"\\nChecking component correlations...\")\n",
    "    x_vel = np.array(dynamic_stats['x_velocity']['values'])\n",
    "    y_vel = np.array(dynamic_stats['y_velocity']['values'])\n",
    "    \n",
    "    if len(x_vel) > 0 and len(y_vel) > 0:\n",
    "        corr = np.corrcoef(x_vel, y_vel)[0, 1]\n",
    "        print(f\"  X-velocity vs Y-velocity correlation: {corr:.4f}\")\n",
    "        if abs(corr) > 0.9:\n",
    "            print(f\"    丘멆잺  High correlation - components might not be independent\")\n",
    "\n",
    "    return feature_importance, skip_indices, keep_indices\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = analyze_feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "621cd14f-79e0-4eb0-ac46-474778cddf61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking all files in: /standard/sds_baek_energetic/von_karman_vortex/Reynolds 1~150/processed_parc/normalized/train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning Files: 100%|郊걱둗郊걱둗郊걱둗郊걱둗郊걱둗| 6/6 [01:42<00:00, 17.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "九 PASSED: No NaN or Inf values were found in any data files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 游늷 UPDATE THIS PATH to your training directory\n",
    "DATA_DIR = Path(\"/standard/sds_baek_energetic/von_karman_vortex/Reynolds 1~150/processed_parc/normalized/train\")\n",
    "\n",
    "def check_tensor(tensor, name, filename):\n",
    "    \"\"\"Checks a single tensor for NaN or Inf values.\"\"\"\n",
    "    if not torch.is_tensor(tensor):\n",
    "        return False\n",
    "    \n",
    "    has_nan = torch.isnan(tensor).any()\n",
    "    has_inf = torch.isinf(tensor).any()\n",
    "    \n",
    "    if has_nan or has_inf:\n",
    "        print(f\"\\n\\n\\033[91m\" + \"=\"*80)\n",
    "        print(f\"仇 CORRUPTED FILE DETECTED: {filename}\")\n",
    "        if has_nan:\n",
    "            print(f\"   - Found NaN in attribute: '{name}' with shape {tensor.shape}\")\n",
    "        if has_inf:\n",
    "            print(f\"   - Found Inf in attribute: '{name}' with shape {tensor.shape}\")\n",
    "        print(\"=\"*80 + \"\\033[0m\\n\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "print(f\"Checking all files in: {DATA_DIR}...\")\n",
    "found_issue = False\n",
    "\n",
    "# Use a glob pattern that matches your files\n",
    "file_list = sorted(list(DATA_DIR.glob(\"*_normalized.pt\")))\n",
    "\n",
    "for file_path in tqdm(file_list, desc=\"Scanning Files\"):\n",
    "    if found_issue:\n",
    "        break\n",
    "    try:\n",
    "        # Assumes the .pt file contains a list of Data objects\n",
    "        data_list_or_obj = torch.load(file_path, weights_only=False)\n",
    "        \n",
    "        # Handle both single Data object and list of Data objects\n",
    "        if not isinstance(data_list_or_obj, list):\n",
    "            data_list = [data_list_or_obj]\n",
    "        else:\n",
    "            data_list = data_list_or_obj\n",
    "\n",
    "        for i, data in enumerate(data_list):\n",
    "            # CORRECTED LINE: Call the keys() method with parentheses\n",
    "            for key in data.keys():\n",
    "                if check_tensor(data[key], key, f\"{file_path.name} [timestep {i}]\"):\n",
    "                    found_issue = True\n",
    "                    break \n",
    "            if found_issue:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\n\\033[91mERROR processing file {file_path.name}: {e}\\033[0m\\n\")\n",
    "        found_issue = True\n",
    "        break\n",
    "\n",
    "if not found_issue:\n",
    "    print(\"\\n九 PASSED: No NaN or Inf values were found in any data files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2500902a-9306-4fb4-b316-bef4eb62879e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking file: Reynolds_20_raw_data_normalized.pt\n",
      "  File contains 400 timesteps. Checking the first 5...\n",
      "    - Timestep 0: global_params = tensor([0.1275])\n",
      "    - Timestep 1: global_params = tensor([0.1275])\n",
      "    - Timestep 2: global_params = tensor([0.1275])\n",
      "    - Timestep 3: global_params = tensor([0.1275])\n",
      "    - Timestep 4: global_params = tensor([0.1275])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# 游늷 UPDATE THIS PATH to a file in your training set\n",
    "FILE_TO_CHECK = Path(\"/standard/sds_baek_energetic/von_karman_vortex/Reynolds 1~150/processed_parc/normalized/train/Reynolds_20_raw_data_normalized.pt\") # Example file name\n",
    "\n",
    "def check_global_params(filepath):\n",
    "    \"\"\"Loads a PyG data file and prints the global_params.\"\"\"\n",
    "    if not filepath.exists():\n",
    "        print(f\"ERROR: File not found at {filepath}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Checking file: {filepath.name}\")\n",
    "    \n",
    "    try:\n",
    "        data_list = torch.load(filepath, weights_only=False)\n",
    "        \n",
    "        if not isinstance(data_list, list) or not data_list:\n",
    "            print(\"  File is empty or not in the expected list format.\")\n",
    "            return\n",
    "\n",
    "        print(f\"  File contains {len(data_list)} timesteps. Checking the first 5...\")\n",
    "        \n",
    "        for i, data in enumerate(data_list[:5]):\n",
    "            if hasattr(data, 'global_params') and data.global_params is not None:\n",
    "                print(f\"    - Timestep {i}: global_params = {data.global_params}\")\n",
    "            else:\n",
    "                print(f\"    - Timestep {i}: No 'global_params' attribute found.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_global_params(FILE_TO_CHECK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1c1950-66a3-4f3e-b034-24855ce285ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.7.0",
   "language": "python",
   "name": "pytorch-2.7.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
